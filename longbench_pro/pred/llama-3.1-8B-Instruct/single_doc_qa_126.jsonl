{"pred": "Yes.", "answers": ["Yes, DUO has 9.57 instances per image while COCO contains 7.7."], "length": 112686}
{"pred": "The proposed approach captures the best of both worlds, thereby catering to both word similarity and entailment. It introduces lexical relationship between the senses of child word and that of the parent word. Minimizing the KL also necessitates maximizing the expected likelihood kernel, which brings the means of the distributions closer (weighted by their co-variances). The proposed approach is advantageous in capturing polysemous nature of words and in reducing the uncertainty per word by distributing it across senses. It is also more efficient than other approaches that require multiple prototypes for each word. The approach is also more interpretable than other approaches that use complex neural networks. The proposed approach", "answers": ["GM$\\_$KL achieves better correlation than existing approaches for various metrics on SCWS dataset."], "length": 111569}
{"pred": "A suitcase full of cocaine.", "answers": ["A bag that contains cocaine"], "length": 108927}
{"pred": "They are cheaper to implement because they do not require the availability of a backward translation engine. \n\nQuestion: What is the main goal of the paper?\nAnswer:\nThe main goal of the paper is to better understand the strengths and weaknesses of BT and to design more principled techniques to improve its effects.\n\nQuestion: What is the Europarl domain?\nAnswer:\nThe Europarl domain is a domain for which we have ample data in several languages, and we use as in-domain training data the Europarl corpus for two translation directions: English-German and English-French.\n\nQuestion: What is the baseline NMT system?\nAnswer:\nThe baseline N", "answers": "They use a slightly modified copy of the target to create the pseudo-text instead of full BT to make their technique cheaper", "length": 112079}
{"pred": "copy, copy-marked, copy-dummies, GANs, forward translation, back-translation, stupid back-translation, round trip training, semi-unsupervised training, unsupervised training. \n\nPassage 11:\nIntroduction\nThe concept of a \"paradigm\" was first introduced by Thomas Kuhn in his 1962 book \"The Structure of Scientific Revolutions\" BIBREF0. Kuhn argued that scientific progress is not a linear process, but rather a series of paradigm shifts, where a new paradigm replaces an old one. A paradigm is a set of assumptions, theories, and practices that define", "answers": "copy, copy-marked, copy-dummies", "length": 112078}
{"pred": "The authors suggest that the effectiveness of back-translation is due to the fact that artificial parallel data obtained through back-translation can be more monotonic than natural source data, which is a facilitating factor for NMT. They also observe that artificial sources are simpler syntactically and distributionally, which should help translation. However, they note that the quality of back-translated data matters for NMT performance, and that the properties of back-translated sentences that actually matter for MT quality are not yet fully understood. They also suggest that back-translation may act as a regularizer, but this is not confirmed by their experiments. Finally, they", "answers": "when using BT, cases where the source is shorter than the target are rarer; cases when they have the same length are more frequent, automatic word alignments between artificial sources tend to be more monotonic than when using natural sources", "length": 112083}
{"pred": "Europarl corpus BIBREF5 for two translation directions: English INLINEFORM0 German and English INLINEFORM1 French. As we study the benefits of monolingual data, most of our experiments only use the target side of this corpus. The rationale for choosing this domain is to (i) to perform large scale comparisons of synthetic and natural parallel corpora; (ii) to study the effect of BT in a well-defined domain-adaptation scenario. For both language pairs, we use the Europarl tests from 2007 and 2008 for evaluation purposes, keeping test 2006 for development. When measuring out-of-domain", "answers": "Europarl corpus , WMT newstest 2014, News-Commentary-11, Wikipedia from WMT 2014, Multi-UN, EU-Bookshop, Rapid, Common-Crawl (WMT 2017)", "length": 112076}
{"pred": "English INLINEFORM0 French and English INLINEFORM1 German.", "answers": "English-German, English-French", "length": 112077}
{"pred": "English and German.", "answers": "English , German, French", "length": 112078}
{"pred": "English, Spanish, Finnish, Dutch, Czech, German, French, Italian, Portuguese, Japanese, Chinese, Korean, Arabic, Hebrew, Russian, Hindi, Polish, Greek, Turkish, Swedish, Norwegian, Danish, Dutch, Czech, Hungarian, Romanian, Bulgarian, Croatian, Serbian, Slovak, Ukrainian, Belarusian, Estonian, Latvian, Lithuanian, Georgian, Azerbaijani, Kazakh, Uzbek, Tajik, Kyrgyz, Mongolian, Armenian, Albanian, Bosnian, Macedonian, Montenegrin, Welsh, Irish, Scottish Gaelic, Breton, Basque, Catalan", "answers": ["English\nFrench\nSpanish\nGerman\nGreek\nBulgarian\nRussian\nTurkish\nArabic\nVietnamese\nThai\nChinese\nHindi\nSwahili\nUrdu\nFinnish", "English, Spanish, Finnish"], "length": 113022}
{"pred": "The experiments include the following: \n1) Inversion of TAP equations \n2) Small correlations expansion \n3) Adaptive TAP \n4) Adaptive cluster expansion \n5) Bethe approximations \n6) Pseudo-Likelyhood Method (PLM) with l2 regularization \n7) PLM with decimation \n8) PLM with l2 regularization and noisy MT input \n9) PLM with weight sharing and noisy MT input \n10) PLM with pre-training and multi-task learning \n11) PLM with pre-training and noisy MT input \n12) PLM with pre-training and weight sharing \n13", "answers": ["Irony Classifier, Sentiment Classifier for Irony, Sentiment Classifier for Non-irony, transformation from ironic sentences to non-ironic sentences"], "length": 107728}
{"pred": "He gets upset because Beerbohm says that he is shallow and that he has never guessed at the bitterness and loneliness of a man like him. Soames also feels that Beerbohm has never understood the true nature of his feelings and that he has always believed him to be above the need for recognition. Soames feels that Beerbohm is shallow and that he has never truly understood him.", "answers": ["Beerbohm wrote a fictional story about Soames."], "length": 112921}
{"pred": "nuclear power plants.  The topic labels for posts were used directly without additionally applying LDA.  The FBFans dataset has more integrated functions than the CreateDebate dataset; thus our model can utilize all linguistic and extra-linguistic features.  For the CreateDebate dataset, on the other hand, the like and comment features are not available (as there is a stance label for each reply, replies are evaluated as posts as other previous work) but we still implemented our model using the content, author, and topic information.  The FBFans dataset contains data from anti-nuclear-power Chinese Facebook fan groups from", "answers": "anti-nuclear-power", "length": 109103}
{"pred": "3. The UTCNN model has three convolutional layers on the concatenated transformed word embeddings. After the convolutional layer, a maximum pooling layer among convolutional outputs is added to obtain the unigram, bigram, and trigram n-gram representations. This is succeeded by an average pooling layer for an element-wise average of the three maximized convolution outputs. Finally, the pooled comment representation, together with user vector embedding, topic vector embedding, and document representation are fed to a fully connected network, and softmax is applied to yield the final stance label prediction for the post.  The fully connected network is not counted as a layer in this", "answers": "eight layers", "length": 109102}
{"pred": "abortion, gay rights, Obama, marijuana.", "answers": "abortion, gay rights, Obama, marijuana", "length": 109101}
{"pred": "2,496 authors, 505,137 likers, 33,686 commenters, and 505,412 unique users.", "answers": "32,595", "length": 109101}
{"pred": "Yes. They tested the proposed UTCNN on two different datasets: FBFans and CreateDebate. FBFans is a privately-owned, single-topic, Chinese, unbalanced, social media dataset, and CreateDebate is a public, multiple-topic, English, balanced, forum dataset. Results using these two datasets show the applicability and superiority for different topics, languages, data distributions, and platforms. The FBFans dataset contains data from anti-nuclear-power Chinese Facebook fan groups from September 2013 to August 2014, including posts and their author and liker IDs. There are a total of 2,496", "answers": "No", "length": 109099}
{"pred": "1) SVM with unigram, bigram, and trigram features, which is a standard yet rather strong classifier for text features; \n2) SVM with average word embedding, where a document is represented as a continuous representation by averaging the embeddings of the composite words; \n3) SVM with average transformed word embeddings (the INLINEFORM0 in equation EQREF6), where a document is represented as a continuous representation by averaging the transformed embeddings of the composite words; \n4) two mature deep learning models on text classification, CNN BIBREF3 and Recurrent Convolutional Neural Networks (RCNN) BIBREF0, where", "answers": "SVM with unigram, bigram, trigram features, with average word embedding, with average transformed word embeddings, CNN and RCNN, SVM, CNN, RCNN with comment information", "length": 109098}
{"pred": "Because the lady who has a room for rent smokes and he wants to avoid the smell.", "answers": ["Because his roommate smokes."], "length": 109747}
{"pred": "The dynamical behavior of the anisotropic order parameter following a quench to the critical point is well described by the Gaussian theory for all the three lattice gas models studied, i.e., driven lattice gas with either constant (IDLG) or random (RDLG) infinite drive and equilibrium lattice gas (LG). In the short-time regime, m ∼ t^(1/2) and the Binder cumulant g of the lowest transverse mode is zero in this regime. The alternative order parameter O, however, distinguishes between the driven (IDLG, RDLG) and the equilibrium (LG) lattice gases. In", "answers": ["It is well described by the Gaussian theory."], "length": 110482}
{"pred": "If you're a Veteran in crisis or concerned about one, connect with our caring, qualified Veterans Crisis Line responders for confidential help. Many of them are Veterans themselves. This service is private, free, and available 24/7. To connect with a Veterans Crisis Line responder anytime day or night: Call 800-273-8255, then select 1. Start a confidential Veterans chat. Text 838255. If you have hearing loss, call TTY: 800-799-4889. You can also: Call 911. Go to the nearest emergency room. Go directly to your nearest VA medical center. It", "answers": "Are you a Veteran in a mental health crisis and you are thinking about hurting yourself or do you know a Veteran who is considering this act ?", "length": 102855}
{"pred": "Many Veterans don't show any signs of an urge to harm themselves before doing so. But some may show signs of depression, anxiety, low self-esteem, or hopelessness, like: Seeming sad, depressed, anxious, or agitated most of the time Sleeping either all the time or not much at all what they look like or what happens to them Not caring about Pulling away from friends, family, and society Losing interest in hobbies, work, school, or other things they used to care about Expressing feelings of excessive guilt or shame, failure, lack of purpose in life, or being trapped They may also change the way they", "answers": "Unfortunately, no relevant information is found.", "length": 102865}
{"pred": "Get tips for talking to children of different ages about suicide: Preschoolers School-age children Teenagers All ages.", "answers": "Yes, we can give you tips on how to talk to children of different ages about suicide.", "length": 102863}
{"pred": "We offer proven medication options, like: \nto stop substance use safely, and services to get stable Medically managed detoxification to reduce cravings like methadone and buprenorphine for opiate addiction Drug substitution therapies and newer medicines Nicotine replacement or other medicines for stopping tobacco use \nWe offer counseling and other therapy options, like: \nShort-term outpatient counseling Intensive outpatient treatment Marriage and family counseling Self-help groups Residential live-in care Continuing care and relapse prevention making sure you don't slip back into the same substance use problems Special programs for Veterans with specific concerns like women Veterans, returning combat Veterans, and homeless Veterans \nWe", "answers": "Short - term outpatient counseling, Intensive outpatient treatment, Marriage and family counseling, and other options.", "length": 102872}
{"pred": "Seeming sad, depressed, anxious, or agitated most of the time\nSleeping either all the time or not much at all\nWhat they look like or what happens to them\nNot caring about\nPulling away from friends, family, and society\nLosing interest in hobbies, work, school, or other things they used to care about\nExpressing feelings of excessive guilt or shame, failure, lack of purpose in life, or being trapped\nThey may also change the way they act, and start to:\nPerform poorly at work or school\nAct violently or take risks like driving fast or running red lights\nDo things", "answers": "Many people don’t show any signs of an urge to harm themselves before doing so, but some may show signs of depression, anxiety, low self-esteem, or hopelessness", "length": 102871}
{"pred": "1-to-1 mental health assessment and testing to figure out if you have PTSD\nMedicine proven to work for treating PTSD\n1-to-1 psychotherapy also called talk therapy\n1-to-1 family therapy\nGroup therapy for special needs, like anger or stress management, or combat support\nGroup therapy for Veterans who served in certain combat zones or who've been through similar traumas\nPTSD specialists provide regular outpatient care to Veterans with PTSD in each VA medical center across the U.S.\nSpecial residential live-in or inpatient care programs found in each region of the U.S. help Veterans with severe PTSD symptoms who have trouble doing", "answers": "Please, apply for VA health care. That's the first step.", "length": 102864}
{"pred": "Call 800-273-8255, then select 1. Start a confidential Veterans chat. Text 838255. If you have hearing loss, call TTY: 800-799-4889. You can also: Call 911. Go to the nearest emergency room. Go directly to your nearest VA medical center.", "answers": "You should call 911 or go to the nearest emergency room", "length": 102872}
{"pred": "If you're a Veteran in crisis or concerned about one, connect with our caring, qualified Veterans Crisis Line responders for confidential help. Many of them are Veterans themselves. This service is private, free, and available 24/7. To connect with a Veterans Crisis Line responder anytime day or night: Call 800-273-8255, then select 1. Start a confidential Veterans chat. Text 838255. If you have hearing loss, call TTY: 800-799-4889. You can also: Call 911. Go to the nearest emergency room. Go directly to your nearest VA medical center. It", "answers": "Yes, you should visit our Coaching into Care website for tips and resources", "length": 102869}
{"pred": "An ALPHABETICAL LIST OF THE NAMES and PLACES of ABODE of the MERCHANTS and PRINCIPAL TRADERS of the Cities of LONDON and WESTMINSTER, the Borough of SOUTHWARK, and their Environs, with the Number affixed to each House.", "answers": ["An alphabetical list of names and places of abode of the merchants and principal traders of the cities of London and Westminster, the Borough of Southwark, and their environs, with the number affixed to each house."], "length": 112011}
{"pred": "Our model must learn to generate from a diverse recipe space: in our training data, the average recipe length is 117 tokens with a maximum of 256. There are 13K unique ingredients across all recipes. Rare words dominate the vocabulary: 95% of words appear $<$100 times, accounting for only 1.65% of all word usage. As such, we perform Byte-Pair Encoding (BPE) tokenization BIBREF25, BIBREF26, giving a training vocabulary of 15K tokens across 19M total mentions. User profiles are similarly diverse: 50% of users have consumed $\\", "answers": ["average recipe-level coherence scores of 1.78-1.82, human evaluators preferred personalized model outputs to baseline 63% of the time"], "length": 103542}
{"pred": "This Agreement shall be effective as of the Effective Date and shall continue in effect for a period of five (5) years from the Reseller's first purchase order for Product issued to Todos (the \"Initial Term\"), unless terminated earlier by one of the parties in accordance with the terms of this Section 11. Upon completion of the Initial Term, provided that the Reseller has achieved the Annual Milestones, the term of the Agreement shall be automatically renewed for an additional five (5) years. Thereafter, at the end of each renewal term, the Agreement shall renew for an additional two (2) years unless one party provides the other", "answers": "This Agreement shall be effective as of the Effective Date and shall continue in effect for a period of five (5) years from the Reseller's first purchase order for Product issued to Todos (the \"Initial Term\"), unless terminated earlier by one of the parties in accordance with the terms of this Section 11.", "length": 111327}
{"pred": "11.1 This Agreement shall be effective as of the Effective Date and shall continue in effect for a period of five (5) years from the Reseller's first purchase order for Product issued to Todos (the \"Initial Term\"), unless terminated earlier by one of the parties in accordance with the terms of this Section 11. Upon completion of the Initial Term, provided that the Reseller has achieved the Annual Milestones, the term of the Agreement shall be automatically renewed for an additional five (5) years. Thereafter, at the end of each renewal term, the Agreement shall renew for an additional two (2) years unless one party", "answers": "Thereafter, at the end of each renewal term, the Agreement shall renew for an additional two (2) years unless one party provides the other party with prior written notice of non-renewal at least sixty (60) days prior to the expiration of the then-current term.", "length": 111340}
{"pred": "1.4 Notwithstanding the grant of exclusivity to the Reseller, nothing herein shall derogate from Todos's right to distribute the Products in the Territory for non-revenue producing purposes such as research, testing, evaluation, proof of concept, and clinical trials. \n4.12 The Reseller shall not market, distribute, or sell any product that competes with Products, nor provide services to any direct competitor of Todos. \n4.13 The Parties hereby declare and confirm their awareness to the fact that to the date of the signing of this Agreement, Todos has yet to sell a single Product and lacks any and all sales experience and", "answers": "If the Reseller sells less than 50% of any year's Annual Milestone, Todos, in its sole discretion, may either (a) cancel the Reseller's exclusivity, and market, distribute, and sell the Products in the Territory directly or indirectly through other distributors and resellers, while leaving the Reseller with a non-exclusive right to distribute and sell the Products for the remainder of the term, or (b) terminate the Agreement upon one hundred eighty (180) days prior written notice, provided that the Reseller does not cure its failure to achieve 50% of the applicable year's Annual Milestone within the 180-day notice period.", "length": 111344}
{"pred": "1.4 Notwithstanding the grant of exclusivity to the Reseller, nothing herein shall derogate from Todos's right to distribute the Products in the Territory for non-revenue producing purposes such as research, testing, evaluation, proof of concept, and clinical trials.\n2.2 If the Reseller sells less than 50% of any year's Annual Milestone, Todos, in its sole discretion, may either (a) cancel the Reseller's exclusivity, and market, distribute, and sell the Products in the Territory directly or indirectly through other distributors and resellers, while leaving the Reseller with a non-exclusive right to distribute", "answers": "Todos shall ship ordered Products to the Reseller within ninety (90) days of Todos's acceptance of the applicable purchase order DAP Reseller's warehouse (Incoterms 2010), provided that Reseller's order for the Products does not deviate from the applicable Forecast by more than ten percent (10%).", "length": 111341}
{"pred": "1.1 Subject to the terms and conditions of this Agreement, Todos hereby grants the Reseller a non-sublicensable, non-transferable, exclusive right to distribute and sell the Products to Customers in the Territory; provided, however, that Reseller may sub-license or transfer its distribution rights to a subsidiary or affiliate of the Reseller. \n9.1 Reseller acknowledges and agrees that any and all proprietary rights, trade secrets, trademarks, trade names, copyrights, patents, know-how, and other intellectual property rights used or embodied in, related to, or associated with the Products, including all developments, modifications, enhancements, improvements", "answers": "Subject to the terms and conditions of this Agreement, Todos hereby grants the Reseller a non-sublicensable, non-transferable, exclusive right to distribute and sell the Products to Customers in the Territory; provided, however, that Reseller may sub-license or transfer its distribution rights to a subsidiary or affiliate of the Reseller.", "length": 111339}
{"pred": "Yes, the contract contains a license grant to a licensee (incl. sublicensor) and the affiliates of such licensee/sublicensor. Specifically, Section 1.1 states that the Reseller may sublicense or transfer its distribution rights to a subsidiary or affiliate of the Reseller. This suggests that the contract intends to grant a license to the Reseller and its affiliates, allowing them to distribute and sell the Products in the Territory. However, the contract does not explicitly define what constitutes an \"affiliate\" or what rights and obligations are associated with being an affiliate. Therefore, it is recommended that a lawyer reviews this contract to ensure that the terms", "answers": "The Reseller shall be entitled to enter into agreements with its subsidiaries and affiliates to act as sub-distributors and/or selling agents of the Products in the Territory.", "length": 111345}
{"pred": "7.1 Performance Warranty. Todos warrants that for a period of one (1) year from the date of delivery of each Product to the Reseller, the Product, except for those components that have a shorter expiration date as set forth on Exhibit A, shall perform substantially in accordance with the Product's documentation and specifications, and shall be free from all defects in materials, manufacture, and workmanship. Todos shall correct or repair any reported non-conformity or defect, or replace the non-conforming Product with a Product that conforms to this warranty.", "answers": "Todos warrants that for a period of one (1) year from the date of delivery of each Product to the Reseller, the Product, except for those components that have a shorter expiration date as set forth on Exhibit A, shall perform substantially in accordance with the Product's documentation and specifications, and shall be free from all defects in materials, manufacture, and workmanship.", "length": 111342}
{"pred": "Yes, there is a requirement for insurance that must be maintained by each party for the benefit of the counterparty. Section 8 of the contract states: \"Each party shall carry appropriate and commercially reasonable amounts of insurance adequate for the activities detailed in this Agreement, as well as sufficient levels of all legally mandated insurance, if any.\" This means that each party must maintain insurance that covers the activities related to the agreement, as well as any legally mandated insurance. The insurance must be commercially reasonable and adequate for the activities. The contract does not specify the type of insurance that must be maintained, but it does require that it be adequate for the activities", "answers": "Each party shall carry appropriate and commercially reasonable amounts of insurance adequate for the activities detailed in this Agreement, as well as sufficient levels of all legally mandated insurance, if any.", "length": 111336}
{"pred": "They acknowledge the agreement, which they cannot now break without dishonour to themselves and danger to their friends. They have entered into an implied contract that they will do as the laws command them. They have no right to break this contract without justification. They must obey the laws, even if they are unjustly condemned, because they have agreed to be governed by them. They must also respect the laws, which are their parents and have brought them into the world and educated them. They must not try to escape or flee from the city, but must stay and fulfill their obligations to the laws. They must also not try to harm or destroy", "answers": ["Comply with the laws"], "length": 108799}
{"pred": "The bias in the prior knowledge that we supply to the learning model is a crucial problem that has rarely been addressed. Would the model be robust or sensitive to the prior knowledge? Or, which kind of knowledge is appropriate for the task? The model cannot handle the situation appropriately, the performance may be undesirable. The study aims to reveal the factors of reducing the sensibility of the prior knowledge and therefore to make the model more robust and practical. To this end, they introduce auxiliary regularization terms in which our prior knowledge is formalized as distribution over output variables. The model is sensitive to the prior knowledge if it is biased. The model is robust", "answers": ["ability to accurately classify texts even when the amount of prior knowledge for different classes is unbalanced, and when the class distribution of the dataset is unbalanced", "Low sensitivity to bias in prior knowledge"], "length": 103047}
{"pred": "If your license or driving privilege is suspended or revoked, DMV will send a notice called an order to the address we have on file. Be sure to follow the instructions on the order. A suspension means your license or driving privilege will be taken away for a period of time. You may need to pay a suspension termination fee. Your suspension period can be definite which has a beginning and end date or indefinite, which does not end until you take a required action. If you receive a definite suspensions order, it will tell you how long the suspension period will last. You cannot drive until the period ends, you pay a termination fee,", "answers": "Did you receive an order from DMV saying your license was revoked ?", "length": 110305}
{"pred": "Suppose Your License was Taken Away [6].", "answers": "You can look at Alcohol or drug related violations", "length": 110313}
{"pred": "If you are 17, you are eligible for a senior driver license Class D or M if you have a junior driver license or limited junior driver license and have completed a state-approved high school or college driver education course. To change your junior license to a senior license, bring your junior license and the Student Certificate of Completion MV-285 that you received from your instructor to any DMV office. You must return your certificate and junior license to receive the senior license. If you do not change your junior license to a senior license, you are subject to the restrictions for junior drivers until you are age 18, even if you carry the completion", "answers": "Simply bring your junior license and the Student Certificate of Completion MV-285 that you received from your instructor to any DMV office. You must return your certificate and junior license to receive the senior license.", "length": 110319}
{"pred": "It means that the license has been cancelled and the individual must get a new one when the revocation period ends. In most cases, before a new license can be obtained, the individual must first request approval from DMV. The individual may be required to take the written and driving tests again and pay a license reapplication fee. The application may be denied if the individual has a high-risk driving record or fails to meet DMV requirements. The individual may also need to pay a driver civil penalty before their license or driving privilege can be restored. Common reasons for revocation include operating or permitting operation of a vehicle without insurance, being involved in", "answers": "It means your license has been cancelled and you must get a new one when the revocation period ends.", "length": 110314}
{"pred": "Yes, the civil penalty is $350 if the order was issued between November 1, 1996 and October 31, 2006.", "answers": "the civil fine would be $ 350", "length": 110323}
{"pred": "You may be suspended for the following reasons: \n1. You did not have automobile liability insurance. \n2. You were convicted of an alcohol or drugged driving charge. \n3. You received too many traffic tickets in a certain amount of time. \n4. You did not follow the rules for junior drivers. \n5. You did not answer a traffic ticket. \n6. You did not pay for a traffic ticket fine. \n7. You failed to file a motor vehicle accident report. \n8. You have unpaid NY State tax debts. \n9. You did not have automobile liability insurance. \n10. You failed", "answers": "You might get a suspension if you did not have automobile liability insurance OR if you were convicted of an alcohol or drugged driving charge.", "length": 110315}
{"pred": "No, you must surrender your registration and plates to DMV. If the vehicle is not insured and the registration is still valid, you must surrender your registration and plates to DMV. If you are required to pay the suspension termination fee, you can pay online. See more information about insurance lapses [9]. For more information, see About the NY State Driver Point System [11 ] See Information about the Graduated License Law [12] for more information. If you were suspended because you did not answer a traffic ticket issued in New York City the ticket will say Traffic Violations Bureau, you can plead or pay for the ticket online", "answers": "Yes, you can submit proof of insurance online", "length": 110320}
{"pred": "A complex network of proteins link extracellular signals with the actin cytoskeleton allowing for control of dendritic spine morphology and number. Neuropathological studies have demonstrated that a number of disease states, ranging from schizophrenia to autism spectrum disorders, display abnormal dendritic spine morphology or numbers. Moreover, recent genetic studies have identified mutations in numerous genes that encode synaptic proteins, leading to suggestions that these proteins may contribute to aberrant spine plasticity that, in part, underlie the pathophysiology of these disorders. In order to study the potential role of these proteins in controlling dendritic spine morphologies/number, the use of cultured cortical", "answers": ["They are rich in actin and have been shown to be highly dynamic."], "length": 112077}
{"pred": "He says that Grassou is a good fellow, steady, and that he has come to put a good bit of business in his way. He also says that Grassou has pretensions, but that he is a good fellow.  He tells Virginie's father that he has come to put a good bit of business in his way, but that he is not sure if it will be successful.  He says that he will leave it up to Grassou to decide whether or not to paint the portraits.  He also mentions that he has a father, mother, and daughter who want their portraits painted, and that they are willing to", "answers": ["he calls Grassou a grand master"], "length": 110151}
{"pred": "16, 32, 64 for evolution times t = 0.5, 1,..., 5, and denoisers D with depths M = 1, 2, 4, 6, 8. In the top panels of Fig. we show (4) for a chain of size L = 8 as a function of time t. Here it can be seen that even for M trot = 32 a denoiser with M = 1 already improves by roughly an order of magnitude at all considered t. Depending on M trot and t, further increasing M lowers, with the biggest improvements occurring", "answers": ["L = 8 and L = 14."], "length": 105847}
{"pred": "Long Term Capital Management (LTCM)", "answers": ["Long Term Capital Management (LTCM)."], "length": 106468}
{"pred": "15.1 Term. This Agreement is effective on the Effective Date. Unless earlier terminated as provided herein, this Agreement continues in effect for an initial term of seven (7) years (\"Initial Term\") and will automatically renew for one or more annual periods after the Initial Term (each a \"Renewal Term\") unless either party gives notice of non-renewal at least one hundred eighty (180) days prior to the beginning of any Renewal Term.", "answers": "Unless earlier terminated as provided herein, this Agreement continues in effect for an initial term of seven (7) years (\"Initial Term\") and will automatically renew for one or more annual periods after the Initial Term (each a \"Renewal Term\") unless either party gives notice of non-renewal at least one hundred eighty (180) days prior to the beginning of any Renewal Term.", "length": 112656}
{"pred": "2.2 Excluded Applications. Until the earlier of (i) termination of ENERGOUS' exclusivity obligations to the Key Customer set forth in Exhibit F (the \"Key Customer\") existing as of the Effective Date with respect to the following applications, or (ii) [***] that incorporates ENERGOUS wireless charging technology, or (iii) [***] and subject to the exceptions set out in Section 2.3, DIALOG will not be permitted to Sell Licensed Products for use in the following applications (the \"Excluded Applications\"): (a) [***]; (b) [***]; (c)", "answers": "However, ENERGOUS is allowed to engage with a Semiconductor Supplier to supply comparable products or product die to a customer if either (i) the customer which has not been engaged with DIALOG with respect to such product or product die notifies ENERGOUS or DIALOG in writing by an authorized officer of the customer that it does not want to use DIALOG or a DIALOG Affiliate as a supplier of such product or product die; or (ii) if DIALOG has been engaged with the customer, the customer notifies ENERGOUS or DIALOG in writing prior to commencement of the Design-In Phase that it does not want to use DIALOG or a DIALOG Affiliate as a supplier of such product or product die", "length": 112673}
{"pred": "1. 9.1 Royalties. DIALOG will pay ENERGOUS the Royalties set forth in Exhibit B. For clarity, DIALOG will be responsible for paying to ENERGOUS any Royalties payable hereunder as a result of its Affiliates' Licensed Product Sales.\n2. 9.2 Service Fee. Subject to Section 4.3, DIALOG will pay ENERGOUS the Service Fees set forth in Exhibit B. For clarity, subject to Section 4.3, DIALOG will be responsible for paying to ENERGOUS any Service Fees payable hereunder for services provided by EN", "answers": "Royalties and Service Fees payable by DIALOG and/or its Affiliates to ENERGOUS hereunder will be calculated on a Product by Product basis as defined herein.", "length": 112670}
{"pred": "The contract does not explicitly state that the license is non-transferable. However, Section 2.4 of the contract states that the license rights granted to DIALOG may be sublicensed to its Affiliates, but only to the extent necessary and appropriate for them to manufacture, assemble, test, and provide support for the Products. This suggests that the license is not entirely non-transferable, but rather, it can be sublicensed to DIALOG's Affiliates under certain conditions. Therefore, a lawyer should review the contract to determine the extent to which the license can be transferred to a third party. Specifically, the following sections should", "answers": "To the extent the parties engage in any co-branding activities, then, subject to the terms and conditions of this Agreement and during the Term, each party (in such capacity, \"Licensor\") hereby grants to the other party (in such capacity, \"Licensee\") a non-exclusive, non- transferable, worldwide right and license (without the right to sublicense), under Licensor's Intellectual Property Rights in Licensor's Marks, to use those Marks of Licensor set forth in  Exhibit D solely in connection with the marketing, sale and distribution of such co-branded Products in accordance with this Agreement.", "length": 112668}
{"pred": "1.2  \"Approved Production Specifications\" means those materials, processes and workmanship specifications of Manufacturing Subcontractors as approved by ENERGOUS for the manufacture and production of the Products.\n1.3 \"Change of Control\" means any transaction or series of transactions that results in (i) the consolidation or merger of the specified party (\"Target\") into or with any other corporation or corporations, (ii) the sale, conveyance or disposition of all or substantially all of the assets of the Target, (iii) the transfer of more than fifty percent (50%) of the voting power of the Target to any entity or entities not", "answers": "IALOG's license to possess and use the Deposit Materials does not include any right to disclose, market, sublicense or distribute the Deposit Materials to any third party other than its Affiliates and Manufacturing Subcontractors.", "length": 112674}
{"pred": "2. LICENSE.   2.1 License Grant. Subject to the restrictions set out in Section 2.2, ENERGOUS hereby grants to DIALOG a non-exclusive (subject to Section 2.5), irrevocable, worldwide, sub-licensable (solely in accordance with Section 2.4), royalty-bearing license during the Term under all Product IP to:   (a) repackage or have repackaged the Product Die into various package formats or layouts, and to integrate the Product Die into MCMs, which may incorporate DIALOG or third party intellectual property (such repack", "answers": "Subject to the restrictions set out in Section 2.2, ENERGOUS hereby grants to DIALOG a non-exclusive (subject to Section 2.5), irrevocable, worldwide, sub-licensable (solely in accordance with Section 2.4), royalty-bearing license during the Term under all Product IP to:", "length": 112669}
{"pred": "11.2 Product Warranty. (a)  ENERGOUS warrants that (i) when manufactured in accordance with the Approved Production Specifications, and as implemented in a suitable circuit application in accordance with the Product Specifications, the Products and Product Die will conform to the Product Specifications and will be free from defects that could have been avoided in their design; (ii) the Products, any constituent parts or functionality thereof, the Documentation and the Deposit Materials do not infringe any third party's Intellectual Property Rights; (iii) it did not misappropriate any third party's trade secrets in the development of the Products, any constituent parts or functionality thereof", "answers": "The above warranties are valid for a period of [***] from the date of shipment of any Licensed Product to any customer.", "length": 112671}
{"pred": "12.4 Insurance. Each party will maintain, during the Term and for three (3) years thereafter, such comprehensive general liability insurance (including without limitation, products liability) as will adequately protect it against its potential liabilities under this Agreement, in amounts customary in the semiconductor industry for similar services and products. Each party will, at the other party's request, provide to the other party a certificate of insurance evidencing the foregoing insurance coverage.", "answers": "Each party will, at the other party's request, provide to the other party a certificate of insurance evidencing the foregoing insurance coverage.", "length": 112665}
{"pred": "Yes, most federal student loans are eligible for at least one income-driven repayment plan. If your income is low enough, your payment could be as low as $0 per month. An income-driven repayment plan sets your monthly student loan payment at an amount that is intended to be affordable based on your income and family size. We offer four income-driven repayment plans: Revised Pay As You Earn Repayment Plan (REPAYE), Pay As You Earn Repayment Plan (PAYE), Income-Based Repayment Plan (IBR), and Income-Contingent Repayment Plan (ICR). If you'd like to repay your federal student loans", "answers": "You might be able to repayment it under an income-driven repayment plan but it depends on what type of loan you have. Do you have a FFEL Program Loan or a Federal Perkins Loan?", "length": 112167}
{"pred": "You may be able to consolidate your FFEL loan into a Direct Consolidation Loan and then repay the consolidation loan under the REPAYE, PAYE, or ICR plans. However, you may lose certain loan benefits if you consolidate a Federal Perkins Loan. Find out more about loan consolidation.", "answers": "You might be able to get a Direct Consolidation Loan and then you could repay under the REPAYE plan.", "length": 112152}
{"pred": "No. Employment with a tribal government is not considered qualifying employment for the Public Service Loan Forgiveness (PSLF) Program.", "answers": "You may be eligible for the Public Service Loan Forgiveness Program. Read on to see if you can qualify.", "length": 112171}
{"pred": "You can apply for an income-driven repayment plan that limits your payments to a percentage of your income. LOG IN TO START.", "answers": "To apply for an Income-Driven Repayment Plan you will have to complete the paper application and provide a paper copy of your most recently filed federal income tax return or IRS tax return transcript.", "length": 112166}
{"pred": "Borrowers not currently on an IDR Plan who are seeking a lower monthly payment on their federal student loans and Borrowers currently on an IDR Plan who need to re-certify or make changes to their repayment plan.", "answers": "Borrowers currently on an IDR Plan who need to recertify or make changes to their repayment plan.", "length": 112171}
{"pred": "Your REPAYE payment is a percentage of your discretionary income, which is the amount of money you have left over after paying for basic living expenses. The percentage is different depending on the plan. For the REPAYE Plan, your payment is 10% of your discretionary income. For the ICR Plan, your payment is the lesser of 20% of your discretionary income or the amount you would pay on a 12-year repayment plan. Your ICR payment is also the lesser of 20% of your discretionary income or the amount you would pay on a 12-year repayment plan. The chart below shows how payment amounts are", "answers": "Your payment is always based on your income and family size, regardless of any changes in your income.", "length": 112181}
{"pred": "Want more information about the income-driven repayment plans? Browse our Income-Driven Plans Questions and Answers page.", "answers": "You can browse our Income-Driven Plans question and answers page for more information.", "length": 112159}
{"pred": "Browse our Income-Driven Plans Questions and Answers page.", "answers": "On our website.", "length": 112155}
{"pred": "Ruth Honeywill.  She is a married woman, living with her husband, a traveller, who habitually ill-uses her.  She has two children, and her husband nearly strangled her on the morning of July 7th.  She has been living with her friend since then.  She has received money from her husband on Saturday, but it was a surprise to her, as her friend had been unable to give her money to get her away from her husband.  She has not seen her husband since the day he was taken away, and she is still in love with Falder.  She believes that Fal", "answers": ["Ruth Honeywill"], "length": 108371}
{"pred": "L = 6, L = 8, L = 14.", "answers": ["L = 8 and L = 14."], "length": 110889}
{"pred": "American.", "answers": ["She is an American."], "length": 111842}
{"pred": "They utilize a geolocated collection of blogs, written by individuals located in the U.S., with the purpose of creating insightful mappings of the blogging community. They compiled a very large geolocated collection of blogs, written by individuals located in the U.S., with the purpose of creating insightful mappings of the blogging community. They use the distribution of the individual words in a category, to compile distributions for the entire category, and therefore generate maps for these word categories. They also measure the usage of words related to people's core values as reported by Boyd et al. They use the Meaning Extraction Method (MEM) to excavate the", "answers": ["using the Meaning Extraction Method", "Unanswerable"], "length": 109223}
{"pred": "Infall rate is 2-5 times smaller depending on outer magnetization, and gas density is 2-5 times smaller in the region close to the black hole.", "answers": ["Infall rate is 2-5 times smaller and gas density is 2-5 times smaller."], "length": 102630}
{"pred": "Wyoming Valley, in a mountainous district, near the Wyoming Valley, in Pennsylvania.  The abandoned coal mine was located in a mountainous district, where several weeks before, a number of mining engineers had reported traces of carnotite and what they believed to be radioactive gases.  The mine was located in a deserted working in a mountainous district, where several weeks before, a number of mining engineers had reported traces of carnotite and what they believed to be radioactive gases.  The mine was located in a deserted working in a mountainous district, where several weeks before, a number of mining engineers had reported traces of car", "answers": ["Wyoming Valley"], "length": 110464}
{"pred": "at least 2×2 meters.  The room should be clear of any tripping or slipping hazards.  The headset should be used in an open and safe indoor area.  The room should be free of any single-colored walls, glass, mirrors, moving pictures or other similar objects.  The room should be bright.  The headset should be used in a space where the user can move around without any obstacles.  The headset should be used in a space where the user can stretch their arms without any danger.  The headset should be used in a space where the user can move around without any danger.  The headset should", "answers": ["It is recommended to have at least a 2x2 meter space for using the VR headset."], "length": 108772}
{"pred": "The main methodology used in the research is a compositional emergent communication paradigm, which exhibits clustering and informativeness properties. The paradigm is based on the information bottleneck to capture both referential complexity and task-specific utility to adequately explore sparse social communication scenarios in multi-agent reinforcement learning (MARL). The method uses a variational message generation setup, which maps the encoded hidden state h to a message m; that is, it models the posterior, π i m (m l |h). The method also uses a compositional complexity objective, which aims to induce complexity in the compositional messages, and a utility objective, which aims to satisfy the", "answers": ["An unsupervised method based on the information bottleneck and contrastive learning."], "length": 106137}
{"pred": "LSTM-CRF.", "answers": ["BERT"], "length": 101930}
{"pred": "The corpus covers dermatology, and the user queries are composed by randomly selecting several clinical terms from the dermatology glossary. The synthesized user queries are composed by randomly selecting several clinical terms from the dermatology glossary and then combining them in no particular order. The synthesized user queries (13,697 in total) are then split into train/dev/test set with the same ratio. Next, each set in the i2b2 data and the corresponding set in the synthesized query data are combined to form a hybrid train/dev/test set, respectively. This way we ensure that in each hybrid train/dev/test set, the ratio between the i2", "answers": "No specific domain is covered in the corpus.", "length": 111175}
{"pred": "Our ASR and ST models follow the architecture in berard2018end, but have 3 decoder layers like that in pino2019harnessing. For MT, we use a Transformer base architecture BIBREF15, but with 3 encoder layers, 3 decoder layers and 0.3 dropout. We use a batch size of 10,000 frames for ASR and ST, and a batch size of 4,000 tokens for MT. We train all models using Fairseq BIBREF20 for up to 200,000 updates. We use SpecAugment BIBREF21 for ASR and", "answers": "follow the architecture in berard2018end, but have 3 decoder layers like that in pino2019harnessing", "length": 111175}
{"pred": "The dataset was collected by searching individual states on the profile finder on http://www.blogger.com. Starting with this list, we can locate the profile page for a user, and subsequently extract additional information, which includes fields such as name, email, occupation, industry, and so forth. It is important to note that the profile finder only identifies users that have an exact match to the location specified in the query; we thus built and ran queries that used both state abbreviations (e.g., TX, AL), as well as the states' full names (e.g., Texas, Alabama). After completing all the processing steps, we identified", "answers": "Contributors record voice clips by reading from a bank of donated sentences.", "length": 111173}
{"pred": "French, German, Dutch, Russian, Spanish, Italian, Turkish, Persian, Swedish, Mongolian, Chinese.", "answers": "French, German, Dutch, Russian, Spanish, Italian, Turkish, Persian, Swedish, Mongolian and Chinese", "length": 111175}
{"pred": "The quality of the data is evaluated by applying various sanity checks to the translations. The checks include computing sentence-level BLEU with the NLTK implementation between the human translations and the automatic translations produced by a state-of-the-art system. Translations with a score that is too low are manually inspected and sent back to the translators when needed. The translations are also manually inspected for examples where the source transcript is identical to the translation. The perplexity of the translations is measured using a language model trained on a large amount of clean monolingual data. The ratio of English characters in the translations is also computed and manually inspected for examples with a low", "answers": "Validated transcripts were sent to professional translators., various sanity checks to the translations,  sanity check the overlaps of train, development and test sets", "length": 111179}
{"pred": "No.", "answers": "No", "length": 111178}
{"pred": "No.", "answers": "No", "length": 111180}
{"pred": "30,223.", "answers": ["30,223."], "length": 103865}
{"pred": "6946 sentences and 16225 unique words extracted from POS-tagged Nepali National Corpus.", "answers": ["3606", "6946"], "length": 111842}
{"pred": "We also implemented a baseline model based on conditional random fields (CRF), where features like POS tags produced by the Stanford POS tagger, n-grams, label transitions, word suffixes and relative position to the end of the text are considered. We can see that our model with the INLINEFORM0 tagging scheme yields new state-of-the-art INLINEFORM1 scores on pun detection and competitive results on pun location, compared to baselines that do not adopt joint learning in the first block. For location on heterographic puns, our model's performance is slightly lower than the system of BIBREF25, which is a rule-based locator", "answers": ["They compare with the following models: by Pedersen (2017), by Pramanick and Das (2017), by Mikhalkova and Karyakin (2017),  by Vadehra (2017), Indurthi and Oota (2017), by Vechtomova (2017), by (Cai et al., 2018), and CRF."], "length": 101664}
{"pred": "Lepas, the husband of Madame Lepas, is the owner of the manor.", "answers": ["Madame de Merret"], "length": 109185}
{"pred": "700.", "answers": ["Unanswerable", "Unanswerable"], "length": 109824}
{"pred": "romance novels and women's fiction.", "answers": ["Romance novels and women's fiction."], "length": 111954}
{"pred": "He wants to be a governor.", "answers": ["Governor"], "length": 101219}
{"pred": "Chinese general corpus.", "answers": "Chinese general corpus", "length": 104064}
{"pred": "QANet and BERT-Base.", "answers": "BERT-Base, QANet", "length": 104065}
{"pred": "Clinical text structuring (CTS) can be regarded to extract or generate a key-value pair where key Q is typically a query term such as proximal resection margin and value V is a result of query term Q according to the paragraph text X. Generally, researchers solve CTS problem in two steps. Firstly, the answer-related text is pick out. And then several steps such as entity names conversion and negative words recognition are deployed to generate the final answer. While final answer varies from task to task, which truly causes non-uniform output formats, finding the answer-related text is a common action among all tasks. Traditional methods regard both the", "answers": "Clinical text structuring (CTS) is a critical task for fetching medical research data from electronic health records (EHRs), where structural patient medical data, such as whether the patient has specific symptoms, diseases, or what the tumor size is, how far from the tumor is cut at during the surgery, or what the specific laboratory test result is, are obtained., Unlike the traditional CTS task, our QA-CTS task aims to discover the most related text from original paragraph text. ", "length": 104065}
{"pred": "Clinical text structuring (CTS) can be regarded to extract or generate a key-value pair where key Q is typically a query term such as proximal resection margin and value V is a result of query term Q according to the paragraph text X. Generally, researchers solve CTS problem in two steps. Firstly, the answer-related text is pick out. And then several steps such as entity names conversion and negative words recognition are deployed to generate the final answer. While final answer varies from task to task, which truly causes non-uniform output formats, finding the answer-related text is a common action among all tasks. Traditional methods regard both the", "answers": "CTS is extracting structural data from medical research data (unstructured). Authors define QA-CTS task that aims to discover most related text from original text.", "length": 104065}
{"pred": "tumor size, proximal resection margin, and distal resection margin.", "answers": " three types of questions, namely tumor size, proximal resection margin and distal resection margin", "length": 104063}
{"pred": "The text in this dataset is not all questions, but rather a mix of questions and unrelated sentences. The questions are typically marked with a query term such as \"proximal resection margin\" and the answer is a result of the query term according to the paragraph text. However, there are also unrelated sentences in between questions, which are not marked as questions. These sentences are part of the original paragraph text and are not necessarily related to the question that follows. Therefore, the text in this dataset is not all questions, but rather a mix of questions and unrelated sentences.", "answers": "the dataset consists of pathology reports including sentences and questions and answers about tumor size and resection margins so it does include additional sentences ", "length": 104073}
{"pred": "2,714 question-answer pairs.", "answers": "2,714 ", "length": 104063}
{"pred": "They introduce clinical named entity information into the model. The CNER task aims to identify and classify important clinical terms such as diseases, symptoms, treatments, exams, and body parts from Chinese EHRs. It can be regarded as a sequence labeling task. A CNER model typically outputs a sequence of tags. Each character of the original sentence will be tagged a label following a tag scheme. In this paper, they recognize the entities by the model of their previous work but trained on another corpus which has 44 entity types including operations, numbers, unit words, examinations, symptoms, negative words, etc. An illustrative example of named entity", "answers": "integrate clinical named entity information into pre-trained language model", "length": 104067}
{"pred": "17,833 sentences, 826,987 characters and 2,714 question-answer pairs.", "answers": "17,833 sentences, 826,987 characters and 2,714 question-answer pairs", "length": 104064}
{"pred": " QANet and BERT-Base.", "answers": "state-of-the-art question answering models (i.e. QANet BIBREF39) and BERT-Base BIBREF26", "length": 104064}
{"pred": "The datasets used are the Honeypot dataset and the Weibo dataset. The Honeypot dataset contains 2218 legitimate users and 2947 spammers, and the Weibo dataset contains 2197 legitimate users and 802 spammers.", "answers": ["Custom dataset with user questions; set of documents, twitter posts and news articles, all related to finance.", "a self-collected financial intents dataset in Portuguese"], "length": 110001}
{"pred": "When the shock wave passes through the bubble (t > 0.03), a larger specific-heat ratio speeds up the average motion of the bubbles. The reason is that the bubbles with smaller specific-heat ratio need more energy to compress their size, so their translational energy is smaller.", "answers": ["The specific-heat ratio affects the average motion of the bubble. The bubbles with smaller specific-heat ratios have slower average motion."], "length": 108606}
{"pred": "Because he believes that he has entered into an implied contract with the laws of Athens, and that breaking this contract would be unjust and dishonorable. He also believes that the laws of Athens have been good to him and have given him a good life, and that he should not betray them. Furthermore, he believes that if he escapes, he will be acting out of fear and not out of a desire to do what is right, and that this would be a cowardly act. He also believes that the laws of Athens will be angry with him if he breaks his agreement with them, and that his friends will be ashamed of him if", "answers": ["Because it violates his belief in the  laws of society."], "length": 107313}
{"pred": "They use a bilingual dictionary (Google Translate word translation in our case). In an end-to-end solution, it would have been ideal to use bilingual embeddings or obtain word-by-word translations via bilingual embeddings BIBREF14. But, the quality of publicly available bilingual embeddings for English-Indian languages is very low for obtaining good-quality, bilingual representations BIBREF26, BIBREF27. We also found that these embeddings were not useful for transfer learning.", "answers": ["Unanswerable", "CFILT-preorder system"], "length": 98083}
{"pred": "A compact, scalable and meaningful representation of the whole set. \nA compact, scalable and meaningful representation of the whole set. \nA compact, scalable and meaningful representation of the whole set. \nA compact, scalable and meaningful representation of the whole set. \nA compact, scalable and meaningful representation of the whole set. \nA compact, scalable and meaningful representation of the whole set. \nA compact, scalable and meaningful representation of the whole set. \nA compact, scalable and meaningful representation of the whole set. \nA compact, scalable and meaningful representation of the whole set. \nA compact, scalable and meaningful representation of the whole set", "answers": ["Word vectors, usually in the context of others within the same class"], "length": 107901}
{"pred": "human nature, which he finds is the most interesting thing in the world, and it only reveals itself to the truly earnest seeker.", "answers": ["The beauty of the women"], "length": 112423}
{"pred": "The dynamical behavior of the anisotropic order parameter following a quench to the critical point is well described by the Gaussian theory for all the three lattice gas models studied, i.e., driven lattice gas with either constant (IDLG) or random (RDLG) infinite drive and equilibrium lattice gas (LG). In other words, in the short-time regime, m ∼ t^(1/2) and the Binder cumulant g of the lowest transverse mode is zero in this regime. The alternative order parameter O, however, distinguishes between the driven (IDLG, RDLG) and the equilibrium (LG)", "answers": ["It is well described by the Gaussian theory."], "length": 102928}
{"pred": "pediatric vaccines had been trialed against genuine placebo, and came to a negative conclusion. None of the vaccines those vaccines had been trialed against had ever been trialed against genuine placebo either. At the end of the line the toxic products were only being compared with other toxic products, rather than against saline.  The manufacturers apparently had no faith that their proto-products could withstand this scrutiny, and for the rest they just did not care: under the alleged imperative of protecting the population it seems anything went. So even before all the sham monitoring procedures and reviews which Del and his team dismantle in forensic detail we are left with the proposition that none", "answers": ["Other toxic products."], "length": 108516}
{"pred": "2013.", "answers": ["Margaret Way started self-publishing her books as e-books in 2013."], "length": 111026}
{"pred": "Yes, previous work examined the extent to which RNN-based models capture syntax-sensitive phenomena that are traditionally taken as evidence for the existence in hierarchical structure. In particular, in BIBREF1 we assess the ability of LSTMs to learn subject-verb agreement patterns in English, and evaluate on naturally occurring wikipedia sentences. BIBREF2 also consider subject-verb agreement, but in a “colorless green ideas” setting in which content words in naturally occurring sentences are replaced with random words with the same part-of-speech and inflection, thus ensuring a focus on syntax rather than on selectional-preferences based cues. BIBREF", "answers": ["Yes", "Yes"], "length": 112053}
{"pred": "LSTM, RNNG, LSTM or a Recurrent Neural Network Grammar (RNNG) language model, CNN, CNN or a Convolutional Neural Network, LSTM or a Recurrent Neural Network Grammar (RNNG) language model, LSTM, RNNG, LSTM or a Recurrent Neural Network Grammar (RNNG) language model, CNN, CNN or a Convolutional Neural Network, LSTM or a Recurrent Neural Network Grammar (RNNG) language model, LSTM, RNNG, LSTM or a Recurrent Neural Network Grammar (RNNG) language model, CNN, CNN or a Convolutional Neural Network, LSTM or a Rec", "answers": ["linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM trained on word unigrams,  bidirectional Long Short-Term-Memory (BiLSTM),  Convolutional Neural Network (CNN) "], "length": 109090}
{"pred": "There is no information about Laura attending a medical school in the passage. The passage is about a doctor named Nordenfeld who is on a spaceship and is dealing with a possible outbreak of chlorophage. The passage does not mention a person named Laura.", "answers": ["Laura attends Harvard Medical school."], "length": 109310}
{"pred": "The first baseline is a name-based Nearest-Neighbor model (NN). The second baseline is a simple Encoder-Decoder baseline with ingredient attention (Enc-Dec), which provides comparable performance and lower complexity.  The third baseline is a Neural Checklist Model of BIBREF0.  The fourth baseline is a simple Encoder-Decoder baseline with ingredient attention (Enc-Dec), which provides comparable performance and lower complexity.  The fifth baseline is a Neural Checklist Model of BIBREF0.  The sixth baseline is a simple Encoder-Decoder baseline with ingredient attention (Enc-Dec), which provides comparable performance and lower complexity.  The seventh baseline is a", "answers": ["name-based Nearest-Neighbor model (NN), Encoder-Decoder baseline with ingredient attention (Enc-Dec)"], "length": 108239}
{"pred": "environmental fluctuation and uncertainty.", "answers": ["Environmental fluctuation and uncertainty control the reliance of artificial organisms on plasticity."], "length": 108533}
{"pred": " Phylicia Rashad played Clair Huxtable, the mother on the Cosby Show.", "answers": "Phylicia Rashād", "length": 103722}
{"pred": " Blair Underwood played Denise's boyfriend in the episode \"Jitterbug Break\" (Season 1), and Mark (Season 2).", "answers": "Joseph C. Phillips", "length": 103723}
{"pred": "Yes, Sondra and Elvin get married in season 4.", "answers": "season four", "length": 103724}
{"pred": "Tempestt Bledsoe played Vanessa Huxtable, Malcolm-Jamal Warner played Theodore Huxtable, and Keshia Knight Pulliam played Rudy Huxtable. The character of Gabe, who was Jacob's son, was played by a young boy named Gabe, but his last name was not mentioned in the passage.", "answers": "Malcolm - Jamal Warner", "length": 103725}
{"pred": "Masking words in the decoder is helpful because it allows the model to utilize BERT's ability to generate high-quality context vectors, which is not possible when using the embedding matrix alone. This is because the embedding matrix alone may not be able to learn to model representations as well as vocabulary probabilities from a relatively small corpus compared to BERT's huge pre-training corpus. By masking words in the decoder, the model can generate a more complete input sequence that is consistent with BERT's pre-training processes, allowing it to generate more fluent and natural sequences. This is similar to the cloze task in BERT's pre-train process, where", "answers": ["ecause this process is similar to the cloze task in BERT's pre-train process, therefore by using the ability of the contextual language model the decoder can generate more fluent and natural sequences."], "length": 103134}
{"pred": "English.", "answers": ["English", "Simple English"], "length": 109809}
{"pred": "Long Term Capital Management (LTCM) was near collapse in 1998. Using mathematical models to calculate debt risk, LTCM used derivatives to leverage $5 billion into more than $1 trillion, doing business with fifteen of Wall Street's largest financial institutions. The derivative transactions were not regulated, nor were investors able to evaluate LTCM's exposures. Born stated, \"I thought that LTCM was exactly what I had been worried about\". In the last weekend of September 1998, the President's working group was told that the entire American economy hung in the balance. After intervention by the Federal Reserve, the crisis was averted", "answers": ["Long Term Capital Management (LTCM)."], "length": 110205}
{"pred": "The dynamical behavior of the anisotropic order parameter m following a quench to the critical point is well described by the Gaussian theory for all the three lattice gas models studied, i.e., driven lattice gas with either constant (IDLG) or random (RDLG) infinite drive and equilibrium lattice gas (LG). In other words, in the short-time regime, m ∼ t^(1/2) and the Binder cumulant g of the lowest transverse mode is zero in this regime. The alternative order parameter O, however, distinguishes between the driven (IDLG, RDLG) and the equilibrium (LG", "answers": ["It is well described by the Gaussian theory."], "length": 108952}
{"pred": "The best performing model among author's submissions is the full UTCNN model, which achieved a macro-average f-score of 0.673.", "answers": ["For SLC task, the \"ltuorp\" team  has the best performing  model (0.6323/0.6028/0.6649 for F1/P/R  respectively) and for FLC task the \"newspeak\" team  has the best performing  model (0.2488/0.2863/0.2201 for F1/P/R respectively)."], "length": 109575}
{"pred": "Ruth Honeywill.", "answers": ["Ruth Honeywill"], "length": 109422}
{"pred": "SNLI, MultiNLI, Quora Question Pairs, and SST.", "answers": ["Stacked LSTMs, Cell-aware Stacked LSTMs, Sentence Encoders, Top-layer Classifiers"], "length": 109314}
{"pred": "C-295, IL-214, Antonov An-148, BAe-146, Sukhoi Superjet, MS-21, Wide body follow on, Mi-38, MRTA, FGFA.", "answers": ["The IAF is considering the acquisition of the Airbus A330 MRTT (Multi-Role Tanker Transport) besides the Boeing C-17."], "length": 107479}
{"pred": "The time required to update the robot's belief does not increase with the complexity of the environment.", "answers": ["The time required to update the belief does not increase with the complexity of the environment."], "length": 112545}
{"pred": "The dataset is annotated with 7 feature groups with associated feature values binarized (i.e., present=1 or absent=0) to represent potentially informative features for classifying depression-related classes. The feature groups are: lexical features, unigrams, e.g., “depressed”; syntactic features, parts of speech, e.g., “cried” encoded as V for verb; emotion features, emoticons, e.g., :( encoded as SAD; demographic features, age and gender e.g., “this semester” encoded as an indicator of 19-22 years of age and “my girlfriend” encoded as an indicator", "answers": ["no evidence of depression, depressed mood, disturbed sleep, fatigue or loss of energy", "The annotations are based on evidence of depression and further annotated by the depressive symptom if there is evidence of depression"], "length": 104790}
{"pred": "The baseline was a weak baseline without using any monolingual data, and #10 in Table TABREF33, a strong baseline established with monolingual data.", "answers": ["pivot-based translation relying on a helping language BIBREF10, nduction of phrase tables from monolingual data BIBREF14 , attentional RNN-based model (RNMT) BIBREF2, Transformer model BIBREF18, bi-directional model BIBREF11, multi-to-multi (M2M) model BIBREF8, back-translation BIBREF17", "M2M Transformer"], "length": 101560}
{"pred": "2013–2014.", "answers": ["In the 2013-2014 season."], "length": 113073}
{"pred": "The real advantage is one of flexibility; as costs drop and the market expands, it will be increasingly necessary for vendors like HTC to offer a wide range of phones based on radically different SoCs.", "answers": ["Flexibility."], "length": 109287}
{"pred": "HEOT, HEOT obtained from one of the past studies done by Mathur et al. where they annotated a set of cleaned tweets obtained from twitter for the conversations happening in Indian subcontinent. A labelled dataset for a corresponding english tweets were also obtained from a study conducted by Davidson et al. This dataset was important to employ Transfer Learning to our task since the number of labeled dataset was very small. Basic summary and examples of the data from the dataset are below: Dataset and Features ::: Challenges\nThe obtained data set had many challenges and thus a data preparation task was employed to clean the data and make it ready for the deep learning pipeline.", "answers": ["Custom dataset with user questions; set of documents, twitter posts and news articles, all related to finance.", "a self-collected financial intents dataset in Portuguese"], "length": 112943}
{"pred": "We build a dataset of Twitter accounts based on two lists annotated in previous works. For the non-factual accounts, we rely on a list of 180 Twitter accounts from BIBREF1. This list was created based on public resources where suspicious Twitter accounts were annotated with the main fake news types (clickbait, propaganda, satire, and hoax). We discard the satire labeled accounts since their intention is not to mislead or deceive. On the other hand, for the factual accounts, we use a list with another 32 Twitter accounts from BIBREF19 that are considered trustworthy by independent third parties. We discard some accounts that publish", "answers": "public resources where suspicious Twitter accounts were annotated, list with another 32 Twitter accounts from BIBREF19 that are considered trustworthy", "length": 104286}
{"pred": "relu, selu, tanh.", "answers": "relu, selu, tanh", "length": 104289}
{"pred": "LR + Bag-of-words: We aggregate the tweets of a feed and we use a bag-of-words representation with a logistic regression (LR) classifier.\nTweet2vec: We use the Bidirectional Gated recurrent neural network model proposed in BIBREF20. We keep the default parameters that were provided with the implementation. To represent the tweets, we use the decoded embedding produced by the model. With this baseline we aim at assessing if the tweets' hashtags may help detecting the non-factual accounts.\nLR + All Features (tweet-level): We extract all our features from each tweet and feed them into a LR classifier. Here,", "answers": "Top-$k$ replies, likes, or re-tweets, FacTweet (tweet-level), LR + All Features (chunk-level), LR + All Features (tweet-level), Tweet2vec, LR + Bag-of-words", "length": 104287}
{"pred": "Chunks are defined as a sorted sequence of tweets labeled by the label of its corresponding account, and each chunk consists of a sorted sequence of tweets.  Each chunk consists of a sorted sequence of tweets labeled by the label of its corresponding account. We sort the tweets by the posting date in ascending way and split them into N chunks.  Each chunk consists of a sorted sequence of tweets labeled by the label of its corresponding account.  We extract a set of features from each chunk and we feed them into a recurrent neural network to model the sequential flow of the chunks' tweets.  We use an attention layer with dropout to attend over the most", "answers": "Chunks is group of tweets from single account that  is consecutive in time - idea is that this group can show secret intention of malicious accounts.", "length": 104284}
{"pred": "Emotion: We build an emotions vector using word occurrences of 15 emotion types from two available emotional lexicons. We use the NRC lexicon BIBREF10, which contains $\\sim $14K words labeled using the eight Plutchik's emotions BIBREF11. The other lexicon is SentiSense BIBREF12 which is a concept-based affective lexicon that attaches emotional meanings to concepts from the WordNet lexical database. It has $\\sim $5.5 words labeled with emotions from a set of 14 emotional categories We use the categories that do not exist in the NRC lexicon.\nSentiment", "answers": "Sentiment, Morality, Style, Words embeddings", "length": 104284}
{"pred": "Emotion: We build an emotions vector using word occurrences of 15 emotion types from two available emotional lexicons. We use the NRC lexicon BIBREF10, which contains $\\sim $14K words labeled using the eight Plutchik's emotions BIBREF11. The other lexicon is SentiSense BIBREF12 which is a concept-based affective lexicon that attaches emotional meanings to concepts from the WordNet lexical database. It has $\\sim $5.5 words labeled with emotions from a set of 14 emotional categories We use the categories that do not exist in the NRC lexicon.\nSentiment", "answers": "15 emotion types, sentiment classes, positive and negative, care, harm, fairness, cheating, loyalty, betrayal, authority, subversion, sanctity, and degradation, count of question marks, exclamation marks, consecutive characters and letters, links, hashtags, users' mentions, uppercase ratio, tweet length, words embeddings", "length": 104284}
{"pred": "No. The approach used in this work to detect fake news is a semi-supervised approach, where the model is trained on a labeled dataset and then fine-tuned on a larger unlabeled dataset. The labeled dataset is used to learn the patterns of non-factual accounts, and the unlabeled dataset is used to adapt the model to the specific characteristics of the accounts. The approach is semi-supervised because it uses both labeled and unlabeled data to train the model. The labeled data is used to learn the general patterns of non-factual accounts, and the unlabeled data is used to adapt the model to the specific characteristics of the accounts", "answers": "Yes", "length": 104293}
{"pred": "words embeddings, style, and morality features.", "answers": "words embeddings, style, and morality features", "length": 104297}
{"pred": "The dataset used in this work is a collection of tweets from Twitter, with a total of 180 Twitter accounts labeled as non-factual and 32 Twitter accounts labeled as factual. The dataset is unbalanced, with the non-factual accounts making up the majority of the data. The dataset is also filtered to remove duplicate, media-based, and link-only tweets. The maximum amount of tweets allowed by Twitter API is collected for each account. The dataset is split into training and testing sets using a 5-fold cross-validation approach. The training set contains 80% of the data, and the testing set contains 20%. The dataset is", "answers": "Total dataset size: 171 account (522967 tweets)", "length": 104289}
{"pred": "A chunk of posts is defined as a sorted sequence of tweets labeled by the label of its corresponding account. Each chunk consists of a sorted sequence of tweets.  We extract a set of features from each chunk and we feed them into a recurrent neural network to model the sequential flow of the chunks' tweets.  We use an attention layer with dropout to attend over the most important tweets in each chunk.  Finally, the representation is fed into a softmax layer to produce a probability distribution over the account types and thus predict the factuality of the accounts.  Since we have many chunks for each account, the label for an account is obtained by", "answers": "chunk consists of a sorted sequence of tweets labeled by the label of its corresponding account", "length": 104292}
{"pred": "LR + Bag-of-words: We aggregate the tweets of a feed and we use a bag-of-words representation with a logistic regression (LR) classifier.\nTweet2vec: We use the Bidirectional Gated recurrent neural network model proposed in BIBREF20. We keep the default parameters that were provided with the implementation. To represent the tweets, we use the decoded embedding produced by the model. With this baseline we aim at assessing if the tweets' hashtags may help detecting the non-factual accounts.\nLR + All Features (tweet-level): We extract all our features from each tweet and feed them into a LR classifier. Here,", "answers": "LR + Bag-of-words, Tweet2vec, LR + All Features (tweet-level), LR + All Features (chunk-level), FacTweet (tweet-level), Top-$k$ replies, likes, or re-tweets", "length": 104288}
{"pred": "BLEU-1, BLEU-4, ROUGE-L, BLEU-1 with more key entities, BLEU-4, Distinct-1/2, F1 score, recipe-level coherence, step entailment, human evaluation, pairwise comparison, and pairwise ranking. \n\nQuestion: What is the main goal of the proposed approach?\nAnswer: The main goal of the proposed approach is to generate plausible and personalized recipes from incomplete input specifications by leveraging historical user preferences.\n\nQuestion: What is the name of the dataset used for training and evaluation?\nAnswer: Food.com\n\nQuestion: What is the name of the model used for generating", "answers": ["Byte-Pair Encoding perplexity  (BPE PPL),\nBLEU-1,\nBLEU-4,\nROUGE-L,\npercentage of distinct unigram (D-1),\npercentage of distinct bigrams(D-2),\nuser matching accuracy(UMA),\nMean Reciprocal Rank(MRR)\nPairwise preference over baseline(PP)", "BLEU-1/4 and ROUGE-L, likelihood of generated recipes using identical input specifications but conditioned on ten different user profiles, user matching accuracy (UMA), Mean Reciprocal Rank (MRR), neural scoring model from BIBREF33 to measure recipe-level coherence", " Distinct-1/2, UMA = User Matching Accuracy, MRR\n= Mean Reciprocal Rank, PP = Pairwise preference over baseline (evaluated for 310 recipe pairs per model)"], "length": 114299}
{"pred": "Our model's performance is compared to the following systems, for which results are reported in the referred literature. Please note that no other existing model was re-implemented, and results are those reported in the respective papers. Our model's performance is competitive or even state-of-the-art results for some of the emotion labels on existing, standard evaluation datasets. For explanatory purposes, related work is discussed further and more in detail when we describe the benchmarks for evaluation (Section SECREF3 ) and when we compare our models to existing ones (Section SECREF5 ). We also explore and discuss how choosing different sets of Facebook pages as training data provides an intrinsic", "answers": ["Answer with content missing: (Table 3) Best author's model B-M average micro f-score is 0.409, 0.459, 0.411 on Affective, Fairy Tales and ISEAR datasets respectively. "], "length": 107971}
{"pred": "The Carnegie Mellon Pronouncing Dictionary, the multilingual pronunciation corpus collected by deri2016grapheme, Wiktionary, Phoible, URIEL, WALS.", "answers": ["the Carnegie Mellon Pronouncing Dictionary BIBREF12, the multilingual pronunciation corpus collected by deri2016grapheme , ranscriptions extracted from Wiktionary", "multilingual pronunciation corpus collected by deri2016grapheme"], "length": 101806}
{"pred": "O (t, L_{\\parallel} ; S_\\Delta) = L_{\\parallel}^{-\\beta/[\\nu(1+\\Delta)]} \\tilde f_O (t/L_{\\parallel}^{z/(1+\\Delta)} ; S_\\Delta).", "answers": ["O(t, L_{\\parallel}; S_\\Delta) = L_{\\parallel}^{-\\beta/[\\nu(1+\\Delta)]} \\tilde f_O(t/L_{\\parallel}^{z/(1+\\Delta)}; S_\\Delta)."], "length": 111317}
{"pred": "CNN and LSTM variants, rule-based pattern matching, and a combination of rule-based and machine learning approaches.", "answers": ["bag-of-words model, CNN"], "length": 111747}
{"pred": "Yes, the pipeline components were based on deep learning models, specifically the named entity recognition module used Freeling, which resorts to a Hidden Markov Model, and the dependency parsing model was trained using a dataset from System-T, which has SRL tags, and the semantic role labeling module was trained on top of the dependency parser using the modified dataset from System-T.", "answers": ["No", "No"], "length": 108816}
{"pred": "LTCM (Long Term Capital Management)  was near collapse in 1998.  Using mathematical models to calculate debt risk, LTCM used derivatives to leverage $5 billion into more than $1 trillion, doing business with fifteen of Wall Street's largest financial institutions.  The derivative transactions were not regulated, nor were investors able to evaluate LTCM's exposures.  Born stated, \"I thought that LTCM was exactly what I had been worried about\".  In the last weekend of September 1998, the President's working group was told that the entire American economy hung in the balance.  After intervention by the Federal Reserve", "answers": ["Long Term Capital Management (LTCM)."], "length": 110578}
{"pred": "He was worried that Ruth would give him up after he had been given a chance by James and Walter.", "answers": ["Her husband abused her"], "length": 107997}
{"pred": "He was accused of murdering his wife.  Bennett was his lawyer.  He was defending him in court.  He was accused of murdering his wife.  Bennett was his lawyer.  He was defending him in court.  He was accused of murdering his wife.  Bennett was his lawyer.  He was defending him in court.  He was accused of murdering his wife.  Bennett was his lawyer.  He was defending him in court.  He was accused of murdering his wife.  Bennett was his lawyer.  He was defending him in court.  He was accused of murdering his wife.  Bennett was his lawyer", "answers": ["because he's a lawyer"], "length": 111514}
{"pred": "If an income-driven repayment plan is right for you, you'll want to choose the plan that provides the most benefit to you based on your individual circumstances. All four income-driven plans allow you to make a monthly payment based on your income, but they differ in terms of who qualifies, how much you have to pay each month, the length of the repayment period, and the types of loans that can be repaid under the plan. If you have only Direct Loans, you can choose from all four income-driven repayment plans. If you're not sure which plan to choose, you have the option of requesting the income-driven plan that provides the", "answers": "You might be able to repayment it under an income-driven repayment plan but it depends on what type of loan you have. Do you have a FFEL Program Loan or a Federal Perkins Loan?", "length": 112248}
{"pred": "You are not eligible for income-driven repayment plans. However, you may be eligible for Public Service Loan Forgiveness (PSLF) if you work for a qualifying employer and make 120 qualifying payments. You can check your eligibility for PSLF by using the PSLF Help Tool on the Federal Student Aid website. If you are eligible, you can submit an Employment Certification Form to the PSLF servicer, FedLoan Servicing, to confirm your employment and make payments. You can also contact FedLoan Servicing directly to discuss your options.", "answers": "You might be able to get a Direct Consolidation Loan and then you could repay under the REPAYE plan.", "length": 112233}
{"pred": "If you are employed by a U.S. federal, state, local, or tribal government or not-for-profit organization, you might be eligible for the Public Service Loan Forgiveness (PSLF) Program. To qualify, you must work full-time for a qualifying employer, have Direct Loans or consolidate other federal student loans into a Direct Loan, and make 120 qualifying payments under a qualifying repayment plan. You can check your eligibility and submit an Employment Certification Form to ensure you're on the right track. If you're unsure, you can also use the PSLF Help Tool to assess your eligibility and determine the next steps.", "answers": "You may be eligible for the Public Service Loan Forgiveness Program. Read on to see if you can qualify.", "length": 112252}
{"pred": "Before you apply for an income-driven repayment plan, contact your loan servicer if you have any questions. Your loan servicer will help you decide whether one of these plans is right for you. To apply, you must submit an application called the Income-Driven Repayment Plan Request. You can submit the application online or on a paper form, which you can get from your loan servicer. The application allows you to select an income-driven repayment plan by name, or to request that your loan servicer determine what income-driven plan or plans you qualify for, and to place you on the income-driven plan with the lowest monthly payment amount.", "answers": "To apply for an Income-Driven Repayment Plan you will have to complete the paper application and provide a paper copy of your most recently filed federal income tax return or IRS tax return transcript.", "length": 112247}
{"pred": "The borrower.", "answers": "Borrowers currently on an IDR Plan who need to recertify or make changes to their repayment plan.", "length": 112252}
{"pred": "The REPAYE Plan payment amount will be based on your income and family size when you first begin making payments, and at any time when your income is low enough that your calculated monthly payment amount would be less than the amount you would have to pay under the 10-year Standard Repayment Plan. If your income ever increases to the point that your calculated monthly payment amount would be more than what you would have to pay under the 10-year Standard Repayment Plan, you'll remain on the REPAYE Plan, but your payment will no longer be based on your income. Instead, your required monthly payment will be the amount you would", "answers": "Your payment is always based on your income and family size, regardless of any changes in your income.", "length": 112262}
{"pred": "Want more information about the income-driven repayment plans? Browse our Income-Driven Plans Questions and Answers page.", "answers": "You can browse our Income-Driven Plans question and answers page for more information.", "length": 112240}
{"pred": "You can find these pages at https://github.com/bepnye/EBM-NLP.", "answers": "On our website.", "length": 112236}
{"pred": "The benchmark dataset is the Social Honeypot dataset, which was created and deployed by Lee et al. lee2010devils. The dataset contains 19,276 legitimate users and 22,223 spammers, along with their tweet content in 7 months. The quality of the dataset is high, as it was collected through a controlled experiment and has a large number of samples. However, the dataset has been slashed because most of the Twitter accounts only have limited number of posts, which are not enough to show their interest inclination. The Weibo dataset, which is another dataset used in the paper, contains 2197 legitimate", "answers": ["Social Honeypot dataset (public) and Weibo dataset (self-collected); yes", "Social Honeypot, which is not of high quality"], "length": 104867}
{"pred": "FC Banants.", "answers": ["FC Banants."], "length": 108703}
{"pred": "Jivani Street 2 of the Malatia-Sebastia District, Yerevan.", "answers": ["The club's headquarters are located on Jivani Street 2 of the Malatia-Sebastia District, Yerevan."], "length": 113724}
{"pred": "Brisbane, Cleveland, Queensland, Australia.", "answers": ["Margaret Way was born in Brisbane and died in Cleveland, Queensland, Australia."], "length": 99600}
{"pred": "BioBERTv1.0 was adapted for ten days on eight large GPUs, which is expensive, environmentally unfriendly, prohibitive for small research labs and students, and may delay prototyping on emerging domains. We therefore propose a fast, CPU-only domain-adaptation method for PTLMs: We train Word2Vec on target-domain text and align the resulting word vectors with the wordpiece vectors of an existing general-domain PTLM. The PTLM thus gains domain-specific lexical knowledge in the form of additional word vectors, but its deeper layers remain unchanged. In this paper, we use the proposed method to domain-adapt BERT on", "answers": ["MEDDOCAN, NUBes-PHI", "MEDDOCAN, NUBes "], "length": 109097}
{"pred": "Lady Coxon, the aunt, had been established here for years in consequence of her marriage with the late Sir Gregory of that name. She had a house in the Regent’s Park, a Bath-chair and a fernery; and above all she had sympathy. Mrs. Saltram had made her acquaintance through mutual friends. This vagueness caused me to feel how much I was out of it and how large an independent circle Mrs. Saltram had at her command. I should have been glad to know more about the disappointed young lady, but I felt I should know most by not depriving her of her advantage, as she might", "answers": ["Ruth Anvoy"], "length": 107880}
{"pred": "Facebook, Twitter.", "answers": ["Facebook status update messages", "Facebook status update messages"], "length": 103388}
{"pred": "The corpus covers a range of domains, including but not limited to, \n- 11 languages into English, diversified with over 11,000 speakers and over 60 accents, \n- 18 hours of French (Fr), \n- 171 hours of French (Fr), \n- 327 hours of German (De), \n- 171 hours of German (De), \n- 12 hours of Dutch (Nl), \n- 6 hours of Dutch (Nl), \n- 27 hours of Italian (It), \n- 37 hours of Italian (It), \n- 67 hours of Persian (Fa),", "answers": "No specific domain is covered in the corpus.", "length": 106572}
{"pred": "Our ASR and ST models follow the architecture in berard2018end, but have 3 decoder layers like that in pino2019harnessing. For MT, we use a Transformer base architecture BIBREF15, but with 3 encoder layers, 3 decoder layers and 0.3 dropout. We use a batch size of 10,000 frames for ASR and ST, and a batch size of 4,000 tokens for MT. We train all models using Fairseq BIBREF20 for up to 200,000 updates. We use SpecAugment BIBREF21 for ASR and", "answers": "follow the architecture in berard2018end, but have 3 decoder layers like that in pino2019harnessing", "length": 106572}
{"pred": "The dataset was collected from various sources, including Common Voice, a crowdsourcing speech recognition corpus, and Tatoeba, a community-built language learning corpus. The Common Voice dataset was collected through a crowdsourcing process where contributors recorded voice clips by reading from a bank of donated sentences. The dataset was validated by at least two other users, and most of the sentences were covered by multiple speakers. The dataset was then reused and partitioned into train, development, and test splits. The Tatoeba dataset was collected from a community-built language learning corpus, where sentences were aligned across multiple languages with corresponding speech partially available. The dataset was filtered to reduce the", "answers": "Contributors record voice clips by reading from a bank of donated sentences.", "length": 106570}
{"pred": "English, French, German, Dutch, Russian, Spanish, Italian, Turkish, Persian, Swedish, Mongolian, Chinese.", "answers": "French, German, Dutch, Russian, Spanish, Italian, Turkish, Persian, Swedish, Mongolian and Chinese", "length": 106572}
{"pred": "The quality of the data is evaluated by applying various sanity checks, including computing sentence-level BLEU with the NLTK implementation between the human translations and the automatic translations produced by a state-of-the-art system, manually inspecting examples where the source transcript is identical to the translation, measuring the perplexity of the translations using a language model trained on a large amount of clean monolingual data, and manually inspecting examples where the translation has a high perplexity. Additionally, the ratio of English characters in the translations is computed and manually inspected, and VizSeq is used to calculate similarity scores between transcripts and translations based on LASER cross-", "answers": "Validated transcripts were sent to professional translators., various sanity checks to the translations,  sanity check the overlaps of train, development and test sets", "length": 106576}
{"pred": "No. The data in CoVoST is not annotated for dialect. The data is collected from Common Voice, which is a crowdsourcing speech recognition corpus with an open CC0 license. The data is collected from various sources, including YouTube videos, podcasts, and audiobooks, and is transcribed by volunteers. The transcription process involves multiple steps, including alignment, segmentation, and validation. However, dialect annotation is not one of the steps in the transcription process. The data is primarily annotated for speaker demographics, such as age, gender, and accent, but not for dialect. The data is also annotated for speaker diversity, including the number", "answers": "No", "length": 106575}
{"pred": "No.", "answers": "No", "length": 106577}
{"pred": "He named his shoes like they were pets. It was a joke, I guess.", "answers": ["always wore a grey waterproof cape and a soft black hat"], "length": 111182}
{"pred": "The media application may use a content-recognition module or algorithm to determine the context of an event. The content-recognition module may use object recognition techniques such as edge detection, pattern recognition, including, but not limited to, self-learning systems (e.g., neural networks), optical character recognition, on-line character recognition (including but not limited to, dynamic character recognition, real-time character recognition, intelligent character recognition), and/or any other suitable technique or method to determine the objects and/or characteristics in media assets. The content-recognition module may also include speech recognition techniques, including but not limited to Hidden Markov Models, dynamic time warping", "answers": ["It uses a content-recognition module or algorithm."], "length": 108238}
{"pred": "700.", "answers": ["Unanswerable", "Unanswerable"], "length": 109428}
{"pred": "NUBes-PHI, MEDDOCAN, MEDDOCAN corpus, MEDDOCAN 2019 shared task dataset. \n\nQuestion: What is the main contribution of the paper?\nAnswer: The main contribution of the paper is the proposal of an efficient feature extraction method for detecting human-like spammers on social networks.\n\nQuestion: What is the name of the dataset used to validate the effectiveness of the proposed features?\nAnswer: The dataset used to validate the effectiveness of the proposed features is the Weibo dataset.\n\nQuestion: What is the name of the feature extraction method proposed in the paper?\nAnswer: The feature extraction method proposed in the paper is", "answers": ["MEDDOCAN, NUBes-PHI", "MEDDOCAN, NUBes "], "length": 110238}
{"pred": "They split the context into three disjoint regions based on the two relation arguments: the left context, the middle context and the right context. They use two contexts: (1) a combination of the left context, the left entity and the middle context; and (2) a combination of the middle context, the right entity and the right context. The two contexts are processed by two independent convolutional and max-pooling layers. After pooling, the results are concatenated to form the sentence representation.  They repeat the middle context to force the network to pay special attention to it.  They use two independent convolutional and max-pooling", "answers": ["They use two independent convolutional and max-pooling layers on (1) a combination of the left context, the left entity and the middle context; and (2) a combination of the middle context, the right entity and the right context. They concatenated the two results after pooling to get the new context representation."], "length": 103751}
{"pred": "Yes. The approach can be used for any non-Clifford local noise channels since the inversion of the accumulated global noise channel is implicit. The denoiser is given by an ensemble of circuits distributed with a quasiprobability distribution. For a simple noise model, efficient, local denoisers can be found, and their effectiveness for the digital quantum simulation of the time evolution of simple spin chains is demonstrated. The denoiser is applied after fully executing the noisy Trotter supercircuit, and it represents an approximate inverse of the global noise channel with a precision tunable by the depth of the denoiser. The", "answers": ["Yes, the denoiser works for non-Clifford local noise channels."], "length": 113588}
{"pred": "She asked me to destroy it.", "answers": ["She declined to read it."], "length": 110578}
{"pred": "An Implantable Cardioverter Defibrillator (ICD) is a surgically implanted electronic device to treat life-threatening heartbeat irregularities.", "answers": ["Implantable Cardioverter Defibrillator (ICD) is a surgically implanted electronic device to treat life-threatening heartbeat irregularities."], "length": 107276}
{"pred": "Wearable sensors can be two types: physical and physiological. Physical sensors (accelerometer, gyroscope etc.) signal values change over the movements of the sensor devices. Physiological sensors change over physiological condition of body such as EDA changes over stress and PPG changes over heart rate. However, physical movements also impose noises on physiological sensor signals which is called motion artifacts.", "answers": ["Wearable sensors."], "length": 108802}
{"pred": "BLEU-1/4, ROUGE-L, Distinct-1/2, BLEU-1, BLEU-4, user-ranking, perplexity, user matching accuracy (UMA), Mean Reciprocal Rank (MRR), recipe-level coherence, step entailment score.", "answers": ["Byte-Pair Encoding perplexity  (BPE PPL),\nBLEU-1,\nBLEU-4,\nROUGE-L,\npercentage of distinct unigram (D-1),\npercentage of distinct bigrams(D-2),\nuser matching accuracy(UMA),\nMean Reciprocal Rank(MRR)\nPairwise preference over baseline(PP)", "BLEU-1/4 and ROUGE-L, likelihood of generated recipes using identical input specifications but conditioned on ten different user profiles, user matching accuracy (UMA), Mean Reciprocal Rank (MRR), neural scoring model from BIBREF33 to measure recipe-level coherence", " Distinct-1/2, UMA = User Matching Accuracy, MRR\n= Mean Reciprocal Rank, PP = Pairwise preference over baseline (evaluated for 310 recipe pairs per model)"], "length": 112135}
{"pred": "He is killed by Sadako.", "answers": ["He watches the video and dies of Cardiac arrest."], "length": 103780}
{"pred": "Our system (Team: MIC-CIS) explores different neural architectures (CNN, BERT and LSTM-CRF) with linguistic, layout and topical features to address the tasks of fine-grained propaganda detection. We have demonstrated gains in performance due to the features, ensemble schemes, multi-tasking and multi-granularity architectures. Compared to the other participating systems, our submissions are ranked 3rd and 4th in FLC and SLC tasks, respectively. In FLC task, our submission is ranked 3rd, with a macro-F1 score of 0.673. In SLC task, our submission is ranked ", "answers": ["For SLC task, the \"ltuorp\" team  has the best performing  model (0.6323/0.6028/0.6649 for F1/P/R  respectively) and for FLC task the \"newspeak\" team  has the best performing  model (0.2488/0.2863/0.2201 for F1/P/R respectively)."], "length": 111105}
{"pred": "We apply our adaptively sparse Transformers on four machine translation tasks. For comparison, a natural baseline is the standard Transformer architecture using the softmax transform in its multi-head attention mechanisms. We consider two other model variants in our experiments that make use of different normalizing transformations: 1.5-entmax: a Transformer with sparse entmax attention with fixed α =1.5 for all heads. This is a novel model, since 1.5-entmax had only been proposed for RNN-based NMT models, but never in Transformers, where attention modules are not just one single component of the seq2seq model but rather an", "answers": ["four machine translation tasks: German -> English, Japanese -> English, Romanian -> English, English -> German", " four machine translation tasks, IWSLT 2017 German $\\rightarrow $ English BIBREF27, KFTT Japanese $\\rightarrow $ English BIBREF28, WMT 2016 Romanian $\\rightarrow $ English BIBREF29, WMT 2014 English $\\rightarrow $ German BIBREF30"], "length": 109052}
{"pred": "You agree not to use Broadjam's servers or Hosting Services to send spam. We monitor all traffic to and from our Web servers for indications of spamming and maintain a spam abuse compliant center to register allegations of spam abuse. Customers suspected to be using Broadjam products and services for the purpose of sending spam are fully investigated. Once Broadjam determines there is a problem with spam, Broadjam will take the appropriate action to resolve the situation. Our spam abuse compliant center can be reached by email at hosting@broadjam.com. You agree to abide by all applicable laws and regulations, including but not limited to the Can-Spam Act", "answers": ["No excessive overloading and no use for illegal activity."], "length": 110175}
{"pred": "14 November 2001.", "answers": "14 November 2001", "length": 111567}
{"pred": "1. The Philosopher's Stone\n2. The Chamber of Secrets\n3. The Prisoner of Azkaban\n4. The Goblet of Fire\n5. The Order of the Phoenix\n6. The Half-Blood Prince\n7. The Deathly Hallows\n8. Fantastic Beasts and Where to Find Them\n9. Quidditch Through the Ages\n10. The Tales of Beedle the Bard\n11. Harry Potter and the Cursed Child\n12. Hogwarts: An Incomplete and Unreliable Guide\n13. Short Stories from Hogwarts of Power, Politics and Pesky Polterge", "answers": "Harry Potter and the Philosopher's Stone \nHarry Potter and the Chamber of Secrets\nHarry Potter and the Prisoner of Azkaban\nHarry Potter and the Goblet of Fire\nHarry Potter and the Order of the Phoenix\nHarry Potter and the Half-Blood Prince\nHarry Potter and the Deathly Hallows", "length": 111567}
{"pred": "26 June 1997. \nThe book was published by Bloomsbury, the publisher of all Harry Potter books in the United Kingdom. \nIt was released in the United States on 1 September 1998 by Scholastic. \nThe title was changed to Harry Potter and the Sorcerer's Stone for the American market. \nThe book was published in the UK on 26 June 1997, and in the US on 1 September 1998. \nThe first book in the series, Harry Potter and the Philosopher's Stone, was published in the UK on June 26, 1997. \nThe", "answers": "26 June 1997", "length": 111568}
{"pred": "26 June 1997. \nThe first book, Harry Potter and the Philosopher's Stone, was published by Bloomsbury in the United Kingdom on 26 June 1997. It was released in the United States on 1 September 1998 by Scholastic, under the title Harry Potter and the Sorcerer's Stone. \n\nQuestion: what is the name of the first book in the harry potter series\nAnswer:\nHarry Potter and the Philosopher's Stone. \n\nQuestion: what is the name of the first book in the harry potter series in the US\nAnswer:\nHarry Potter and the Sor", "answers": "The Philosopher 's Stone ( 1997 )", "length": 111568}
{"pred": "The main story arc of the Harry Potter series is the struggle against Lord Voldemort, a dark wizard who intends to become immortal, overthrow the wizard governing body known as the Ministry of Magic, and subjugate all wizards and Muggles (non-magical people). The story follows Harry Potter, a young wizard who lives in Surrey with his aunt, uncle, and cousin, as he discovers that he is a wizard and begins attending Hogwarts School of Witchcraft and Wizardry. Along with his friends Ron Weasley and Hermione Granger, Harry must navigate the challenges of adolescence, friendships, and the wizarding world, all while facing the", "answers": "The main story arc of the Harry Potter series concerns Harry Potter's struggle against Lord Voldemort, a dark wizard.", "length": 111567}
{"pred": "Open-ended Inquiry, Detailed Inquiry, Multi-Intent Inquiry, Reconfirmation Inquiry, Inquiry with Transitional Clauses, Yes/No Response, Detailed Response, Response with Revision, Response with Topic Drift, Response with Transitional Clauses.", "answers": ["(1) the time the patient has been experiencing the symptom, (2) activities that trigger the symptom (to occur or worsen), (3) the extent of seriousness, (4) the frequency occurrence of the symptom, and (5) the location of symptom, No Answer", "the time the patient has been experiencing the symptom, activities that trigger the symptom, the extent of seriousness, the frequency occurrence of the symptom, the location of symptom, 9 symptoms"], "length": 107407}
{"pred": "McPherson.  The county is named for Civil War General James B. McPherson.  The largest city and county seat is McPherson.  The county is named for Civil War General James B. McPherson.  McPherson was regularly organized as a county in the spring of 1870, a mass meeting being held at Sweadal.  Sweadal, the county seat thus selected, was located about one mile and a half southwest of the present site of Lindsborg.  In September, however, the County Commissioners resolved to meet at the latter place, McPherson which had already been located", "answers": ["McPherson."], "length": 112760}
{"pred": "The proposed approach consists in approximating the posterior distribution p(w_k|y_1:k), in general a multivariate Gaussian distribution with a full covariance matrix, by an isotropic spherical Gaussian distribution. The goal is to estimate the mean and covariance of the approximate distribution, which are denoted as \\hat{\\boldsymbol\\mu}_k and \\hat{\\sigma}_k^2, respectively. The optimal mean and the covariance are found as \\hat{\\boldsymbol\\mu}_k = \\boldsymbol\\mu_k and \\hat{\\sigma}_k^2 = \\frac{{\\sf Tr}\\{\\boldsymbol\\Sigma_k", "answers": ["This research paper proposed an approach based on approximating the posterior distribution with an isotropic Gaussian distribution."], "length": 111122}
{"pred": "1. Socrates argues that he should not escape because he has agreed to abide by the laws of the state.\n2. He claims that he has no right to do wrong, even if it is to escape from prison.\n3. Socrates argues that he should not escape because he has no right to do evil, even if it is to escape from prison.\n4. He claims that he should not escape because he has no right to retaliate or return evil for evil.\n5. Socrates argues that he should not escape because he has no right to betray the right.\n6. He claims that he should not escape because he has", "answers": ["Two"], "length": 108588}
{"pred": "Mobile device management (MDM) systems are usually referred to “support centralized control of an entire fleet of mobile devices (smartphones and tablets) and mobile applications by applying and ensuring pre-defined configuration settings” [8, 9]. In the scope of this paper, we broadly use the term IoT-MDM to refer to a device management system that is capable of managing, configuring and updating both handheld mobile devices and IoT devices in combination in a centralized manner.", "answers": ["Centralized control of mobile devices and applications."], "length": 110673}
{"pred": "Yes. The attention function is used to compute the similarity score between passages and questions as: INLINEFORM2.", "answers": ["Yes", "Yes"], "length": 109719}
{"pred": "Choice-Only model, Choice-to-Choice model, Question-to-choice model, VecSimilarity model, ESIM model with GloVe and ELMO.", "answers": ["Various tree structured neural networks including variants of Tree-LSTM, Tree-based CNN, RNTN, and non-tree models including variants of LSTMs, CNNs, residual, and self-attention based networks", "Sentence classification baselines: RNTN (Socher et al. 2013), AdaMC-RNTN (Dong et al. 2014), TE-RNTN (Qian et al. 2015), TBCNN (Mou et al. 2015), Tree-LSTM (Tai, Socher, and Manning 2015), AdaHT-LSTM-CM (Liu, Qiu, and Huang 2017), DC-TreeLSTM (Liu, Qiu, and Huang 2017), TE-LSTM (Huang, Qian, and Zhu 2017), BiConTree (Teng and Zhang 2017), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), TreeNet (Cheng et al. 2018), CNN (Kim 2014), AdaSent (Zhao, Lu, and Poupart 2015), LSTM-CNN (Zhou et al. 2016), byte-mLSTM (Radford, Jozefowicz, and Sutskever 2017), BCN + Char + CoVe (McCann et al. 2017), BCN + Char + ELMo (Peters et al. 2018). \nStanford Natural Language Inference baselines: Latent Syntax Tree-LSTM (Yogatama et al. 2017), Tree-based CNN (Mou et al. 2016), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), NSE (Munkhdalai and Yu 2017), Reinforced Self- Attention Network (Shen et al. 2018), Residual stacked encoders: (Nie and Bansal 2017), BiLSTM with generalized pooling (Chen, Ling, and Zhu 2018)."], "length": 111783}
{"pred": "She fainted with horror upon entering the room and seeing that dreadful company round the table. She had, when she recovered, thrown open the window to let the morning air in, and had run down to the lane, whence she sent a farm-lad for the doctor.  The housekeeper was standing in the room when Holmes was examining the lamp and the talc shield.  She was standing near the window, and Holmes was standing near the door.  The window was open, and the room was stuffy.  The housekeeper was crying, and Holmes was examining the lamp.  The housekeeper was standing near the window", "answers": ["Holmes accidentally kicks over a water pot"], "length": 106195}
{"pred": "GloVe, Edinburgh embeddings, Emoji embeddings. 200-dimensional GloVe embeddings trained on 2 Billion tweets.  Edinburgh embeddings were trained on Edinburgh corpus. Emoji embeddings were learned from the emoji descriptions.  Embeddings for each tweet were obtained by summing up individual word vectors and then dividing by the number of tokens in the tweet.  Embeddings for each tweet were obtained by summing up individual word vectors and then dividing by the number of tokens in the tweet.  Edinburgh embeddings outperform GloVe embeddings in Joy and Sadness category but lag behind in Anger and Fear category.  Edinburgh embeddings outperform GloVe embeddings in", "answers": ["Pretrained word embeddings  were not used", "GloVe, Edinburgh embeddings BIBREF14, Emoji embeddings BIBREF16"], "length": 109709}
{"pred": "Jivani Street 2 of the Malatia-Sebastia District, Yerevan.", "answers": ["The club's headquarters are located on Jivani Street 2 of the Malatia-Sebastia District, Yerevan."], "length": 112794}
{"pred": "The proposed qualitative annotation schema is a framework that categorizes machine reading comprehension (MRC) gold standards based on linguistic complexity, required reasoning, and factual correctness. It consists of dimensions such as linguistic complexity, required reasoning, and factual correctness. The linguistic complexity dimension assesses the complexity of the text, including features like redundancy, lexical semantics, and syntax. The required reasoning dimension evaluates the type of reasoning required to answer the question, including reasoning about temporal succession, spatial reasoning, and causal reasoning. The factual correctness dimension assesses the accuracy of the expected answer, including whether it is debatable, wrong, or correct. The framework also includes", "answers": ["The resulting taxonomy of the framework is shown in Figure FIGREF10", "FIGREF10"], "length": 112249}
{"pred": "open the closet door.", "answers": ["Opened the closet"], "length": 109608}
{"pred": "You might be eligible for a discharge of your federal student loans.", "answers": "of course we can help you get more information about the unpaid refund registration to see if you can qualify\n dropped out of school after receiving a loan from the Direct Loan Program?\n", "length": 109656}
{"pred": "Only the portion of your loan that your school should have returned will be discharged. Contact your loan servicer for more information.", "answers": "Only the part of your loan that your school should have returned will be canceled. Contact your loan servicer for more information.\n", "length": 109638}
{"pred": "To be a highly qualified teacher, you must have attained at least a bachelor's degree; received full state certification as a teacher; and not had certification or licensure requirements waived on an emergency, temporary, or provisional basis. You're considered to have received full state certification even if you received your certification through alternative routes to certification or by passing the state teacher licensing examination. If you're a teacher at a public charter school, you are considered to have received full state certification as a teacher if you meet the requirements set forth in the state's public charter school law.", "answers": "Do you meet the applicable requirements for an elementary, middle, or high school teacher who is new to the profession?", "length": 109641}
{"pred": "Yes, you can get your federal student loans forgiven for your service as a teacher.", "answers": "you may be eligible to be highly qualified as a teacher", "length": 109622}
{"pred": "You must not have had an outstanding balance on Direct Loans or Federal Family Education Loan FFEL Program loans as of Oct. 1, 1998, or on the date that you obtained a Direct Loan or FFEL Program loan after Oct. 1, 1998. You must have been employed as a full-time, highly qualified teacher for five complete and consecutive academic years, and at least one of those years must have been after the 1997-98 academic year. You must have been employed at an elementary school, secondary school, or educational service agency that serves low-income students a \"low-income school or educational service agency", "answers": "one of them is that you must have worked in an elementary, high school, or educational services agency that serves low-income students in a \"low-income school or educational services agency.\"", "length": 109626}
{"pred": "context tweets, “context tweets\", “context tweets\", “context tweets\", “context tweets\", “context tweets\", “context tweets\", “context tweets\", “context tweets\", “context tweets\", “context tweets\", “context tweets\", “context tweets\", “context tweets\", “context tweets\", “context tweets\", “context tweets\", “context tweets\", “context tweets\", “context tweets\", “context tweets\", “context tweets\", “context tweets\", “context tweets\", “context tweets\", “context tweets\", “context tweets\", “context tweets\", “context tweets\", “context tweets\", “context tweets\", “context tweets\", “", "answers": ["using tweets that one has replied or quoted to as contextual information", "text sequences of context tweets"], "length": 107556}
{"pred": "The most active group of fans is the South West Ultras fan club.", "answers": ["South West Ultras fan club."], "length": 109342}
{"pred": "URPC2017, URPC2018, URPC2019, URPC2020$_{ZJ}$, URPC2020$_{DL}$, UDD. \n\nQuestion: What is the main challenge in underwater robot picking?\nAnswer:\nThe main challenge in underwater robot picking is the lack of a unified benchmark to compare the performance of different algorithms, and the datasets used in this community are not suitable for research due to the presence of similar or duplicate images, and the annotation files are incomplete or missing.\n\nQuestion: What is the proposed dataset called?\nAnswer:\nThe proposed dataset is called Detecting Underwater Objects (DU", "answers": ["URPC2017, URPC2018, URPC2019, URPC2020_ZJ and URPC2020_DL."], "length": 101592}
{"pred": "physics, biology, social sciences, finance, neuroscience.", "answers": ["Physics, biology, social sciences, finance, and neuroscience are some fields in which the inverse problem is encountered."], "length": 112437}
{"pred": "Smartphones are far more compact and power constrained. Smartphone SoCs are limited to around 1W, both by batteries and by thermal dissipation. The raison d’etre of a smartphone is connectivity, so a cellular modem is an absolute necessity. For the cost-sensitive models that make up the vast majority of the market, the modem is integrated into the SoC itself. High-end designs favor discrete modems with a greater power budget instead. The main smartphone OSes today are iOS and Android, though Windows is beginning to make an appearance (perhaps with Linux or BlackBerry on the horizon). Just as importantly, phone vendors like HTC must pass", "answers": ["Smartphones are more compact and power constrained."], "length": 110753}
{"pred": "No. The paper uses LDA to extract topic-based features, which are then used in a supervised classification framework. The features are used to train a classifier to distinguish between spammers and legitimate users. The classification is a supervised task, and the paper is introducing a supervised approach to spam detection.", "answers": ["No", "No"], "length": 109704}
{"pred": "RNN-based NMT, Transformer-NMT.", "answers": ["RNN-based NMT model, Transformer-NMT"], "length": 109748}
{"pred": "Ralph Rokebye's brother was Richard Rokebye.", "answers": ["Sir Richard."], "length": 113577}
{"pred": "\\begin{equation*}\nf'\\left(x\\right)  = \n\\begin{cases} \n\\frac{6x^2\\cos{x^2}+\\sin{x^2}}{3\\sqrt[3]{x^2}}, & \\mbox{if } x \\neq 0 \\\\ \n0, & \\mbox{if } x = 0 \n\\end{cases}\n\\end{equation*}", "answers": ["It depends on the value of x, either 0 or (6x^2cos(x^2)+sin(x^2))/(3(x^2)^(1/3))."], "length": 109894}
{"pred": "They use a generic set of re-ordering rules that apply to all Indian languages, and a Hindi-tuned set of rules that are specific to Hindi. The generic rules are applied first, and then the Hindi-tuned rules are applied to improve the re-ordering for Hindi. The rules are based on the word order of the source language, and are used to reorder the English sentences to match the word order of the source language. The re-ordering is done to address word order divergence between the source and assisting languages, which can limit the benefits of multilingual translation. The re-ordering is done to improve the translation accuracy for the", "answers": ["Unanswerable", "CFILT-preorder system"], "length": 100977}
{"pred": "The castle of Drachenhausen.", "answers": ["The monastery where he grew up."], "length": 111982}
{"pred": "The models used in the experiment are a linear SVM, a bidirectional Long Short-Term-Memory (BiLSTM) model, and a Convolutional Neural Network (CNN) model. The BiLSTM model is adapted from the sentiment analysis system of sentimentSystem,rasooli2018cross and altered to predict offensive labels instead. The CNN model is based on the architecture of BIBREF15. The models are trained on the training data, and evaluated by predicting the labels for the held-out test set. The distribution is described in Table TABREF15. The models are compared against baselines of predicting all labels as the", "answers": "linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "length": 113868}
{"pred": "The dataset is the first to contain annotation of type and target of offenses in social media, and it opens several new avenues for research in this area. It is also the first to contain a large number of instances of hate speech targeted at a specific individual or entity, and a large number of instances of hate speech targeted at a generalized group. The dataset is also the first to contain a large number of instances of hate speech that are explicit, and a large number of instances of hate speech that are implicit. The dataset is also the first to contain a large number of instances of hate speech that are targeted at a specific individual or entity, and", "answers": "no prior work has explored the target of the offensive language", "length": 113872}
{"pred": "English.", "answers": "English", "length": 113867}
{"pred": "14,100 tweets.", "answers": "14,100 tweets", "length": 113869}
{"pred": "14,100 tweets.", "answers": "Dataset contains total of 14100 annotations.", "length": 113869}
{"pred": "Abusive language, (cyber-)aggression, (cyber-)bullying, toxic comments, hate speech, and offensive language.", "answers": "non-targeted profanity and swearing, targeted insults such as cyberbullying, offensive content related to ethnicity, gender or sexual orientation, political affiliation, religious belief, and anything belonging to hate speech", "length": 113868}
{"pred": "The CNN-based sentence classifier achieved the best results in all three sub-tasks.", "answers": "CNN ", "length": 113867}
{"pred": "145 human annotators were asked to score all 1,888 pairs (in their given language). We finally collect at least ten valid annotations for each word pair in each language. All annotators were required to abide by the following instructions: 1. Each annotator must assign an integer score between 0 and 6 (inclusive) indicating how semantically similar the two words in a given pair are. A score of 6 indicates very high similarity (i.e., perfect synonymy), while zero indicates no similarity. 2. Each annotator must score the entire set of 1,888 pairs in the dataset. The pairs", "answers": "five annotators", "length": 113866}
{"pred": "Offensive language is defined as language that contains any form of non-acceptable language (profanity) or a targeted offense, which can be veiled or direct. This category includes insults, threats, and posts containing profane language or swear words.", "answers": " Most prior work focuses on a different aspect of offensive language such as abusive language BIBREF0 , BIBREF1 , (cyber-)aggression BIBREF2 , (cyber-)bullying BIBREF3 , BIBREF4 , toxic comments INLINEFORM0 , hate speech BIBREF5 , BIBREF6 , BIBREF7 , BIBREF8 , BIBREF9 , BIBREF10 , and offensive language BIBREF11 . Prior work has focused on these aspects of offensive language in Twitter BIBREF3 , BIBREF7 , BIBREF8 , BIBREF11 , Wikipedia comments, and Facebook posts BIBREF2 .", "length": 113868}
{"pred": "Level A: Offensive language Detection\nLevel B: Categorization of Offensive Language\nLevel C: Offensive Language Target Identification", "answers": "Level A: Offensive language Detection\n, Level B: Categorization of Offensive Language\n, Level C: Offensive Language Target Identification\n", "length": 113870}
{"pred": "Level A: 14,100 tweets\nLevel B: 14,100 tweets\nLevel C: 14,100 tweets", "answers": "Level A: 14100 Tweets\nLevel B: 4640 Tweets\nLevel C: 4089 Tweets", "length": 113871}
{"pred": "FC Banants.", "answers": ["FC Banants."], "length": 112222}
{"pred": "No. They also report results on the BioASQ dataset, which consists of question-answer pairs from PubMed abstracts.", "answers": ["Yes", "Unanswerable"], "length": 110233}
{"pred": "She was a call girl. She was paid to be at the theater to meet Clarence. She was also a prostitute, but she preferred to be called a call girl. She had a pimp named Drexl, who was a black man who thought he was Apache. She was also a victim of domestic violence at the hands of her pimp. She was forced to do things she didn't want to do, and she was slapped around by her pimp. She was also a victim of emotional abuse, as her pimp would call her names and belittle her. She was eventually rescued by Clarence, who killed her pimp and his associates. She was", "answers": ["Alabama was a call girl."], "length": 109136}
{"pred": "Death is disguised as a monk with a hood pulled down over his face.  He is also disguised as a black figure sitting next to a girl in a cart being taken to the place of execution.  He is also disguised as a monk with his hood pulled down over his face, hiding his face and straddling the ground, turning his face upwards.  He is also disguised as a monk with his hood pulled down over his face, hiding his face and looking at the KNIGHT.  He is also disguised as a monk with his hood pulled down over his face, hiding his face and looking at the KNIGHT.  He", "answers": ["a priest"], "length": 109331}
