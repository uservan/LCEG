{"pred": "The specific-heat ratio affects the average motion of the bubble by increasing the velocity of the bubble.", "answers": ["The specific-heat ratio affects the average motion of the bubble. The bubbles with smaller specific-heat ratios have slower average motion."], "length": 57140}
{"pred": "Season 7", "answers": "seventh season", "length": 52945}
{"pred": "The seventh season of Game of Thrones premiered on HBO on July 16, 2017, and consisted of 7 episodes.  The penultimate episode also runs for 71 minutes -- around 16 minutes longer than an average Game of Thrones episode.  The first five episodes mostly run longer than average ( 55 minutes ), at 59, 59, 63, 50, and 59 minutes respectively.", "answers": "August 6 , 2017", "length": 52946}
{"pred": "7", "answers": "seven", "length": 52946}
{"pred": "The last episode of Game of Thrones Season 7 is \"The Dragon and the Wolf\".", "answers": "August 27 , 2017", "length": 52946}
{"pred": "July 16, 2017", "answers": "July 16 , 2017", "length": 52947}
{"pred": "BLEU-1/4 and ROUGE-L are metrics used for evaluation.", "answers": "BLEU-1/4 and ROUGE-L, likelihood of generated recipes using identical input specifications but conditioned on ten different user profiles, user matching accuracy (UMA), Mean Reciprocal Rank (MRR), neural scoring model from BIBREF33 to measure recipe-level coherence", "length": 57238}
{"pred": "The recipes are written in English.", "answers": "English", "length": 57242}
{"pred": "Our model must learn to generate from a diverse recipe space: in our training data, the average recipe length is 117 tokens with a maximum of 256. There are 13K unique ingredients across all recipes. Rare words dominate the vocabulary: 95% of words appear $<$100 times, accounting for only 1.65% of all word usage. As such, we perform Byte-Pair Encoding (BPE) tokenization (BIBREF25) giving a training vocabulary of 15K tokens across 19M total mentions. User profiles are similarly diverse: 50% of users have consumed $\\le $6 recipes", "answers": "average recipe-level coherence scores of 1.78-1.82, human evaluators preferred personalized model outputs to baseline 63% of the time", "length": 57240}
{"pred": "The baseline models are the following:\n1. Lesk algorithm\n2. Babelfy\n3. IMS\n4. Bi-LSTM\n5. Bi-LSTM+att. + LEX + POS\n6. GAS\n7. CAN+ s\n8. HCAN", "answers": "name-based Nearest-Neighbor model (NN), Encoder-Decoder baseline with ingredient attention (Enc-Dec)", "length": 57237}
{"pred": "The interactions between users and recipes were obtained from user reviews on Food.com.", "answers": "from Food.com", "length": 57238}
{"pred": "The passage does not explicitly state where the recipes come from. However, it mentions that the model must learn to generate from a diverse recipe space, and that the average recipe length is 117 tokens with a maximum of 256. This suggests that the model is trained on a large dataset of recipes, but the specific source of the data is not mentioned.", "answers": "from Food.com", "length": 57239}
{"pred": "American.", "answers": ["American"], "length": 57710}
{"pred": "He was plump, dark and sexy, self-conscious of clothes and appeal, fickle.", "answers": ["always wore a grey waterproof cape and a soft black hat"], "length": 56797}
{"pred": "The proposed qualitative annotation schema is a high-level categorization of gold standards according to linguistic complexity, required reasoning, and factual correctness.", "answers": ["The resulting taxonomy of the framework is shown in Figure FIGREF10", "FIGREF10"], "length": 56340}
{"pred": "$\\Gamma_e = \\frac{e^2}{4\\pi \\epsilon_0 a_{ws}}\\frac{1}{k_B T_e}$", "answers": ["It is the ratio of the average unscreened electron-electron potential energy to kinetic energy."], "length": 56784}
{"pred": "Table TABREF22 shows statistics of the datasets. It can be observed that average length of Fisher is much higher than 20 newsgroups and CSAT. Cumulative distribution of document lengths for each dataset is shown in Fig. FIGREF21. It can be observed that almost all of the documents in Fisher dataset have length more than 1000 words. For CSAT, more than 50% of the documents have length greater than 500 and for 20newsgroups only 10% of the documents have length greater than 500.", "answers": ["CSAT dataset, 20 newsgroups, Fisher Phase 1 corpus", "CSAT dataset , 20 newsgroups, Fisher Phase 1 corpus"], "length": 55649}
{"pred": "The future direction mentioned in the conclusion is that the researchers will continue to explore the effectiveness of incorporating gloss knowledge into supervised WSD systems.", "answers": ["Verifying other meta-information such as patient's gender, age, race, etc."], "length": 55208}
{"pred": "The problem encountered when building the fuselage sides is that the sides are laid out flat with reference to the top longeron measured to a straight chalk line. The bowing problem starts when the side panels are bent and sloped to form the fuselage box section. If the sides were not sloped (tumbled home), the section formed would be cylindrical and the longerons would lie flat.", "answers": ["The longerons bow up from the building surface, forming a \"banana\" shape."], "length": 57042}
{"pred": "The forthcoming book is \"Legacies of Losing in American Politics\".", "answers": ["Legacies of Losing in American Politics and an expanded edition of The Rhetorical Presidency in the Princeton Classics series."], "length": 54533}
{"pred": "2.5", "answers": "2 molecules ATP", "length": 54524}
{"pred": "Oxygen", "answers": "other inorganic molecules", "length": 54524}
{"pred": "The Krebs cycle", "answers": "oxidative phosphorylation", "length": 54521}
{"pred": "ATP", "answers": "biochemical energy", "length": 54523}
{"pred": "4", "answers": "two net molecules of ATP", "length": 54524}
{"pred": "The baseline used was an LSTM encoder-decoder on character sequences.", "answers": ["error detection system by Rei2016", "error detection system by Rei2016"], "length": 57406}
{"pred": "The framework uses a complex-valued latent space dynamics, where the latent variables are treated independently. The latent space is complex-valued, and the latent variables are modeled as complex numbers.", "answers": ["By using a propagator in the latent space."], "length": 56960}
{"pred": "Smartphones are far more compact and power-constrained than tablets, with transistor density doubling every 24 months. This limits the number of transistors that can be packed into a smartphone, resulting in a lower power consumption and a smaller form factor. Tablets, on the other hand, are more akin to the PC, with a power budget that is much greater, and a design that is more akin to a PC.", "answers": ["Smartphones are more compact and power constrained."], "length": 57527}
{"pred": "They looked at FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney.", "answers": ["FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney", "FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney."], "length": 57441}
{"pred": "The invertibility condition for our neural projector is that it must be invertible, meaning that it must be able to transform the data space back into itself. This is necessary to derive tractable exact inference and marginal likelihood computation procedures.", "answers": ["The neural projector must be invertible.", "we constrain our neural projector with two requirements: (1) INLINEFORM0 and (2) INLINEFORM1 exists"], "length": 53232}
{"pred": "The decision to base the water rates on usage reflected the fact that the senior issue is very difficult. If someone is retired for twenty years, it is realistic to think prices don’t go up during the twenty years of retirement. Think what prices were in 1990 compared to today. Should seniors never have to pay for capital improvements?", "answers": ["Fairness."], "length": 56592}
{"pred": "Margaret Way was born in Brisbane, Australia, and died in Cleveland, Queensland, Australia.", "answers": ["Margaret Way was born in Brisbane and died in Cleveland, Queensland, Australia."], "length": 55413}
{"pred": "The Joint model combines textual and visual representations to predict the quality of a document.", "answers": "visual model is based on fine-tuning an Inception V3 model BIBREF1 over visual renderings of documents, while our textual model is based on a hierarchical biLSTM. We further combine the two into a joint model. , neural network models", "length": 57088}
{"pred": "The Joint model combines textual and visual representations to predict the quality of a document.", "answers": "Inception V3, biLSTM", "length": 57088}
{"pred": "Yes, they did.", "answers": "No", "length": 57090}
{"pred": "No, the methods that work best on academic papers do not work best on Wikipedia.", "answers": "No", "length": 57095}
{"pred": "The Joint model achieved the highest accuracy in 11 out of 18 cases. It is also worth noting that all models achieve higher scores for FA, GA, and Stub articles than B, C and Start articles. This can be explained in part by the fact that FA and GA articles must pass an official review based on structured criteria, and in part by the fact that Stub articles are usually very short, which is discriminative for Inception, and Joint. All models perform worst on the B and C quality classes. It is difficult to differentiate B articles from C articles even for Wikipedia contributors. As evidence of this, when we crawled a", "answers": "59.4% on wikipedia dataset, 93.4% on peer-reviewed archive AI papers, 77.1%  on peer-reviewed archive Computation and Language papers, and 79.9% on peer-reviewed archive Machine Learning papers", "length": 57088}
{"pred": "Textual features are more useful than visual features.", "answers": "It depends on the dataset. Experimental results over two datasets reveal that textual and visual features are complementary. ", "length": 57090}
{"pred": "English", "answers": "English", "length": 57086}
{"pred": "The dataset contains 7,782 underwater images after deleting overly similar images and has a more accurate annotation with four types of classes (holothurian, echinus, scallop, and starfish).", "answers": "a sample of  29,794 wikipedia articles and 2,794 arXiv papers ", "length": 57087}
{"pred": "They get their ground truth quality judgments from the test set of the datasets.", "answers": "quality class labels assigned by the Wikipedia community, a paper is considered to have been accepted (i.e. is positively labeled) if it matches a paper in the DBLP database or is otherwise accepted by any of the following conferences: ACL, EMNLP, NAACL, EACL, TACL, NIPS, ICML, ICLR, or AAAI", "length": 57090}
{"pred": "The dataset of hashtags is sourced from the Stanford Sentiment Analysis Dataset.", "answers": ["1,268 randomly selected tweets in the Stanford Sentiment Analysis Dataset BIBREF36, all 12,594 unique English hashtags and their associated tweets from the same Stanford dataset", "Stanford Sentiment Analysis Dataset BIBREF36"], "length": 55595}
{"pred": "The speech emotion recognition task is one of the most important problems in the field of paralinguistics. This field has recently broadened its applications, as it is a crucial factor in optimal human-computer interactions, including dialog systems. The goal of speech emotion recognition is to predict the emotional content of speech and to classify speech according to one of several labels (i.e., happy, sad, neutral, and angry). Various types of deep learning methods have been applied to increase the performance of emotion classifiers; however, this task is still considered to be challenging for several reasons. First, insufficient data for training complex neural network-based models are available", "answers": ["probabilistic model", "Logistic Regression, Multilayer Perceptron"], "length": 57718}
{"pred": "The German gentleman, a nephew of the person of the house, is extremely nasty and nasty to the other boarders.", "answers": ["the German professor"], "length": 55413}
{"pred": "Extra-Linguistic Features for Stance Classification", "answers": "anti-nuclear-power", "length": 56245}
{"pred": "The UTCNN model has 5 layers.", "answers": "eight layers", "length": 56244}
{"pred": "Topic: abortion\nTopic: gay rights\nTopic: Obama\nTopic: marijuana\nTopic: nuclear power\nTopic: radiation\nTopic: air pollution\nTopic: crime\nTopic: madness\nTopic: death\nTopic: prejudice\nTopic: corruption\nTopic: madness\nTopic: death\nTopic: prejudice\nTopic: corruption\nTopic: madness\nTopic: death\nTopic: prejudice\nTopic: corruption\nTopic: madness\nTopic: death\nTopic: prejudice\nTopic: corruption\nTopic: madness\nTopic: death\nTopic: prejudice\nTopic: corruption\nTopic: madness\nTopic: death\nTopic: prejudice\n", "answers": "abortion, gay rights, Obama, marijuana", "length": 56243}
{"pred": "9.3M arguments for EN (240k CoNLL + 9.1M Europarl) and 4.43M for DE (32k CoNLL + 4.4M Europarl).", "answers": "32,595", "length": 56243}
{"pred": "Yes, they collected the two datasets.", "answers": "No", "length": 56241}
{"pred": "Baselines: 1) SVM with unigram, bigram, and trigram features, 2) SVM with average word embedding, 3) SVM with average transformed word embeddings, 4) two mature deep learning models on text classification, 5) the above SVM and deep learning models with comment information, 6) UTCNN without user information, representing a pure-text CNN model, and 7) UTCNN without the LDA model, representing how UTCNN works with a single-topic dataset.", "answers": "SVM with unigram, bigram, trigram features, with average word embedding, with average transformed word embeddings, CNN and RCNN, SVM, CNN, RCNN with comment information", "length": 56240}
{"pred": "The name of the generative interactive model used in the method is C-GDBN (Coupled Generalized Dynamic Bayesian Network).", "answers": ["The generative interactive model used in the method is called the Coupled Generalized Dynamic Bayesian Network (C-GDBN)."], "length": 56388}
{"pred": "Yes, they use attention.", "answers": ["Yes", "Yes"], "length": 56048}
{"pred": "The approach of previous work is to use shallow methods, which are compared to deeper models for LID.", "answers": "'shallow' naive Bayes, SVM, hierarchical stacked classifiers, bidirectional recurrent neural networks", "length": 55916}
{"pred": "No, the lexicon is not the same for all languages. In the paper, it is mentioned that the lexicon is built over all the data and therefore includes the vocabulary from both the training and testing sets.", "answers": "Yes", "length": 55918}
{"pred": "The lexicon is obtained by building a hierarchical naive Bayesian and lexicon-based classifier for LID of short pieces of text of 15-20 characters long.", "answers": "built over all the data and therefore includes the vocabulary from both the training and testing sets", "length": 55916}
{"pred": "Macro-averaged Mean Absolute Error (INLINEFORM0) is used.", "answers": "average classification accuracy, execution performance", "length": 55914}
{"pred": "The Nguni languages are similar to each other and harder to distinguish. The same is true of the Sotho languages.", "answers": "Nguni languages (zul, xho, nbl, ssw), Sotho languages (nso, sot, tsn)", "length": 55916}
{"pred": "The datasets for the DSL 2015 & DSL 2017 shared tasks BIBREF1 are often used in LID benchmarks and also available on Kaggle. The DSL datasets, like other LID datasets, consists of text sentences labelled by language. The 2017 dataset, for example, contains 14 languages over 6 language groups with 18000 training samples and 1000 testing samples per language.", "answers": "DSL 2015, DSL 2017, JW300 parallel corpus , NCHLT text corpora", "length": 55919}
{"pred": "Yes, the paper reports the performance of a baseline model on South African languages LID.", "answers": "Yes", "length": 55925}
{"pred": "Yes, the proposed algorithm improves on the state-of-the-art methods.", "answers": "Yes", "length": 55920}
{"pred": "The ground truth for fake news is established by manually labeling the tweets as containing fake news or not containing them based on the characterization presented before.", "answers": ["Ground truth is not established in the paper"], "length": 58320}
{"pred": "More than 120 novels.", "answers": ["Margaret Way wrote more than 120 novels."], "length": 52324}
{"pred": "They obtain psychological dimensions of people by analyzing the language use in their blog posts.", "answers": ["using the Meaning Extraction Method", "Unanswerable"], "length": 55014}
{"pred": "O (t, L_{\\parallel} ; S_\\Delta) = L_{\\parallel}^{-\\beta/[\\nu(1+\\Delta)]} \\tilde f_O (t/L_{\\parallel}^{z/(1+\\Delta)} ; S_\\Delta).", "answers": ["O(t, L_{\\parallel}; S_\\Delta) = L_{\\parallel}^{-\\beta/[\\nu(1+\\Delta)]} \\tilde f_O(t/L_{\\parallel}^{z/(1+\\Delta)}; S_\\Delta)."], "length": 56341}
{"pred": "The 2017 general election was held on 23 September.", "answers": ["23 September."], "length": 54517}
{"pred": "The maximum radius of the emitting region for the black body model is 4.5$\\pm$0.2$\\times$10$^{8}$\\,cm.", "answers": ["4.5x10^8 cm."], "length": 56769}
{"pred": "Related Work\nThere are several general-purpose deep learning frameworks, such as TensorFlow, PyTorch and Keras, which have gained popularity in NLP community. These frameworks offer huge flexibility in DNN model design and support various NLP tasks. However, building models under these frameworks requires a large overhead of mastering these framework details. Therefore, higher level abstraction to hide the framework details is favored by many engineers.\n\nThere are also several popular deep learning toolkits in NLP, including OpenNMT BIBREF0, AllenNLP BIBREF1 etc. OpenNMT is an open-source toolkit mainly targeting neural machine translation or other", "answers": ["Embedding Layer, Neural Network Layers, Loss Function, Metrics", "Embedding Layer, Neural Network Layers, Loss Function, Metrics"], "length": 55550}
{"pred": "The extended middle context uses all parts of the sentence (the relation arguments, left of the relation arguments, between the arguments, right of the arguments) and pays special attention to the middle part.", "answers": ["They use two independent convolutional and max-pooling layers on (1) a combination of the left context, the left entity and the middle context; and (2) a combination of the middle context, the right entity and the right context. They concatenated the two results after pooling to get the new context representation."], "length": 55809}
{"pred": "They test their method on standard benchmarks for emotion classification, which include the Affective Text dataset, the Fairy Tales dataset, and the ISEAR dataset.", "answers": ["None"], "length": 57495}
{"pred": "The results show that our proposed model achieves new state-of-the-art performance in MSR and AS, and comparable performance in CITYU and PKU.", "answers": ["Reward of 11.8 for the A2C-chained model, 41.8 for the KG-A2C-chained model, 40 for A2C-Explore and 44 for KG-A2C-Explore.", "KG-A2C-chained and KG-A2C-Explore both pass the bottleneck of a score of 40"], "length": 58054}
{"pred": "The best performing model among author's submissions is the submission ranked at 4th position in the FLC task.", "answers": ["For SLC task, the \"ltuorp\" team  has the best performing  model (0.6323/0.6028/0.6649 for F1/P/R  respectively) and for FLC task the \"newspeak\" team  has the best performing  model (0.2488/0.2863/0.2201 for F1/P/R respectively)."], "length": 57027}
{"pred": "The German gentleman is the one who converses with great fluency, and I feel as if it would be right to give up trying to learn French, and just try to learn to speak our own tongue as these English speak it.", "answers": ["the German professor"], "length": 58203}
{"pred": "He is jumpy at the time.", "answers": ["He throws himself down the stairs"], "length": 55353}
{"pred": "In BIBREF17, authors use BERT for document classification but the average document length is less than BERT maximum length 512. TransformerXL is an extension to the Transformer architecture that allows it to better deal with long inputs for the language modelling task. It relies on the auto-regressive property of the model, which is not the case in our tasks.", "answers": ["Answer with content missing: (Data section) Chinese with version 5.1 of the Chinese Penn Treebank (CTB)"], "length": 56751}
{"pred": "The interlayer Berry connection polarizability arises from layer hybridization of electrons under the chiral crystal symmetry characteristic of twisted bilayers.", "answers": ["The momentum space curl of the interlayer Berry connection polarizability generates the crossed nonlinear dynamical Hall effect."], "length": 55382}
{"pred": "Adelaide Mulville.", "answers": ["Ruth Anvoy"], "length": 56929}
{"pred": "Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, Pointer-Gen+ARL-SEN.", "answers": ["Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN"], "length": 56533}
{"pred": "The datasets used in Passage 4 are: \n1. Stanford Sentiment Treebank (SST) \n2. IMDB movie reviews \n3. Stanford Sentiment Treebank (SST) (again) \n4. Microsoft Research Paraphrase Corpus (MRPC) \n5. Google Sentiment Analysis Dataset (GSAD) \n6. Stanford Sentiment Treebank (SST) (again) \n7. Reddit (CMV) dataset \n8. Wikipedia (Conversations Gone Awry) dataset \n9. Google Speech-to-Text (G-STT) dataset \n10. Automatic Speech Recognition (ASR)", "answers": ["Answer with content missing: (Data section) Chinese with version 5.1 of the Chinese Penn Treebank (CTB)"], "length": 56465}
{"pred": "The passage does not explicitly state how the ground truth for fake news is established. However, it mentions that \"neither of us is prepared to admit diabolical intrusions into the affairs of men.\" This implies that the authors of the passage are aware of the challenges of defining fake news and are suggesting that the ground truth may not be easily established.", "answers": ["Ground truth is not established in the paper"], "length": 54095}
{"pred": "Contrastive learning.", "answers": ["An unsupervised method based on the information bottleneck and contrastive learning."], "length": 55852}
{"pred": "Socrates compares going against the law to being a destroyer of the city.", "answers": ["Your parents"], "length": 57645}
{"pred": "The model is compared to term frequency methods, which are a type of natural language processing (NLP) technique used to analyze and summarize the content of a text.", "answers": ["bag-of-words model, CNN"], "length": 57329}
{"pred": "A research opportunity for children, teens, and young adults with autism is being studied.", "answers": ["A study on the effects of Brazilian Jiu Jitsu and psychotherapy on people with autism."], "length": 56786}
{"pred": "The name of the contract is \"Real Estate Education Training Program Development Agreement\"", "answers": "REAL ESTATE EDUCATION TRAINING PROGRAM DEVELOPMENT AGREEMENT", "length": 55031}
{"pred": "The passage does not mention the \"Renewal Term\" at all. It only discusses the initial term, automatic extensions, and unilateral extensions with prior notice.", "answers": "The Term shall automatically renew thereafter for successive 5-year terms unless either party provides prior written notice of termination not less than 90 days prior to the end of such five-year term.", "length": 55049}
{"pred": "The contract should be reviewed by a lawyer to ensure compliance with relevant laws and regulations, including antitrust laws, to avoid any potential issues related to revenue or profit sharing.", "answers": "In lieu of any other royalty, the parties shall share Cash Sales from the sale of such independently developed T&B Products that are generated directly and independently by LEA as follows: [●%] to LEA [●%] to T&B", "length": 55050}
{"pred": "There is no mention of \"Volume Restriction\" in the provided passages. The contract appears to be related to \"Hashtag Segmentation\" and \"Hashtag Tokenization\" services. The contract mentions that the parties agree to use Twitter-specific language models, but it does not mention any specific requirements or restrictions related to volume usage.", "answers": "T&B Personality shall make six (6) public appearances each year of the Term, including an appearance at LEA's annual Hall of Fame Symposium, for the purpose of promoting the Business, which appearances may include autograph sessions, book signings, appearances at LEA's workshops, seminars and symposiums with each such session not to exceed four (4) hours", "length": 55050}
{"pred": "There is no mention of joint or shared ownership of intellectual property between the parties to the contract in the provided passages. The contract only mentions that T&B and LEA wish to conduct business to develop and operate a \"Tarek's Real Estate 101\" branded seminar style education business that uses, among other things, the names, images, and likenesses of Tarek El Moussa to market and sell customers real estate investing oriented education products.", "answers": "LEA and T&B shall jointly own all jointly-created work product including, but not limited to, ideas, any and all concepts, designs, Customer Data (including client lists) generated through the conduct of the Business, programs, software, reports, or other intellectual property and tangible work product, produced for the Business , regardless of whether such were incorporated into or used by the Business (collectively \"Work Product\"), shall be and remain the joint property of LEA and T&B when produced provided, however, (i) to the extent LEA has contributed distinct and divisible work product to the Business during the Term (\"LEA Work Product\"), such LEA Work Product shall remain frozen for a period not to exceed 90 days, during which time T&B may acquire a license for the LEA Work Product by reimbursing LEA direct and verifiable costs LEA incurred in producing the LEA Work Product during the Term and (ii) to the extent T&B has contributed distinct and divisible work product to the Business during the Term (\"T&B Work Product\"), such T&B Work Product shall remain frozen for a period not to exceed 90 days, during which time LEA may acquire license for the T&B Work Product by reimbursing T&B direct and verifiable costs T&B incurred in producing the T&B Work Product during the Term.", "length": 55046}
{"pred": "There is no mention of insurance in the contract.", "answers": "T&B and Tarek El Moussa shall be named as an additional insured on such insurance and proof of such inclusion shall be provided to T&B.", "length": 55045}
{"pred": "The UTCNN model has 3 layers.", "answers": ["eight layers"], "length": 56971}
{"pred": "The baselines for Track-1 and Track-2 were the performance of the best models on the validation set.", "answers": ["Unanswerable", "LF-MMI Attention\nSeq2Seq \nRNN-T \nChar E2E LF-MMI \nPhone E2E LF-MMI \nCTC + Gram-CTC"], "length": 56366}
{"pred": "They match words before reordering them by pre-ordering the assisting language sentences to match the word order of the source language.", "answers": ["Unanswerable", "CFILT-preorder system"], "length": 52873}
{"pred": "52% of the annotators marked translations as having equal quality. Among the cases where one of the translations was marked better than the other, the DocRepair translation was marked better in 73% of the cases.", "answers": ["Unanswerable", "Unanswerable"], "length": 57308}
{"pred": "The story takes place in the United States.", "answers": ["In Socrates Cell?"], "length": 57271}
{"pred": "1900 births", "answers": ["Goodwin became a Naval aviator in January 1929."], "length": 53755}
{"pred": "Crito's visit was to bring the young Baron Otto to the White Cross on the Hill, where he would be taken to safety and sanctuary.", "answers": ["To smuggle Socrates out of prison and into a life of exile."], "length": 58076}
{"pred": "The robustness of a model is defined as how well it can handle the data imbalance issue, which is a common problem in many NLP tasks such as sentiment analysis, named entity recognition, machine reading comprehension, and paraphrase identification. The robustness of a model is measured by its ability to generalize well to unseen data, and to perform well on unseen examples. In the context of NLP, robustness is often measured by the performance of the model on a validation set, and the F1 score is often used as a metric to evaluate the model's performance.", "answers": ["ability to accurately classify texts even when the amount of prior knowledge for different classes is unbalanced, and when the class distribution of the dataset is unbalanced", "Low sensitivity to bias in prior knowledge"], "length": 57240}
{"pred": "The baseline system for the SLC task was a very simple logistic regression classifier with default parameters, where we represented the input instances with a single feature: the length of the sentence.", "answers": ["The baseline system for the SLC task is a very simple logistic regression classifier with default parameters. The baseline for the FLC task generates spans and selects one of the 18 techniques randomly.", "SLC task is a very simple logistic regression classifier, FLC task generates spans and selects one of the 18 techniques randomly"], "length": 55837}
{"pred": " Mufti-e-Azam-e-Hind received Khilafat from his paternal uncle, Huzoor Mufti-e-Azam Hind, Shaikh Mufti Muhammad Mustapha Raza Khan (rahmatullahi alaih).", "answers": ["Mufti-e-Azam-e-Hind received Khilafat in the Qaderi, Chishti, Nakshbandi, Suharwardi, and Madaari Orders."], "length": 56460}
{"pred": "No, Arabic is not one of the 11 languages in CoVost.", "answers": ["No", "No"], "length": 55739}
{"pred": "Yes, PolyResponse was evaluated against some baseline.", "answers": ["No", "No"], "length": 57266}
{"pred": "BIBREF17, BIBREF18, BIBREF23, BIBREF24, BIBREF25, BIBREF26, BIBREF19, BIBREF20, BIBREF21, BIBREF22, BIBREF23, BIBREF24, BIBREF25, BIBREF26, BIBREF19, BIBREF20, BIBREF21, BIBREF22.", "answers": ["BIBREF17, BIBREF18, TensiStrength BIBREF13, TwitterNLP BIBREF6, BIBREF19, CogComp-NLP BIBREF20, Stanford NLP NER BIBREF21", "BIBREF23, BIBREF17, BIBREF18, BIBREF19, BIBREF24, BIBREF25, BIBREF26"], "length": 53767}
{"pred": "The subjects were presented with a stimulus, which is a visual image of a raised porch of JOE CLARK'S Store and the street in front of it.", "answers": ["7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)", "KARA ONE BIBREF17 , composed of multimodal data for stimulus-based, imagined and articulated speech state corresponding to 7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)"], "length": 53892}
{"pred": "Baseline experiments using SVMs and neural networks to identify the offensive tweets, discriminate between insults, threats, and profanity, and finally to identify the target of the offensive messages.", "answers": ["They compare with the following models: by Pedersen (2017), by Pramanick and Das (2017), by Mikhalkova and Karyakin (2017),  by Vadehra (2017), Indurthi and Oota (2017), by Vechtomova (2017), by (Cai et al., 2018), and CRF."], "length": 57522}
{"pred": "Relation detection is the first step in KBQA, and it is a key step in KBQA. It is a crucial component in KBQA as it enables the system to understand the meaning of the input question and the context in which it is being asked. Relation detection is a subtask of KBQA that involves identifying the relations between entities in the input question. The core component for KBQA is the relation detection model, which is responsible for identifying the relations between entities in the input question.", "answers": ["answer questions by obtaining information from KB tuples ", "hierarchical matching between questions and relations with residual learning"], "length": 52664}
{"pred": "Legacies of Losing in American Politics, with Nicole Mellow (University of Chicago Press, Fall 2017), and an expanded edition of The Rhetorical Presidency in the Princeton Classics series (Princeton, Fall 2017).", "answers": ["Legacies of Losing in American Politics and an expanded edition of The Rhetorical Presidency in the Princeton Classics series."], "length": 56017}
{"pred": "Long Term Capital Management (LTCM).", "answers": ["Long Term Capital Management (LTCM)."], "length": 53710}
{"pred": "The authors present evidence that the model can capture some biases in data annotation and collection by showing that the model is more accurate when pre-trained on cloze questions than when trained on the supervised data. They also show that the model is more accurate when pre-trained on cloze questions than when trained on the supervised data.", "answers": ["The authors showed few tweets where neither and implicit hatred content exist but the model was able to discriminate"], "length": 54206}
{"pred": "She would often climb the steepest ladder of the crudded rack Up to some beaked cape of cloud sublime, And like Arion on the dolphin's back Ride singing through the shoreless air;--oft-time Following the serpent lightning's winding track, She ran upon the platforms of the wind, And laughed to hear the fire-balls roar behind.", "answers": ["Strange dreams."], "length": 54311}
{"pred": "You can get health care through CHAMPVA if you don't qualify for TRICARE and at least one of the descriptions below is true for you. At least one of these must be true. You re: The spouse or child of a Veteran who s been rated permanently and totally disabled for a service - connected disability by a VA regional benefit office, or The surviving spouse or child of a Veteran who died from a VA - rated service - connected disability, or The surviving spouse or child of a Veteran who was at the time of death rated permanently and totally disabled from a service - connected disability, or The surviving spouse or child of a", "answers": "Are you the spouse or surviving spouse of or a child of a Veteran with disabilities or a Veteran who has died ?", "length": 56972}
{"pred": "The Crito is a dialogue between Socrates and Crito, where Socrates is afraid that Crito is pressuring him to escape from prison. Crito is trying to persuade Socrates to escape, but Socrates is unwilling to do so. Crito is trying to convince Socrates that the laws of Athens will not allow him to escape, and that he will be punished if he does. Socrates is trying to convince Crito that he is not acting out of fear, but rather out of a desire to do good and to live a virtuous life.", "answers": "If you don t qualify for TRICARE the Department of Defense s health care program for active - duty and retired service members and their families", "length": 56969}
{"pred": "You may be eligible for CHAMPVA benefits if you don't qualify for TRICARE and at least one of the descriptions below is true for you. At least one of these must be true. You re: The spouse or child of a Veteran who s been rated permanently and totally disabled for a service-connected disability by a VA regional benefit office, or The surviving spouse or child of a Veteran who died from a VA-rated service-connected disability, or The surviving spouse or child of a Veteran who was at the time of death rated permanently and totally disabled from a service-connected disability, or The surviving spouse or child of a service member who died", "answers": "you may be able to get health insurance through the Civilian Health and Medical Program of the Department of Veterans Affairs CHAMPVA .", "length": 56977}
{"pred": "Yes, you can call 800-273-8255, then select 1. Start a confidential Veterans chat. Text 838255. If you have hearing loss, call TTY: 800-799-4889. You can also call 911. Go to the nearest emergency room. Go directly to your nearest VA medical center. It doesn't matter what your discharge status is or if you're enrolled in VA health care. Find your nearest VA medical center.", "answers": "Yes, you can contact the caregiver support line at 855-260-3274", "length": 56992}
{"pred": "You need to be eligible for CHAMPVA benefits.", "answers": "You will qualify for this program based on your Veteran’s qualifications. You may be eligible if the Veteran you’re caring for meets both of the requirements  ", "length": 56979}
{"pred": "According to the passage, to get CHAMPVA benefits, you need to apply for these benefits. To apply, you need to submit the required documents, including Application for CHAMPVA Benefits VA Form 10-10d, and Other Health Insurance Certification VA Form 10-7959c, and Documents related to your Medicare status. You also need to sign and date the form. Download VA Form 10-10CG PDF Mail the form and any supporting documents to: Program of Comprehensive Assistance for Family CaregiversHealth Eligibility Center2957 Clairmont Road NE, Ste 200Atlanta, GA 30329-1647", "answers": "Okay. Does the veteran you are caring for have a serious injury including traumatic brain injury, psychological trauma, or other mental disorder that was caused or made worse by their active-duty service on or after September 11, 2001?", "length": 56974}
{"pred": "The days of Socrates are drawing to a close; the fatal ship has been seen off Sunium, as he is informed by his aged friend and contemporary Crito, who visits him before the dawn has broken; he himself has been warned in a dream that on the third day he must depart.", "answers": "If that is true then you are not eligible for this benefit.", "length": 56974}
{"pred": "Monsieur Vervelle wants his child to wed a man named Iwata.", "answers": ["Pierre Grassou."], "length": 53199}
{"pred": "Smartphones are far more compact and power-constrained, with transistor density doubling every 24 months, whereas tablets are more akin to the PC both technically and economically.", "answers": ["Smartphones are more compact and power constrained."], "length": 56735}
{"pred": "The future direction mentioned in the conclusion is to explore directions for ironies that are more difficult to model.", "answers": ["Verifying other meta-information such as patient's gender, age, race, etc."], "length": 55666}
{"pred": "The RNN layer works better.", "answers": ["Transformer over BERT (ToBERT)", "The transformer layer"], "length": 53226}
{"pred": "The lover hid in the cupboard.", "answers": ["In a closet."], "length": 56939}
{"pred": "No, Arabic is not one of the 11 languages in CoVost.", "answers": ["No", "No"], "length": 57306}
{"pred": "Text-based classifiers", "answers": ["AllWords model by counting the frequencies of all the remaining words and training a multinomial Naive Bayes classifier"], "length": 52820}
