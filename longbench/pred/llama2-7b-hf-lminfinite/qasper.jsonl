{"pred": "The ground truth for fake news is established by a single person who manually annotated the dataset.\n\nQuestion: What is the difference between the distribution of friends/followers of accounts spreading fake news and those spreading viral content?\n\nAnswer: The distribution of friends/followers of accounts spreading fake news is more evenly distributed than those spreading viral content.\n\nQuestion: What is the difference between the distribution of mentions of other users in tweets containing fake news and those containing viral content?\n\nAnswer: The distribution of mentions of other users in tweets containing fake news is more even", "answers": ["Ground truth is not established in the paper"], "all_classes": null, "length": 4588}
{"pred": "GhostVLAD is an extension of the NetVLAD approach, which we discussed in the previous section. The GhostVLAD model was proposed for face recognition by Y. Zhong [10]. GhostVLAD works exactly similar to NetVLAD except it adds Ghost clusters along with the NetVLAD clusters. So, now we will have a K+G number of clusters instead of K clusters. Where G is the number of ghost clusters, we want to add (typically 2-4). The Ghost clusters are added to map any noisy or irrelevant content into ghost clusters and are", "answers": ["extension of the NetVLAD, adds Ghost clusters along with the NetVLAD clusters", "An extension of NetVLAD which replaces hard assignment-based clustering with soft assignment-based clustering with the additon o fusing Ghost clusters to deal with noisy content."], "all_classes": null, "length": 3769}
{"pred": "By how much does their model outperform the state of the art results?\n\nQuestion: What is the accuracy of their model?\n\nAnswer: What is the accuracy of their model?\n\nQuestion: What is the accuracy of their model?\n\nAnswer: What is the accuracy of their model?\n\nQuestion: What is the accuracy of their model?\n\nAnswer: What is the accuracy of their model?\n\nQuestion: What is the accuracy of their model?\n\nAnswer: What is the accuracy of their model?\n\nQuestion: What is the accuracy of their model?\n\nAnswer: What", "answers": ["the attention model, MDREA, also outperforms the best existing research results (WAP 0.690 to 0.688)"], "all_classes": null, "length": 4975}
{"pred": "The article proposes the use of context tweets to improve the accuracy of neural network models.\n\nQuestion: What is the most accurate model?\n\nAnswer: The most accurate model is the RNN with LTC modules.\n\nQuestion: What is the most significant improvement?\n\nAnswer: The most significant improvement is the use of context tweets in RNN models.\n\nQuestion: What is the most significant decrease in accuracy?\n\nAnswer: The most significant decrease in accuracy is the use of character-level features in neural network models.\n\nQuestion: What is the most significant improvement in accuracy?\n", "answers": ["using tweets that one has replied or quoted to as contextual information", "text sequences of context tweets"], "all_classes": null, "length": 3493}
{"pred": "They looked at the following Facebook pages:\n\nTime\n\nThe Guardian\n\nDisney\n\nAnswer: They looked at the following Facebook pages:\n\nTime\n\nThe Guardian\n\nDisney\n\nAnswer: They looked at the following Facebook pages:\n\nTime\n\nThe Guardian\n\nDisney\n\nAnswer: They looked at the following Facebook pages:\n\nTime\n\nThe Guardian\n\nDisney\n\nAnswer: They looked at the following Facebook pages:\n\nTime\n\nThe Guardian\n\nDisney\n\nAnswer: They looked at the following Facebook pages:\n\nTime\n\n", "answers": ["FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney", "FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney."], "all_classes": null, "length": 5026}
{"pred": "Yes.\n\nQuestion: Do the hashtag and SemEval datasets contain only English data?\n\nAnswer: Yes.\n\nQuestion: Do the hashtag and SemEval datasets contain only English data?\n\nAnswer: Yes.\n\nQuestion: Do the hashtag and SemEval datasets contain only English data?\n\nAnswer: Yes.\n\nQuestion: Do the hashtag and SemEval datasets contain only English data?\n\nAnswer: Yes.\n\nQuestion: Do the hashtag and SemEval datasets contain only English data?\n\nAnswer: Yes.\n", "answers": ["Yes", "Yes"], "all_classes": null, "length": 6442}
{"pred": "The evaluation is based on the automatic evaluation of the baseline method.\n\nQuestion: What is the baseline method?\n\nAnswer: The baseline method is a simple bag-of-words approach.\n\nQuestion: What is the evaluation script?\n\nAnswer: The evaluation script is a script that evaluates the baseline method.\n\nQuestion: What is the corpus?\n\nAnswer: The corpus is a corpus of concept maps.\n\nQuestion: What is the annotation script?\n\nAnswer: The annotation script is a script that annotates the corpus.\n\nQuestion: What", "answers": ["Answer with content missing: (Evaluation Metrics section) Precision, Recall, F1-scores, Strict match, METEOR, ROUGE-2"], "all_classes": null, "length": 6369}
{"pred": "\n\nQuestion: What is the evaluation protocol?\n\nAnswer:\n\nQuestion: What is the evaluation metric?\n\nAnswer:\n\nQuestion: What is the evaluation metric for the extractive setting?\n\nAnswer:\n\nQuestion: What is the evaluation metric for the abstractive setting?\n\nAnswer:\n\nQuestion: What is the evaluation metric for the QA setting?\n\nAnswer:\n\nQuestion: What is the evaluation metric for the abstractive setting?\n\nAnswer:\n\nQuestion: What is the evaluation metric for the QA setting?\n\nAnswer:\n\nQuestion: What", "answers": ["CNN/DailyMail news highlights, New York Times Annotated Corpus, XSum", "the CNN/DailyMail news highlights dataset BIBREF24, the New York Times Annotated Corpus (NYT; BIBREF25), XSum BIBREF22"], "all_classes": null, "length": 7377}
{"pred": "The proposed approach is different from other WSD approaches employing word embeddings in the following ways:\n\n1. The proposed approach uses a variant of max-margin objective based on the asymmetric KL divergence energy function to capture textual entailment (asymmetry) and word similarity (symmetry).\n\n2. The proposed approach uses a Gaussian mixture model to capture multi-sense word distribution.\n\n3. The proposed approach uses a stricter bound on KL between Gaussian mixtures.\n\n4. The proposed approach uses a stricter bound on KL between", "answers": ["GM$\\_$KL achieves better correlation than existing approaches for various metrics on SCWS dataset."], "all_classes": null, "length": 3983}
{"pred": "Their ensemble method works by taking the average of the predictions from each of the constituent single models.\n\nQuestion: How does their ensemble method work?\n\nAnswer: Their ensemble method works by taking the average of the predictions from each of the constituent single models.\n\nQuestion: How does their ensemble method work?\n\nAnswer: Their ensemble method works by taking the average of the predictions from each of the constituent single models.\n\nQuestion: How does their ensemble method work?\n\nAnswer: Their ensemble method works by taking the average of the predictions from each of the constituent single models.\n\nQuestion", "answers": ["simply averaging the predictions from the constituent single models"], "all_classes": null, "length": 5931}
{"pred": "The source of the Friends dataset is the Friends TV show. The source of the EmotionPush dataset is the Twitter dataset.\n\nQuestion: What is the size of the datasets?\n\nAnswer: The size of the Friends dataset is 1,000 dialogues. The size of the EmotionPush dataset is 1,000 dialogues.\n\nQuestion: What is the size of the training and validation datasets?\n\nAnswer: The size of the training dataset is 920 dialogues. The size of the validation dataset is 180 dialogues.\n\nQuestion", "answers": ["Friends TV sitcom, Facebook messenger chats"], "all_classes": null, "length": 5132}
{"pred": "English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\n", "answers": ["English", "Simple English"], "all_classes": null, "length": 3672}
{"pred": "IMDb dataset\n\nQuestion: What is the optimal combination of word2vec hyper-parameters for intrinsic and extrinsic NLP purposes?\n\nAnswer: The optimal combination of word2vec hyper-parameters for intrinsic and extrinsic NLP purposes is the combination of skipgram using hierarchical softmax and window size of 8 for 300 dimensions.\n\nQuestion: What is the optimal combination of word2vec hyper-parameters for intrinsic and extrinsic NLP purposes?\n\nAnswer: The optimal combination of word2vec hyper-parameters for intrinsic and extrinsic NLP purposes is the combination", "answers": ["IMDb dataset of movie reviews", "IMDb"], "all_classes": null, "length": 3827}
{"pred": "The proposed system achieves 99.9% accuracy on the DL-PS dataset, 99.9% accuracy on the EC-MT dataset, and 99.9% accuracy on the EC-UQ dataset.\n\nQuestion: What is the difference between the proposed system and the strong baseline system?\n\nAnswer: The proposed system is able to achieve 99.9% accuracy on the DL-PS dataset, 99.9% accuracy on the EC-MT dataset, and 99.9% accuracy on the EC-UQ dataset.\n\nQuestion", "answers": ["F1 scores of 85.99 on the DL-PS data,  75.15 on the EC-MT data and 71.53 on the EC-UQ data ", "F1 of 85.99 on the DL-PS dataset (dialog domain); 75.15 on EC-MT and 71.53 on EC-UQ (e-commerce domain)"], "all_classes": null, "length": 9246}
{"pred": "Yes\n\nQuestion: What is the name of the dataset?\n\nAnswer: ZuCo 2.0\n\nQuestion: What is the purpose of the dataset?\n\nAnswer: The purpose of the dataset is to provide a freely available corpus of eye movement and electrical brain activity recordings during natural reading as well as during annotation.\n\nQuestion: What is the corpus size?\n\nAnswer: The corpus size is 1800 sentences.\n\nQuestion: What is the corpus format?\n\nAnswer: The corpus format is a text file with one sentence per line.\n", "answers": ["No"], "all_classes": null, "length": 5465}
{"pred": "What is the best way to answer a question?\n\nAnswer: What is the best way to answer a question?\n\nAnswer: What is the best way to answer a question?\n\nAnswer: What is the best way to answer a question?\n\nAnswer: What is the best way to answer a question?\n\nAnswer: What is the best way to answer a question?\n\nAnswer: What is the best way to answer a question?\n\nAnswer: What is the best way to answer a question?\n\nAnswer: What is the best way to answer a question?\n\nAnswer: What is", "answers": ["Custom dataset with user questions; set of documents, twitter posts and news articles, all related to finance.", "a self-collected financial intents dataset in Portuguese"], "all_classes": null, "length": 19949}
{"pred": "Which stock market sector achieved the best performance?\n\nAnswer: Which stock market sector achieved the best performance?\n\nAnswer: Which stock market sector achieved the best performance?\n\nAnswer: Which stock market sector achieved the best performance?\n\nAnswer: Which stock market sector achieved the best performance?\n\nAnswer: Which stock market sector achieved the best performance?\n\nAnswer: Which stock market sector achieved the best performance?\n\nAnswer: Which stock market sector achieved the best performance?\n\nAnswer: Which stock market sector achieved the best performance?\n\nAnswer: Which stock market sector achieved the best performance?\n\n", "answers": ["Energy with accuracy of 0.538", "Energy"], "all_classes": null, "length": 17702}
{"pred": "\n\nThe RNN-based NMT model is compared with the SMT model.\n\nQuestion: what is the average length of the sentences translated by the NMT models?\n\nAnswer:\n\nThe average length of the sentences translated by the RNN-based NMT model is 17.12.\n\nThe average length of the sentences translated by the SMT model is 15.50.\n\nThe average length of the sentences translated by the Transformer model is 16.78.\n\nThe average length of the sentences translated by the reference is 16.", "answers": ["RNN-based NMT model, Transformer-NMT"], "all_classes": null, "length": 5818}
{"pred": "The three regularization terms are neutral features, maximum entropy, and KL divergence.\n\nQuestion: What is the difference between the three regularization terms?\n\nAnswer: The neutral features regularization term doesn't require any modification of GE-FL but just finding out some common features. The maximum entropy regularization term assumes that the categories are uniformly distributed, which may not be the case in practice, and it will have a degraded performance if the assumption is violated. The KL divergence performs much better on unbalanced corpora than other methods. The reason is that KL divergence util", "answers": ["a regularization term associated with neutral features, the maximum entropy of class distribution regularization term, the KL divergence between reference and predicted class distribution", "a regularization term associated with neutral features,  the maximum entropy of class distribution, KL divergence between reference and predicted class distribution"], "all_classes": null, "length": 5573}
{"pred": "The baselines are the SVM with unigram, bigram, and POS-dependencies, CNN, and RCNN.\n\nQuestion: What is the difference between the SVM and the CNN?\n\nAnswer: The SVM is a linear classifier, while the CNN is a non-linear classifier.\n\nQuestion: What is the difference between the SVM and the RCNN?\n\nAnswer: The SVM is a linear classifier, while the RCNN is a non-linear classifier.\n\nQuestion: What is the difference between the CNN and the RCNN?\n\nAnswer", "answers": ["SVM with unigram, bigram, and trigram features, SVM with average word embedding, SVM with average transformed word embeddings, CNN, ecurrent Convolutional Neural Networks, SVM and deep learning models with comment information", "SVM with unigram, bigram, trigram features, with average word embedding, with average transformed word embeddings, CNN and RCNN, SVM, CNN, RCNN with comment information"], "all_classes": null, "length": 6680}
{"pred": "By 10%\n\nQuestion: How much did they improve?\n\nAnswer: By 10%\n\nQuestion: How much did they improve?\n\nAnswer: By 10%\n\nQuestion: How much did they improve?\n\nAnswer: By 10%\n\nQuestion: How much did they improve?\n\nAnswer: By 10%\n\nQuestion: How much did they improve?\n\nAnswer: By 10%\n\nQuestion: How much did they improve?\n\nAnswer: By 10%\n\nQuestion: How much did they improve", "answers": ["They decrease MAE in 0.34"], "all_classes": null, "length": 4183}
{"pred": "The softmax transformer is a linear transformation that maps a vector of probabilities to a vector of probabilities. The softmax transformer is a linear transformation that maps a vector of probabilities to a vector of probabilities. The softmax transformer is a linear transformation that maps a vector of probabilities to a vector of probabilities. The softmax transformer is a linear transformation that maps a vector of probabilities to a vector of probabilities. The softmax transformer is a linear transformation that maps a vector of probabilities to a vector of probabilities. The softmax transformer is a linear transformation that maps a vector", "answers": ["the attention heads in the proposed adaptively sparse Transformer can specialize more and with higher confidence", "We introduce sparse attention into the Transformer architecture"], "all_classes": null, "length": 8623}
{"pred": "the baseline is the sentence-level translation.\n\nQuestion: what is the DocRepair model?\n\nAnswer: the DocRepair model is a monolingual model that performs automatic post-editing on a sequence of sentence-level translations.\n\nQuestion: what is the main contribution of the paper?\n\nAnswer: the main contribution of the paper is the introduction of the DocRepair model, which performs automatic post-editing on a sequence of sentence-level translations.\n\nQuestion: what is the main limitation of the paper?\n\nAnswer: the main limitation of the paper", "answers": [" MT system on the data released by BIBREF11", "Transformer base, two-pass CADec model"], "all_classes": null, "length": 5973}
{"pred": "We use the following metrics for evaluation:\n\n- Accuracy: Accuracy is the percentage of correct predictions.\n\n- LAS: LAS is the average LAS score over all the languages.\n\n- LAS per language: LAS per language is the average LAS score over all the sentences in the dataset.\n\n- LAS per language per task: LAS per language per task is the average LAS score over all the sentences in the dataset for each task.\n\n- LAS per language per task per language: LAS per language per task per language is the average LAS", "answers": ["translation probabilities, Labeled Attachment Scores (LAS)", "accuracy, Labeled Attachment Scores (LAS)"], "all_classes": null, "length": 6075}
{"pred": "The attention module is pretrained on the MT data.\n\nQuestion: What is the difference between the attention module and the encoder?\n\nAnswer: The attention module is a sub-net of the encoder.\n\nQuestion: What is the difference between the attention module and the decoder?\n\nAnswer: The attention module is a sub-net of the decoder.\n\nQuestion: What is the difference between the attention module and the decoder?\n\nAnswer: The attention module is a sub-net of the decoder.\n\nQuestion: What is the difference between the attention module and", "answers": ["the model is pre-trained on CTC-based ASR task and MT task in the pre-training stage."], "all_classes": null, "length": 8338}
{"pred": "The features obtained are gaze-based features and cognitive features.\n\nQuestion: What kind of stylistic features are obtained?\n\nAnswer: The features obtained are gaze-based features and cognitive features.\n\nQuestion: What kind of stylistic features are obtained?\n\nAnswer: The features obtained are gaze-based features and cognitive features.\n\nQuestion: What kind of stylistic features are obtained?\n\nAnswer: The features obtained are gaze-based features and cognitive features.\n\nQuestion: What kind of stylistic features are obtained?\n\nAnswer:", "answers": ["Unanswerable"], "all_classes": null, "length": 5628}
{"pred": "The encoder has a character-based encoder-decoder architecture.\n\nQuestion: What is the auxiliary objective of the system?\n\nAnswer: The auxiliary objective of the system is to predict the MSD tag of the target form.\n\nQuestion: What is the effect of the multilingual training?\n\nAnswer: The effect of the multilingual training is to improve the performance of the system.\n\nQuestion: What is the effect of the monolingual finetuning?\n\nAnswer: The effect of the monolingual finetuning is to improve the performance of the", "answers": ["LSTM", "LSTM"], "all_classes": null, "length": 3500}
{"pred": "WordNet is a lexical database for English. It is not a knowledge base.\n\nQuestion: Is WordNet useful for taxonomic reasoning for this task?\n\nAnswer: WordNet is a lexical database for English. It is not a knowledge base.\n\nQuestion: Is WordNet useful for taxonomic reasoning for this task?\n\nAnswer: WordNet is a lexical database for English. It is not a knowledge base.\n\nQuestion: Is WordNet useful for taxonomic reasoning for this task?\n\nAnswer: WordNet is a lexical database for English. It is not a", "answers": ["Unanswerable", "Yes"], "all_classes": null, "length": 10168}
{"pred": "\n\nThe baselines are:\n\n1. A conventional automatic speech recognition (ASR) system typically consists of several independently learned components: an acoustic model to predict context-dependent sub-phoneme states (senones) from audio, a graph structure to map senones to phonemes, and a pronunciation model to map phonemes to words.\n\n2. Hybrid systems combine hidden Markov models to model state dependencies with neural networks to predict states BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 . Newer approaches such as end-to-", "answers": ["Unanswerable", "LF-MMI Attention\nSeq2Seq \nRNN-T \nChar E2E LF-MMI \nPhone E2E LF-MMI \nCTC + Gram-CTC"], "all_classes": null, "length": 3273}
{"pred": "100\n\nQuestion: How many users do they look at?\n\nAnswer: 100\n\nQuestion: How many users do they look at?\n\nAnswer: 100\n\nQuestion: How many users do they look at?\n\nAnswer: 100\n\nQuestion: How many users do they look at?\n\nAnswer: 100\n\nQuestion: How many users do they look at?\n\nAnswer: 100\n\nQuestion: How many users do they look at?\n\nAnswer: 100\n\nQuestion:", "answers": ["22,880 users", "20,000"], "all_classes": null, "length": 6060}
{"pred": "\n\nThe metrics used for evaluation are:\n\n1. BLEU-1/4: BLEU-1/4 are the BLEU scores for the first/fourth word of the generated recipe.\n\n2. ROUGE-L: ROUGE-L is the ROUGE score for the longest common subsequence of the generated recipe and the gold recipe.\n\n3. UMA: UMA is the recipe-level coherence score for the generated recipe.\n\n4. Step Entailment: Step Entailment is the recipe-", "answers": ["Byte-Pair Encoding perplexity  (BPE PPL),\nBLEU-1,\nBLEU-4,\nROUGE-L,\npercentage of distinct unigram (D-1),\npercentage of distinct bigrams(D-2),\nuser matching accuracy(UMA),\nMean Reciprocal Rank(MRR)\nPairwise preference over baseline(PP)", "BLEU-1/4 and ROUGE-L, likelihood of generated recipes using identical input specifications but conditioned on ten different user profiles, user matching accuracy (UMA), Mean Reciprocal Rank (MRR), neural scoring model from BIBREF33 to measure recipe-level coherence", " Distinct-1/2, UMA = User Matching Accuracy, MRR\n= Mean Reciprocal Rank, PP = Pairwise preference over baseline (evaluated for 310 recipe pairs per model)"], "all_classes": null, "length": 4718}
{"pred": "They create the following labels:\n\n1. \"No answer\"\n\n2. \"Yes\"\n\n3. \"No\"\n\n4. \"Unanswerable\"\n\n5. \"Other\"\n\n6. \"Unknown\"\n\n7. \"Not applicable\"\n\n8. \"Not applicable\"\n\n9. \"Not applicable\"\n\n10. \"Not applicable\"\n\n11. \"Not applicable\"\n\n12. \"Not applicable\"\n\n13. \"Not applicable\"\n\n14. \"Not applicable\"\n\n15. \"Not applicable", "answers": ["(1) the time the patient has been experiencing the symptom, (2) activities that trigger the symptom (to occur or worsen), (3) the extent of seriousness, (4) the frequency occurrence of the symptom, and (5) the location of symptom, No Answer", "the time the patient has been experiencing the symptom, activities that trigger the symptom, the extent of seriousness, the frequency occurrence of the symptom, the location of symptom, 9 symptoms"], "all_classes": null, "length": 5599}
{"pred": "The model was trained on 1000 random sentences.\n\nQuestion: How much data is needed to train the universal encoder?\n\nAnswer: The model was trained on 1000 random sentences.\n\nQuestion: How much data is needed to train the re-weighted model?\n\nAnswer: The model was trained on 1000 random sentences.\n\nQuestion: How much data is needed to train the re-weighted model?\n\nAnswer: The model was trained on 1000 random sentences.\n\nQuestion: How much data is needed to train the", "answers": ["57,505 sentences", "57,505 sentences"], "all_classes": null, "length": 6460}
{"pred": "The tasks used for evaluation are the ones that are used to evaluate the performance of the model.\n\nQuestion: What is the difference between the $\\alpha $-entmax and the softmax?\n\nAnswer: The $\\alpha $-entmax is a transformation of the softmax. The softmax is a transformation of the logits.\n\nQuestion: What is the difference between the $\\alpha $-entmax and the softmax?\n\nAnswer: The $\\alpha $-entmax is a transformation of the softmax. The softmax is a transformation of the logits.\n\nQuestion: What is the difference between the $\\", "answers": ["four machine translation tasks: German -> English, Japanese -> English, Romanian -> English, English -> German", " four machine translation tasks, IWSLT 2017 German $\\rightarrow $ English BIBREF27, KFTT Japanese $\\rightarrow $ English BIBREF28, WMT 2016 Romanian $\\rightarrow $ English BIBREF29, WMT 2014 English $\\rightarrow $ German BIBREF30"], "all_classes": null, "length": 8616}
{"pred": "The improvement in performance for Estonian in the NER task is significant.\n\nQuestion: What is the improvement in performance for Slovenian in the NER task?\n\nAnswer: The improvement in performance for Slovenian in the NER task is significant.\n\nQuestion: What is the improvement in performance for Finnish in the NER task?\n\nAnswer: The improvement in performance for Finnish in the NER task is significant.\n\nQuestion: What is the improvement in performance for Swedish in the NER task?\n\nAnswer: The improvement in performance for Swedish in the NER task is significant.\n", "answers": ["5 percent points.", "0.05 F1"], "all_classes": null, "length": 5218}
{"pred": "The background is the information that you have about the world.\n\nQuestion: What is the question?\n\nAnswer: The question is the question that you are asking.\n\nQuestion: What is the answer?\n\nAnswer: The answer is the answer to the question.\n\nQuestion: What is the question?\n\nAnswer: The question is the question that you are asking.\n\nQuestion: What is the answer?\n\nAnswer: The answer is the answer to the question.\n\nQuestion: What is the question?\n\nAnswer: The question is the question that you are asking.\n\nQuestion", "answers": ["Unanswerable"], "all_classes": null, "length": 11882}
{"pred": "Yes, the paper is introducing an unsupervised approach to spam detection.\n\nQuestion: What is the difference between the two datasets?\n\nAnswer: The Honeypot dataset is collected by Lee et al. lee2010devils, and the Weibo dataset is collected by us.\n\nQuestion: What is the difference between the two feature extraction methods?\n\nAnswer: The two feature extraction methods are based on the topic probability vector. The first method is to extract the topic probability vector for each user, and the second method is to extract the topic probability vector for each", "answers": ["No", "No"], "all_classes": null, "length": 4099}
{"pred": "The Nguni languages are similar to each other and harder to distinguish. The same is true of the Sotho languages.\n\nQuestion: Which languages are dissimilar to each other?\n\nAnswer: The Nguni languages are dissimilar to each other and harder to distinguish. The same is true of the Sotho languages.\n\nQuestion: Which languages are similar to English?\n\nAnswer: The Nguni languages are similar to English. The same is true of the Sotho languages.\n\nQuestion: Which languages are dissimilar to English?\n\nAnswer: The Nguni languages are dissimilar to English", "answers": ["Nguni languages (zul, xho, nbl, ssw), Sotho languages (nso, sot, tsn)", "The Nguni languages are similar to each other, The same is true of the Sotho languages"], "all_classes": null, "length": 2984}
{"pred": "They compared the performance of 2-layers LSTM model with 2-layers LSTM model trained with sMBR.\n\nQuestion: what is the difference between the two lstm models?\n\nAnswer: The 2-layers LSTM model trained with sMBR is distilled from 7-layers LSTM model.\n\nQuestion: what is the difference between the two lstm models?\n\nAnswer: The 2-layers LSTM model trained with sMBR is distilled from 7-layers LSTM model.\n\nQuestion: what is the difference", "answers": ["Unidirectional LSTM networks with 2, 6, 7, 8, and  9 layers."], "all_classes": null, "length": 6270}
{"pred": "The data set is a subset of the arXiv dataset, which contains 1,000,000 articles.\n\nQuestion: How many articles are in the data set?\n\nAnswer: The data set contains 1,000,000 articles.\n\nQuestion: How many articles are in the data set that are not accepted?\n\nAnswer: The data set contains 1,000,000 articles. Of these, 1,000,000 are accepted, and 0 are not accepted.\n\nQuestion: How many articles are", "answers": ["a sample of  29,794 wikipedia articles and 2,794 arXiv papers "], "all_classes": null, "length": 6432}
{"pred": "The human judgements were assembled by the authors of the article.\n\nQuestion: What is the purpose of the article?\n\nAnswer: The purpose of the article is to show how the attention mechanism can be used to improve the performance of the translation model.\n\nQuestion: What is the purpose of the attention mechanism?\n\nAnswer: The purpose of the attention mechanism is to enable a correspondence between the sentences of the two languages when one of them is a simple fusional language.\n\nQuestion: What is the purpose of the encoderâ€“decoder architecture?\n\nAnswer: The purpose of the enc", "answers": ["50 human annotators ranked a random sample of 100 translations by Adequacy, Fluency and overall ranking on a 5-point scale.", "adequacy, precision and ranking values"], "all_classes": null, "length": 8014}
{"pred": "Yes, they do.\n\nQuestion: Do they test their framework performance on a language pair that is not commonly used, such as English-to-French?\n\nAnswer: No, they do not.\n\nQuestion: Do they test their framework performance on a language pair that is not commonly used, such as English-to-French?\n\nAnswer: No, they do not.\n\nQuestion: Do they test their framework performance on a language pair that is not commonly used, such as English-to-French?\n\nAnswer: No, they do not.\n\nQuestion: Do they", "answers": ["Yes", "Yes"], "all_classes": null, "length": 6518}
{"pred": "The models are evaluated by measuring the accuracy of the decoder in reconstructing the target sentence from the keywords.\n\nQuestion: What is the main technical contribution of this paper?\n\nAnswer: The main technical contribution is to draw inspiration from the multi-objective optimization literature and view the tradeoff as a sequence of constrained optimization problems, where we minimize the expected cost subject to varying expected reconstruction error constraints.\n\nQuestion: What is the main takeaway of this paper?\n\nAnswer: The main takeaway is that we can learn communication schemes that are both efficient and accurate by jointly training", "answers": ["by training an autocomplete system on 500K randomly sampled sentences from Yelp reviews", "efficiency of a communication scheme $(q_{\\alpha },p_{\\beta })$ by the retention rate of tokens, which is measured as the fraction of tokens that are kept in the keywords, accuracy of a scheme is measured as the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence"], "all_classes": null, "length": 3024}
{"pred": "\n\nAnswer the question based on the above article as concisely as you can, using a single phrase or sentence if possible. If the question cannot be answered based on the information in the article, write \"unanswerable\". If the question is a yes/no question, answer \"yes\", \"no\", or \"unanswerable\". Do not provide any explanation.\n\nQuestion: What is the difference between a sentence and a phrase?\n\nAnswer:\n\nAnswer the question based on the above article as concisely as you can, using a single phrase or sentence if possible. If the question cannot be answered", "answers": ["Precision, Recall, F-measure, accuracy", "Precision, Recall and F-measure"], "all_classes": null, "length": 4973}
{"pred": "The source domain is the domain of the source reviews and the target domain is the domain of the target reviews.\n\nQuestion: What is the dataset?\n\nAnswer: The dataset is the Amazon review dataset.\n\nQuestion: What is the task?\n\nAnswer: The task is to predict whether a review is positive or negative.\n\nQuestion: What is the model?\n\nAnswer: The model is a neural network.\n\nQuestion: What is the training set?\n\nAnswer: The training set is the source reviews.\n\nQuestion: What is the test set?\n\nAnswer: The test set", "answers": ["Book, electronics, beauty, music, IMDB, Yelp, cell phone, baby, DVDs, kitchen", "we use set 1 of the source domain as the only source with sentiment label information during training, and we evaluate the trained model on set 1 of the target domain, Book (BK), Electronics (E), Beauty (BT), and Music (M)"], "all_classes": null, "length": 7696}
{"pred": "\n\nThe PRU is compared with the following RNN models:\n\n1. LSTM (Bengio et al., 2007)\n\n2. RNN (Elman et al., 1990)\n\n3. GRU (Cho et al., 2014)\n\n4. QRNN (Sutskever et al., 2011)\n\n5. RNN-T (Graves et al., 2014)\n\n6. LSTM-T (Graves et al., ", "answers": ["Variational LSTM, CharCNN, Pointer Sentinel-LSTM, RHN, NAS Cell, SRU, QRNN, RAN, 4-layer skip-connection LSTM, AWD-LSTM, Quantized LSTM"], "all_classes": null, "length": 5321}
{"pred": "NeuronBlocks includes the following modules:\n\n- Word/character embedding\n- CNN/RNN/BiLSTM/Transformer\n- Attention\n- Dropout\n- Layer Norm\n- Batch Norm\n- Focal Loss\n- F1/Accuracy\n- AUC\n- MSE/RMSE\n- ExactMatch/F1\n\nQuestion: What are the supported NLP tasks in NeuronBlocks?\n\nAnswer: NeuronBlocks supports the following NLP tasks:\n\n- Text Classification\n- Text Matching\n", "answers": ["Embedding Layer, Neural Network Layers, Loss Function, Metrics", "Embedding Layer, Neural Network Layers, Loss Function, Metrics"], "all_classes": null, "length": 2819}
{"pred": "\n\nAnswer the question based on the above article as concisely as you can, using a single phrase or sentence if possible. If the question cannot be answered based on the information in the article, write \"unanswerable\". If the question is a yes/no question, answer \"yes\", \"no\", or \"unanswerable\". Do not provide any explanation.\n\nQuestion: what is the best way to train a neural network for this task?\n\nAnswer:\n\nAnswer the question based on the above article as concisely as you can, using a single phrase or sentence if possible. If the question", "answers": ["the Carnegie Mellon Pronouncing Dictionary BIBREF12, the multilingual pronunciation corpus collected by deri2016grapheme , ranscriptions extracted from Wiktionary", "multilingual pronunciation corpus collected by deri2016grapheme"], "all_classes": null, "length": 4924}
{"pred": "The baselines are the following:\n\n1. BERT: bert-base-uncaseds3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz (The model used by BIBREF12)\n\n2. RoBERTa: roberta-bases3.amazonaws.com/models.huggingface.co/bert/roberta-base-pytorch_model.bin (RoBERTa-base does not have an uncased variant)\n\n3. XLNet", "answers": ["varied from Maximum Entropy Classifiers (BIBREF4) to Support Vector Machines (BIBREF5,BIBREF6,BIBREF7,BIBREF8), Recursive Neural Networks (BIBREF9,BIBREF10), Convolutional Neural Networks (BIBREF11) and most recently transfer learning-based architectures like Bidirectional Encoder Representation from Transformers (BERT) (BIBREF12)"], "all_classes": null, "length": 3848}
{"pred": "English, Spanish, French, German, Italian, Portuguese, Russian, and Chinese.\n\nQuestion: What is the difference between the Translate-Test and the Zero-Shot approaches?\n\nAnswer: The Translate-Test approach trains the model on the same language it is tested on, whereas the Zero-Shot approach trains the model on a different language.\n\nQuestion: What is the difference between the Translate-Test and the Translate-Train approaches?\n\nAnswer: The Translate-Test approach trains the model on the same language it is tested on, whereas the Translate-Train", "answers": ["English\nFrench\nSpanish\nGerman\nGreek\nBulgarian\nRussian\nTurkish\nArabic\nVietnamese\nThai\nChinese\nHindi\nSwahili\nUrdu\nFinnish", "English, Spanish, Finnish"], "all_classes": null, "length": 6303}
{"pred": "They test their method on predicting hashtags for a held-out set of posts.\n\nQuestion: What is the size of the training set?\n\nAnswer: The training set contains 2 million tweets for training, 10K for validation and 50K for testing, with a total of 2039 distinct hashtags.\n\nQuestion: What is the size of the test set?\n\nAnswer: The test set contains 2,000 posts which had the maximum and minimum number of out-of-vocabulary words respectively, where vocabul", "answers": ["None"], "all_classes": null, "length": 4031}
{"pred": "Yes.\n\nQuestion: Do they use a recurrent neural network?\n\nAnswer: Yes.\n\nQuestion: Do they use a convolutional neural network?\n\nAnswer: No.\n\nQuestion: Do they use a bidirectional recurrent neural network?\n\nAnswer: No.\n\nQuestion: Do they use a bidirectional convolutional neural network?\n\nAnswer: No.\n\nQuestion: Do they use a convolutional recurrent neural network?\n\nAnswer: No.\n\nQuestion: Do they use a convolutional bidirectional recurrent neural network?\n\nAnswer: No", "answers": ["Yes", "Yes"], "all_classes": null, "length": 6540}
{"pred": "PolyResponse was evaluated against a baseline that uses a standard retrieval model to retrieve the top 10 responses per user utterance.\n\nQuestion: What is the size of the training data?\n\nAnswer: The training data is a subset of the Reddit dataset, which contains 725M sentences extracted from online reviews.\n\nQuestion: What is the size of the test data?\n\nAnswer: The test data is a subset of the Yelp dataset, which contains 125,830 sentences extracted from online reviews.\n\nQuestion: What is the size of the test data", "answers": ["No", "No"], "all_classes": null, "length": 4464}
{"pred": "They use the Linguistic Inquiry and Word Count (LIWC) BIBREF12 .\n\nQuestion: What is the LIWC?\n\nAnswer: The LIWC is a lexical analysis tool that measures the psychological dimensions of people's language.\n\nQuestion: What are the dimensions of the LIWC?\n\nAnswer: The dimensions of the LIWC are:\n\n1. Values\n2. Mood\n3. Social\n4. Cognition\n5. Sensation\n6. Arousal\n7. Thought\n8.", "answers": ["using the Meaning Extraction Method", "Unanswerable"], "all_classes": null, "length": 2318}
{"pred": "The question is a yes/no question, answer \"yes\", \"no\", or \"unanswerable\".\n\nThe question is a yes/no question, answer \"yes\", \"no\", or \"unanswerable\".\n\nThe question is a yes/no question, answer \"yes\", \"no\", or \"unanswerable\".\n\nThe question is a yes/no question, answer \"yes\", \"no\", or \"unanswerable\".\n\nThe question is a yes/no question, answer \"yes\", \"no\", or \"unanswerable\".\n\nThe question is a yes/no question", "answers": ["claim, premise, backing, rebuttal, and refutation", "claim, premise, backing, rebuttal, refutation"], "all_classes": null, "length": 22543}
{"pred": "1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, ", "answers": ["Unanswerable", "Answer with content missing: (Parent subsections) combine precisions for n-gram orders 1-4"], "all_classes": null, "length": 5866}
{"pred": "The Twitter dataset consists of 295,000 conversations.\n\nQuestion: How large is the OSG dataset?\n\nAnswer: The OSG dataset consists of 295,000 conversations.\n\nQuestion: How many conversations in the Twitter dataset are therapeutic?\n\nAnswer: 10% of conversations in the Twitter dataset are therapeutic.\n\nQuestion: How many conversations in the OSG dataset are therapeutic?\n\nAnswer: 10% of conversations in the OSG dataset are th", "answers": ["1,873 Twitter conversation threads, roughly 14k tweets", "1,873 Twitter conversation threads, roughly 14k tweets"], "all_classes": null, "length": 5470}
{"pred": "The question is a yes/no question, answer \"yes\", \"no\", or \"unanswerable\". Do not provide any explanation.\n\n### Unanswerable questions\n\nAnswer: The question is a yes/no question, answer \"yes\", \"no\", or \"unanswerable\". Do not provide any explanation.\n\n### Unanswerable questions\n\nAnswer: The question is a yes/no question, answer \"yes\", \"no\", or \"unanswerable\". Do not provide any explanation.\n\n### Unanswerable questions\n\nAnswer: The question is a yes/no question", "answers": ["Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese", "Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese"], "all_classes": null, "length": 24381}
{"pred": "The model is applied to two datasets: CMV and Wikipedia.\n\nQuestion: What is the model?\n\nAnswer: The model is a neural network that processes comments as they happen and takes the full conversational context into account to make an updated prediction at each step.\n\nQuestion: What is the model's performance?\n\nAnswer: The model achieves state-of-the-art performance on the task of forecasting derailment in two different datasets that we release publicly.\n\nQuestion: What is the model's architecture?\n\nAnswer: The model is a neural network that", "answers": [" `Conversations Gone Awry' dataset, subreddit ChangeMyView", "An expanded version of the existing 'Conversations Gone Awry' dataset and the ChangeMyView dataset, a subreddit whose only annotation is whether the conversation required action by the Reddit moderators. "], "all_classes": null, "length": 7557}
{"pred": "No.\n\nReferences\n\n[1] Agatha Project. 2018. Agatha: AI-based analysis of open sources information for surveillance/crime control. [Online]. Available: http://agatha-project.eu/\n\n[2] Agatha Project. 2018. Agatha: AI-based analysis of open sources information for surveillance/crime control. [Online]. Available: http://agatha-project.eu/\n\n[3] Agatha Project. 2", "answers": ["No", "No"], "all_classes": null, "length": 3502}
{"pred": "The quality of the data is empirically evaluated by comparing the results of the baseline models to the human translations. The human translations are used as a gold standard to evaluate the quality of the baseline models.\n\nQuestion: What is the size of the dataset? \n\nAnswer: The size of the dataset is 11 hours of speeches in 11 languages.\n\nQuestion: What is the size of the dataset? \n\nAnswer: The size of the dataset is 11 hours of speeches in 11 languages.\n\nQuestion: What is the size of the dataset?", "answers": ["Validated transcripts were sent to professional translators., various sanity checks to the translations,  sanity check the overlaps of train, development and test sets", "computed sentence-level BLEU, We manually inspected examples where the source transcript was identical to the translation, measured the perplexity of the translations, computed the ratio of English characters in the translations, calculate similarity scores between transcripts and translations"], "all_classes": null, "length": 4237}
{"pred": "The audio and text sequences are combined using a feed-forward neural network.\n\nQuestion: What is the difference between the ARE and TRE models?\n\nAnswer: The ARE model uses only audio features, while the TRE model uses both audio and text features.\n\nQuestion: What is the difference between the MDRE and MDREA models?\n\nAnswer: The MDRE model uses only audio features, while the MDREA model uses both audio and text features.\n\nQuestion: What is the difference between the ARE and MDRE models?\n\nAnswer: The ARE model uses only", "answers": ["combines the information from these sources using a feed-forward neural model", "encodes the information from audio and text sequences using dual RNNs and then combines the information from these sources using a feed-forward neural model"], "all_classes": null, "length": 4973}
{"pred": "by 2.11 BLEU, 1.7 FKGL and 1.07 SARI\n\nQuestion: how does the method compare to other methods?\n\nAnswer: NMT+synthetic is significantly better than PBMT-R, Dress, and SBMT-SARI on Simplicity.\n\nQuestion: what is the effectiveness of the method?\n\nAnswer: Our method produces better results comparing with the baselines, which demonstrates the effectiveness of adding simplified training data.\n\nQuestion: what is the conclusion of the paper?\n\n", "answers": ["For the WikiLarge dataset, the improvement over baseline NMT is 2.11 BLEU, 1.7 FKGL and 1.07 SARI.\nFor the WikiSmall dataset, the improvement over baseline NMT is  8.37 BLEU.", "6.37 BLEU"], "all_classes": null, "length": 3672}
{"pred": "1\n\nQuestion: how many humans evaluated the results?\n\nAnswer: 1\n\nQuestion: how many humans evaluated the results?\n\nAnswer: 1\n\nQuestion: how many humans evaluated the results?\n\nAnswer: 1\n\nQuestion: how many humans evaluated the results?\n\nAnswer: 1\n\nQuestion: how many humans evaluated the results?\n\nAnswer: 1\n\nQuestion: how many humans evaluated the results?\n\nAnswer: 1\n\nQuestion: how many humans evaluated the results?\n\nAnswer: 1\n\nQuestion: how many humans", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 5974}
{"pred": "\n\nAnswer the question based on the above article as concisely as you can, using a single phrase or sentence if possible. If the question cannot be answered based on the information in the article, write \"unanswerable\". If the question is a yes/no question, answer \"yes\", \"no\", or \"unanswerable\". Do not provide any explanation.\n\nQuestion: What is their definition of fake news?\n\nAnswer:\n\nAnswer the question based on the above article as concisely as you can, using a single phrase or sentence if possible. If the question cannot be answered based on the", "answers": ["Viral tweets are the ones that are retweeted more than 1000 times", "those that contain a high number of retweets"], "all_classes": null, "length": 4589}
{"pred": "CNN\n\nQuestion: Which basic neural architecture perform best by itself?\n\nAnswer: CNN\n\nQuestion: Which basic neural architecture perform best by itself?\n\nAnswer: CNN\n\nQuestion: Which basic neural architecture perform best by itself?\n\nAnswer: CNN\n\nQuestion: Which basic neural architecture perform best by itself?\n\nAnswer: CNN\n\nQuestion: Which basic neural architecture perform best by itself?\n\nAnswer: CNN\n\nQuestion: Which basic neural architecture perform best by itself?\n\nAnswer: CNN\n\nQuestion: Which basic neural architecture perform best by itself?\n\nAnswer: CNN\n", "answers": ["BERT"], "all_classes": null, "length": 2932}
{"pred": "The data was collected using crowdsourcing.\n\nQuestion: what is the size of the database?\n\nAnswer: The size of the database is 1000 speakers for each language.\n\nQuestion: what is the size of the test set?\n\nAnswer: The size of the test set is 1000 speakers for each language.\n\nQuestion: what is the size of the training set?\n\nAnswer: The size of the training set is 1000 speakers for each language.\n\nQuestion: what is the size of the development set?\n\n", "answers": ["Android application"], "all_classes": null, "length": 5870}
{"pred": "The RQE method is a hybrid of machine learning and deep learning methods.\n\nQuestion: What is the best way to train a deep learning model for RQE?\n\nAnswer: The best way to train a deep learning model for RQE is to use a large collection of question-answer pairs.\n\nQuestion: What is the best way to train a deep learning model for RQE?\n\nAnswer: The best way to train a deep learning model for RQE is to use a large collection of question-answer pairs.\n\nQuestion: What is the best way to train a deep", "answers": ["Logistic Regression, neural networks"], "all_classes": null, "length": 11716}
{"pred": "The benchmark dataset is the Honeypot dataset. It is a public dataset and its quality is high.\n\nQuestion: What is the dataset and is its quality high?\n\nAnswer: The dataset is the Weibo dataset. It is a self-built dataset and its quality is high.\n\nQuestion: What is the dataset and is its quality high?\n\nAnswer: The dataset is the Weibo dataset. It is a self-built dataset and its quality is high.\n\nQuestion: What is the dataset and is its quality high?\n\nAnswer: The dataset is the Weibo dataset", "answers": ["Social Honeypot dataset (public) and Weibo dataset (self-collected); yes", "Social Honeypot, which is not of high quality"], "all_classes": null, "length": 4086}
{"pred": "The decoder has an LSTM encoder and an LSTM decoder.\n\nQuestion: What is the context window?\n\nAnswer: The context window is the entire available context.\n\nQuestion: What is the auxiliary objective?\n\nAnswer: The auxiliary objective is to predict the MSD tag of the target form.\n\nQuestion: What is the multilingual approach?\n\nAnswer: The multilingual approach is to train the auxiliary component in a multilingual fashion, over sets of two to three languages.\n\nQuestion: What is the effect of the multilingual", "answers": ["LSTM", "LSTM"], "all_classes": null, "length": 3500}
{"pred": "Yes.\n\nQuestion: Do they report results only on Chinese data?\n\nAnswer: No.\n\nQuestion: Do they report results only on English data?\n\nAnswer: No.\n\nQuestion: Do they report results only on Chinese data?\n\nAnswer: No.\n\nQuestion: Do they report results only on English data?\n\nAnswer: No.\n\nQuestion: Do they report results only on Chinese data?\n\nAnswer: No.\n\nQuestion: Do they report results only on English data?\n\nAnswer: No.\n\nQuestion: Do they report results only on Chinese data", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 6315}
{"pred": "The best performing model is the ensemble of r19 and r21 (i.e., 15 models) on dev (external) set. The F1 is 0.673.\n\nQuestion: What is the best performing model among author's submissions, what performance it had?\n\nAnswer: The best performing model is the ensemble of r19 and r21 (i.e., 15 models) on dev (external) set. The F1 is 0.673.\n\nQuestion: What is the best performing model among author's submissions", "answers": ["For SLC task, the \"ltuorp\" team  has the best performing  model (0.6323/0.6028/0.6649 for F1/P/R  respectively) and for FLC task the \"newspeak\" team  has the best performing  model (0.2488/0.2863/0.2201 for F1/P/R respectively)."], "all_classes": null, "length": 2940}
{"pred": "the baseline is the best model trained on the original parallel data.\n\nQuestion: what is the improvement?\n\nAnswer: the improvement is the BLEU score of the best model trained on the mixture of original parallel and pseudo-parallel data.\n\nQuestion: what is the improvement of the best model?\n\nAnswer: the improvement of the best model is the BLEU score of the best model trained on the mixture of original parallel and pseudo-parallel data.\n\nQuestion: what is the improvement of the best model trained on the mixture of original parallel and pseudo-parallel data?\n\nAnswer: the", "answers": ["pivot-based translation relying on a helping language BIBREF10, nduction of phrase tables from monolingual data BIBREF14 , attentional RNN-based model (RNMT) BIBREF2, Transformer model BIBREF18, bi-directional model BIBREF11, multi-to-multi (M2M) model BIBREF8, back-translation BIBREF17", "M2M Transformer"], "all_classes": null, "length": 7635}
{"pred": "What was their highest recall score?\n\nIn one more experiment, we would like to find the question's highest recall score.\n\nQuestion: What was their highest recall score?\n\nQuestion: What was their highest recall score?\n\nQuestion: What was their highest recall score?\n\nQuestion: What was their highest recall score?\n\nQuestion: What was their highest recall score?\n\nQuestion: What was their highest recall score?\n\nQuestion: What was their highest recall score?\n\nQuestion: What was their highest recall score?\n\nQuestion: What was their highest recall score?\n\n", "answers": ["0.7033", "0.7033"], "all_classes": null, "length": 11028}
{"pred": "The paper explores the use of word embeddings to quantify semantic similarity and relatedness.\n\nQuestion: What is the difference between the two measures of semantic similarity?\n\nAnswer: The two measures of semantic similarity are the cosine similarity and the Jaccard similarity. The cosine similarity is a measure of the similarity between two vectors. The Jaccard similarity is a measure of the similarity between two sets.\n\nQuestion: What is the difference between the two measures of relatedness?\n\nAnswer: The two measures of relatedness are the Spearman's rank correlation and the Pearson's", "answers": ["Skipâ€“gram, CBOW", "integrated vector-res, vector-faith, Skipâ€“gram, CBOW"], "all_classes": null, "length": 6471}
{"pred": "The CFILT-preorder system for reordering English sentences to match the Indian language word order. It contains two re-ordering systems: (1) generic rules that apply to all Indian languages and (2) hindi-tuned rules which improve the generic rules by incorporating improvements found through an error analysis of English-Hindi reordering.\n\nAnswer: The CFILT-preorder system for reordering English sentences to match the Indian language word order. It contains two re-ordering systems: (1) generic rules that apply to all Indian languages and (2) hindi", "answers": ["Unanswerable", "CFILT-preorder system"], "all_classes": null, "length": 3500}
{"pred": "No.\n\nQuestion: Does the paper explore extraction from electronic health records?\n\nAnswer: No.\n\nQuestion: Does the paper explore extraction from electronic health records?\n\nAnswer: No.\n\nQuestion: Does the paper explore extraction from electronic health records?\n\nAnswer: No.\n\nQuestion: Does the paper explore extraction from electronic health records?\n\nAnswer: No.\n\nQuestion: Does the paper explore extraction from electronic health records?\n\nAnswer: No.\n\nQuestion: Does the paper explore extraction from electronic health records?\n\nAnswer: No.", "answers": ["Yes"], "all_classes": null, "length": 4525}
{"pred": "The experts used for annotation were:\n\n* Elias Wright\n* Gian Mascioli\n* Kiara Pillay\n* Harrison Kay\n* Eliel Talo\n* Alexander Fagella\n* N. Cameron Russell\n\nQuestion: What is the answer to the question?\n\nAnswer: The answer to the question is:\n\n* The answer to the question is:\n\nQuestion: What is the answer to the question?\n\nAnswer: The answer to the question is:\n\nQuestion: What is the answer to the question?\n\nAnswer: The answer to the question is", "answers": ["Individuals with legal training", "Yes"], "all_classes": null, "length": 6054}
{"pred": "The models used for painting embedding are CNN-RNN and the models used for language style transfer are Seq2Seq with global attention and Seq2Seq with pointer networks.\n\nAnswer: The models used for painting embedding are CNN-RNN and the models used for language style transfer are Seq2Seq with global attention and Seq2Seq with pointer networks.\n\nAnswer: The models used for painting embedding are CNN-RNN and the models used for language style transfer are Seq2Seq with global attention and Seq2Seq with pointer networks.\n\nAnswer: The models used for painting embedding are CNN-RNN and the models", "answers": ["generating a poem from images we use an existing actor-critic architecture, various types of sequence to sequence models"], "all_classes": null, "length": 2509}
{"pred": "The transformer layer works better.\n\nQuestion: Is the position embedding important for the model?\n\nAnswer: The position embedding is important for the model.\n\nQuestion: Is the model better for long documents or short documents?\n\nAnswer: The model is better for long documents.\n\nQuestion: Is the model better for short documents or long documents?\n\nAnswer: The model is better for long documents.\n\nQuestion: Is the model better for short documents or long documents?\n\nAnswer: The model is better for long documents.\n\nQuestion: Is the model better for short documents or long documents", "answers": ["Transformer over BERT (ToBERT)", "The transformer layer"], "all_classes": null, "length": 4263}
{"pred": "Yes.\n\nQuestion: Do the authors hypothesize that humans' robustness to noise is due to their general knowledge?\n\nAnswer: Yes.\n\nQuestion: Do the authors hypothesize that humans' robustness to noise is due to their general knowledge?\n\nAnswer: Yes.\n\nQuestion: Do the authors hypothesize that humans' robustness to noise is due to their general knowledge?\n\nAnswer: Yes.\n\nQuestion: Do the authors hypothesize that humans' robustness to noise is due to their general knowledge?\n\nAnswer: Yes.\n\nQuestion: Do the", "answers": ["Yes", "Yes"], "all_classes": null, "length": 6614}
{"pred": "They addressed three topics of cyberbullying: sexism, racism, and personal attacks.\n\nQuestion: What SMPs did they use?\n\nAnswer: They used three SMPs: Formspring, Twitter, and Wikipedia.\n\nQuestion: What DNN models did they use?\n\nAnswer: They used four DNN models: logistic regression, support vector machine, random forest, and BLSTM with attention.\n\nQuestion: What transfer learning flavors did they use?\n\nAnswer: They used three flavors of transfer learning: complete transfer learning, feature level transfer learning,", "answers": ["personal attack, racism, and sexism", "racism, sexism, personal attack, not specifically about any single topic"], "all_classes": null, "length": 5034}
{"pred": "The new context representation is obtained by concatenating the left context, the left entity and the middle context.\n\nQuestion: What is the difference between the CNN and the RNN?\n\nAnswer: The CNN is a convolutional neural network which computes a weighted combination of all words in the sentence. The RNN is a recurrent neural network which computes a weighted combination of all words in the sentence and only considers their resulting activations.\n\nQuestion: What is the difference between the CNN and the RNN?\n\nAnswer: The CNN is a convolutional neural network which computes a weighted", "answers": ["They use two independent convolutional and max-pooling layers on (1) a combination of the left context, the left entity and the middle context; and (2) a combination of the middle context, the right entity and the right context. They concatenated the two results after pooling to get the new context representation."], "all_classes": null, "length": 3794}
{"pred": "There are 4 different types of entities in the dataset.\n\nQuestion: What is the most common type of entity in the dataset?\n\nAnswer: The most common type of entity in the dataset is \"PERSON\".\n\nQuestion: What is the most common type of entity in the ILPRL dataset?\n\nAnswer: The most common type of entity in the ILPRL dataset is \"LOCATION\".\n\nQuestion: What is the most common type of entity in the OurNepali dataset?\n\nAnswer: The most common type of entity in the OurNepali dataset is \"PERSON\".", "answers": ["OurNepali contains 3 different types of entities, ILPRL contains 4 different types of entities", "three"], "all_classes": null, "length": 4914}
{"pred": "The resulting annotated data is of higher quality than the original data.\n\nQuestion: How much higher quality is the resulting annotated data?\n\nAnswer: The resulting annotated data is of higher quality than the original data.\n\nQuestion: How much higher quality is the resulting annotated data?\n\nAnswer: The resulting annotated data is of higher quality than the original data.\n\nQuestion: How much higher quality is the resulting annotated data?\n\nAnswer: The resulting annotated data is of higher quality than the original data.\n\nQuestion: How much higher quality is the resulting annotated data?", "answers": ["improvement when the difficult subset with expert annotations is mixed with the remaining crowd annotation is 3.5 F1 score, much larger than when a random set of expert annotations are added"], "all_classes": null, "length": 6457}
{"pred": "\n\nAnswer the question based on the above article as concisely as you can, using a single phrase or sentence if possible. If the question cannot be answered based on the information in the article, write \"unanswerable\". If the question is a yes/no question, answer \"yes\", \"no\", or \"unanswerable\". Do not provide any explanation.\n\nQuestion: Is there a gender bias in ASR performance?\n\nAnswer:\n\nAnswer the question based on the above article as concisely as you can, using a single phrase or sentence if possible. If the question cannot be answered based", "answers": ["Women represent 33.16% of the speakers"], "all_classes": null, "length": 6059}
{"pred": "The article states that the authors' deliberation models achieve the state of the art performance for transformer networks (constrained models) on the English-German dataset, as compared to BIBREF30 .\n\nQuestion: What is the main difference between the deliberation models and the base models?\n\nAnswer: The deliberation models lead to significant improvements over the base performance across test sets (average INLINEFORM0 , INLINEFORM1 ).\n\nQuestion: What is the main difference between the deliberation models and the multimodal models?\n\nAnswer: The deliberation models show significant improvements over", "answers": ["the English-German dataset"], "all_classes": null, "length": 2916}
{"pred": "Our model is compared to the Transformer encoder and the Transformer encoder is compared to the Bi-LSTM encoder.\n\nQuestion: What is the difference between the Transformer encoder and the Bi-LSTM encoder?\n\nAnswer: The Transformer encoder is a self-attention based encoder while the Bi-LSTM encoder is a recurrent neural network based encoder.\n\nQuestion: What is the difference between the Transformer encoder and the Bi-LSTM encoder?\n\nAnswer: The Transformer encoder is a self-attention", "answers": ["Baseline models are:\n- Chen et al., 2015a\n- Chen et al., 2015b\n- Liu et al., 2016\n- Cai and Zhao, 2016\n- Cai et al., 2017\n- Zhou et al., 2017\n- Ma et al., 2018\n- Wang et al., 2019"], "all_classes": null, "length": 5822}
{"pred": "\n\nQuestion: What is the difference between the crowd and the model?\n\nAnswer:\n\nQuestion: What is the difference between the crowd and the model in expectation inference?\n\nAnswer:\n\nQuestion: What is the difference between the crowd and the model in expectation inference?\n\nAnswer:\n\nQuestion: What is the difference between the crowd and the model in expectation inference?\n\nAnswer:\n\nQuestion: What is the difference between the crowd and the model in expectation inference?\n\nAnswer:\n\nQuestion: What is the difference between the crowd and the model in expectation inference?\n\nAnswer", "answers": ["probabilistic model", "Logistic Regression, Multilayer Perceptron"], "all_classes": null, "length": 7400}
{"pred": "\n\nGoogle Cloud\n\nTensiStrength\n\nRosette Text Analytics\n\nCogComp-NLP\n\nStanford NLP NER\n\nTwitterNLP\n\nBIBREF17\n\nBIBREF18\n\nBIBREF19\n\nBIBREF20\n\nBIBREF21\n\nBIBREF22\n\nBIBREF23\n\nBIBREF24\n\nBIBREF25\n\nBIBREF26\n\nBIBREF27\n\nBIBREF0\n\nB", "answers": ["BIBREF17, BIBREF18, TensiStrength BIBREF13, TwitterNLP BIBREF6, BIBREF19, CogComp-NLP BIBREF20, Stanford NLP NER BIBREF21", "BIBREF23, BIBREF17, BIBREF18, BIBREF19, BIBREF24, BIBREF25, BIBREF26"], "all_classes": null, "length": 2560}
{"pred": "The experiments are performed on the SQuAD dataset.\n\nQuestion: What is the performance of the proposed model?\n\nAnswer: The performance of the proposed model is evaluated on the SQuAD dataset. The results are shown in Table 1.\n\nQuestion: What is the performance of the proposed model on the SQuAD dataset?\n\nAnswer: The performance of the proposed model on the SQuAD dataset is shown in Table 1.\n\nQuestion: What is the performance of the proposed model on the SQuAD dataset?\n\nAnswer: The performance of the proposed model on the SQuAD", "answers": ["SQuAD", "SQuAD"], "all_classes": null, "length": 6264}
{"pred": "\n\nQuestion: what is the difference between the proposed method and the existing methods?\n\nAnswer:\n\nQuestion: what are the advantages of the proposed method?\n\nAnswer:\n\nQuestion: what are the disadvantages of the proposed method?\n\nAnswer:\n\nQuestion: what are the limitations of the proposed method?\n\nAnswer:\n\nQuestion: what are the future directions?\n\nAnswer:\n\nQuestion: what are the conclusions?\n\nAnswer:\n\nQuestion: what are the references?\n\nAnswer:\n\nQuestion: what are the acknowledgments?\n\n", "answers": ["BOW-Tags, BOW-KL(Tags), BOW-All, GloVe"], "all_classes": null, "length": 7029}
{"pred": "Yes\n\nQuestion: Do they use attention?\n\nAnswer: No\n\nQuestion: Do they use attention?\n\nAnswer: Unanswerable\n\nQuestion: Do they use attention?\n\nAnswer: Unanswerable\n\nQuestion: Do they use attention?\n\nAnswer: Unanswerable\n\nQuestion: Do they use attention?\n\nAnswer: Unanswerable\n\nQuestion: Do they use attention?\n\nAnswer: Unanswerable\n\nQuestion: Do they use attention?\n\nAnswer: Unanswerable\n\nQuestion: Do they use attention?\n\nAnswer: Unanswerable\n", "answers": ["Yes", "Yes"], "all_classes": null, "length": 2883}
{"pred": "\n\nAnswer the question based on the above article as concisely as you can, using a single phrase or sentence if possible. If the question cannot be answered based on the information in the article, write \"unanswerable\". If the question is a yes/no question, answer \"yes\", \"no\", or \"unanswerable\". Do not provide any explanation.\n\nQuestion: What is the difference between RoBERT and ToBERT?\n\nAnswer:\n\nAnswer the question based on the above article as concisely as you can, using a single phrase or sentence if possible. If the question cannot", "answers": ["CSAT dataset, 20 newsgroups, Fisher Phase 1 corpus", "CSAT dataset , 20 newsgroups, Fisher Phase 1 corpus"], "all_classes": null, "length": 4253}
{"pred": "Penn Treebank\n\nQuestion: What is the size of the vocabulary?\n\nAnswer: 187\n\nQuestion: What is the size of the training set?\n\nAnswer: 209,772\n\nQuestion: What is the size of the test set?\n\nAnswer: 209,772\n\nQuestion: What is the size of the development set?\n\nAnswer: 209,772\n\nQuestion: What is the size of the validation set?\n\nAnswer: 209,772", "answers": ["the IMDb movie review dataset BIBREF17", "IMDb movie review"], "all_classes": null, "length": 5555}
{"pred": "No.\n\nQuestion: Are the results of the BERT models comparable to the results of the LSTM models?\n\nAnswer: Yes.\n\nQuestion: Are the results of the BERT models comparable to the results of the LSTM models?\n\nAnswer: Yes.\n\nQuestion: Are the results of the BERT models comparable to the results of the LSTM models?\n\nAnswer: Yes.\n\nQuestion: Are the results of the BERT models comparable to the results of the LSTM models?\n\nAnswer: Yes.\n\nQuestion: Are the", "answers": ["Yes", "Yes"], "all_classes": null, "length": 2419}
{"pred": "No.\n\nReason: The dataset contains more than twice as many tweets about Trump than about the other candidates.\n\nAnswer: No.\n\nReason: The dataset contains more than twice as many tweets about Trump than about the other candidates.\n\nAnswer: No.\n\nReason: The dataset contains more than twice as many tweets about Trump than about the other candidates.\n\nAnswer: No.\n\nReason: The dataset contains more than twice as many tweets about Trump than about the other candidates.\n\nAnswer: No.\n\nReason: The dataset contains more than", "answers": ["No"], "all_classes": null, "length": 2560}
{"pred": "The invertibility condition is a constraint that the model must satisfy.\n\nQuestion: What is the invertibility condition?\n\nAnswer: The invertibility condition is a constraint that the model must satisfy.\n\nQuestion: What is the invertibility condition?\n\nAnswer: The invertibility condition is a constraint that the model must satisfy.\n\nQuestion: What is the invertibility condition?\n\nAnswer: The invertibility condition is a constraint that the model must satisfy.\n\nQuestion: What is the invertibility condition?\n\nAnswer: The invertibility condition is a constraint that the model must satisfy.\n\nQuestion", "answers": ["The neural projector must be invertible.", "we constrain our neural projector with two requirements: (1) INLINEFORM0 and (2) INLINEFORM1 exists"], "all_classes": null, "length": 6757}
{"pred": "\n\nAnswer the question based on the above article as concisely as you can, using a single phrase or sentence if possible. If the question cannot be answered based on the information in the article, write \"unanswerable\". If the question is a yes/no question, answer \"yes\", \"no\", or \"unanswerable\". Do not provide any explanation.\n\nQuestion: How does proposed qualitative annotation schema looks like?\n\nAnswer:\n\nAnswer the question based on the above article as concisely as you can, using a single phrase or sentence if possible. If the question cannot be answered based", "answers": ["The resulting taxonomy of the framework is shown in Figure FIGREF10", "FIGREF10"], "all_classes": null, "length": 7455}
{"pred": "The size of WikiSmall is 89,042 sentence pairs, and the size of WikiLarge is 296,402 sentence pairs.\n\nQuestion: what are the metrics used to evaluate the performance of text simplification?\n\nAnswer: The metrics are BLEU, FKGL and SARI.\n\nQuestion: what are the results of all models on WikiLarge dataset?\n\nAnswer: The results of all models on WikiLarge dataset are shown in Table 1. We can see that our method (NMT+synthetic)", "answers": ["training set has 89,042 sentence pairs, and the test set has 100 pairs, training set contains 296,402, 2,000 for development and 359 for testing", "WikiSmall  89 142 sentence pair and  WikiLarge 298 761 sentence pairs. "], "all_classes": null, "length": 3672}
{"pred": "The baselines are the methods that are used to train the model.\n\nQuestion: What is the model?\n\nAnswer: The model is the model that is used to train the model.\n\nQuestion: What is the model?\n\nAnswer: The model is the model that is used to train the model.\n\nQuestion: What is the model?\n\nAnswer: The model is the model that is used to train the model.\n\nQuestion: What is the model?\n\nAnswer: The model is the model that is used to train the model.\n\nQuestion: What is the model?\n", "answers": ["Vanilla ST baseline, encoder pre-training, in which the ST encoder is initialized from an ASR model, decoder pre-training, in which the ST decoder is initialized from an MT model, encoder-decoder pre-training, where both the encoder and decoder are pre-trained, many-to-many multi-task model where the encoders and decoders are derived from pre-trained ASR and MT models, Triangle+pre-train: BIBREF18 DBLP:conf/naacl/AnastasopoulosC18 proposed a triangle multi-task strategy for speech translation", "Vanilla ST baseline, Pre-training baselines, Multi-task baselines, Many-to-many+pre-training, Triangle+pre-train", "Vanilla ST baseline: The vanilla ST BIBREF9 has only a speech encoder and a decoder. It is trained from scratch on the ST-TED corpus.\n\nPre-training baselines: We conduct three pre-training baseline experiments: 1) encoder pre-training, in which the ST encoder is initialized from an ASR model; 2) decoder pre-training, in which the ST decoder is initialized from an MT model; and 3) encoder-decoder pre-training, where both the encoder and decoder are pre-trained. The ASR model has the same architecture with vanilla ST model, trained on the mixture of ST-TED and TED-LIUM2 corpus. The MT model has a text encoder and decoder with the same architecture of which in TCEN. It is first trained on WMT data (out-of-domain) and then fine-tuned on in-domain data.\n\nMulti-task baselines: We also conduct three multi-task baseline experiments including one-to-many setting, many-to-one setting, and many-to-many setting. In the first two settings, we train the model with $\\alpha _{st}=0.75$ while $\\alpha _{asr}=0.25$ or $\\alpha _{mt}=0.25$. For many-to-many setting, we use $\\alpha _{st}=0.6, \\alpha _{asr}=0.2$ and $\\alpha _{mt}=0.2$.. For MT task, we use only in-domain data.\n\nMany-to-many+pre-training: We train a many-to-many multi-task model where the encoders and decoders are derived from pre-trained ASR and MT models. "], "all_classes": null, "length": 8334}
{"pred": "English\n\nQuestion: What is the name of the dataset used in this paper?\n\nAnswer: PTC\n\nQuestion: What is the name of the model used in this paper?\n\nAnswer: BERT\n\nQuestion: What is the name of the model used in this paper?\n\nAnswer: BERT\n\nQuestion: What is the name of the model used in this paper?\n\nAnswer: BERT\n\nQuestion: What is the name of the model used in this paper?\n\nAnswer: BERT\n\nQuestion: What is the name of the model used in this paper?\n", "answers": ["Unanswerable", "English"], "all_classes": null, "length": 6658}
{"pred": "The models are trained on the training data, and evaluated by predicting the labels for the held-out test set. The distribution is described in Table TABREF15 . We evaluate and compare the models using the macro-averaged F1-score as the label distribution is highly imbalanced. Per-class Precision (P), Recall (R), and F1-score (F1), also with other averaged metrics are also reported. The models are compared against baselines of predicting all labels as the majority or minority classes.\n\nQuestion: What is the performance of the models?\n", "answers": ["linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM trained on word unigrams,  bidirectional Long Short-Term-Memory (BiLSTM),  Convolutional Neural Network (CNN) "], "all_classes": null, "length": 3661}
{"pred": "No, the questions are not useful.\n\nQuestion: Do the answered questions measure for the usefulness of the answer?\n\nAnswer: No, the questions are not useful.\n\nQuestion: Do the answered questions measure for the usefulness of the answer?\n\nAnswer: No, the questions are not useful.\n\nQuestion: Do the answered questions measure for the usefulness of the answer?\n\nAnswer: No, the questions are not useful.\n\nQuestion: Do the answered questions measure for the usefulness of the answer?\n\nAnswer: No, the questions are not useful.\n\nQuestion:", "answers": ["No"], "all_classes": null, "length": 2366}
{"pred": "GloVe and Edinburgh embeddings\n\nQuestion: what is the best feature extractor?\n\nAnswer: +/-EffectWordNet\n\nQuestion: what is the best feature extractor for anger?\n\nAnswer: +/-EffectWordNet\n\nQuestion: what is the best feature extractor for fear?\n\nAnswer: NRC Hashtag Sentiment Lexicon\n\nQuestion: what is the best feature extractor for sadness?\n\nAnswer: NRC Hashtag Sentiment Lexicon\n\nQuestion: what is the best feature extractor for joy?\n\nAnswer: Sentiment1", "answers": ["Pretrained word embeddings  were not used", "GloVe, Edinburgh embeddings BIBREF14, Emoji embeddings BIBREF16"], "all_classes": null, "length": 2808}
{"pred": "The authors found that their models were able to generate coherent and personalized recipes.\n\nQuestion: What were their results on the new dataset?\n\nAnswer: The authors found that their models were able to generate coherent and personalized recipes.\n\nQuestion: What were their results on the new dataset?\n\nAnswer: The authors found that their models were able to generate coherent and personalized recipes.\n\nQuestion: What were their results on the new dataset?\n\nAnswer: The authors found that their models were able to generate coherent and personalized recipes.\n", "answers": ["average recipe-level coherence scores of 1.78-1.82, human evaluators preferred personalized model outputs to baseline 63% of the time"], "all_classes": null, "length": 4720}
{"pred": "The combination of rewards for reinforcement learning is a common method of reinforcement learning.\n\nQuestion: What is the combination of rewards for reinforcement learning?\n\nAnswer: The combination of rewards for reinforcement learning is a common method of reinforcement learning.\n\nQuestion: What is the combination of rewards for reinforcement learning?\n\nAnswer: The combination of rewards for reinforcement learning is a common method of reinforcement learning.\n\nQuestion: What is the combination of rewards for reinforcement learning?\n\nAnswer: The combination of rewards", "answers": ["irony accuracy, sentiment preservation", " irony accuracy and sentiment preservation"], "all_classes": null, "length": 7005}
{"pred": "The authors demonstrate that their model does not work well with Shakespeare style transfer for the given painting. This happens when the style transfer dataset does not have similar words in the training set of sentences. A solution would be to expand the style transfer dataset, for a better representation of the poem data.\n\nAnswer: The authors demonstrate that their model does not work well with Shakespeare style transfer for the given painting. This happens when the style transfer dataset does not have similar words in the training set of sentences. A solution would be to expand the style transfer dataset, for a better representation of the poem data.\n\nAnswer: The authors demonstrate that", "answers": ["Since we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer", "we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer as shown in Figure FIGREF12 for \"Starry Night\" with a low average content score"], "all_classes": null, "length": 2507}
{"pred": "They compared to the following systems:\n\n1. The system by Tang et al. (2014)\n\n2. The system by Iacobacci et al. (2015)\n\n3. The system by Tang et al. (2015)\n\n4. The system by Iacobacci et al. (2015)\n\n5. The system by Tang et al. (2015)\n\n6. The system by Iacobacci et al. (2015)\n\n7. The system by Tang et al", "answers": ["Affective Text, Fairy Tales, ISEAR", " Affective Text dataset, Fairy Tales dataset, ISEAR dataset"], "all_classes": null, "length": 5027}
{"pred": "The distribution results were that there were differences around exposure, characteristics of accounts spreading fake news and the tone of the content.\n\nQuestion: What were their findings?\n\nAnswer: Their findings were that it is indeed possible to model and automatically detect fake news.\n\nQuestion: What were their conclusions?\n\nAnswer: Their conclusions were that it is indeed possible to model and automatically detect fake news.\n\nQuestion: What were their recommendations?\n\nAnswer: Their recommendations were that it is indeed possible to model and automatically detect fake news.\n\nQuestion: What were their limitations?", "answers": ["Distributions of Followers, Friends and URLs are significantly different between the set of tweets containing fake news and those non containing them, but for Favourites, Mentions, Media, Retweets and Hashtags they are not significantly different"], "all_classes": null, "length": 4584}
{"pred": "The dataset of hashtags is sourced from Twitter.\n\nQuestion: How is the dataset of hashtags sourced?\n\nAnswer: The dataset of hashtags is sourced from Twitter.\n\nQuestion: How is the dataset of hashtags sourced?\n\nAnswer: The dataset of hashtags is sourced from Twitter.\n\nQuestion: How is the dataset of hashtags sourced?\n\nAnswer: The dataset of hashtags is sourced from Twitter.\n\nQuestion: How is the dataset of hashtags sourced?", "answers": ["1,268 randomly selected tweets in the Stanford Sentiment Analysis Dataset BIBREF36, all 12,594 unique English hashtags and their associated tweets from the same Stanford dataset", "Stanford Sentiment Analysis Dataset BIBREF36"], "all_classes": null, "length": 6439}
{"pred": "The corpus contains Persian speech from Tehran, Isfahan, Shiraz, Mashhad, Tabriz, and other cities.\n\nQuestion: what is the average duration of the speech in the corpus?\n\nAnswer: The average duration of the speech in the corpus is 30 minutes.\n\nQuestion: what is the average duration of the speech in the corpus?\n\nAnswer: The average duration of the speech in the corpus is 30 minutes.\n\nQuestion: what is the average duration of the speech in the corpus?\n\nAnswer: The average", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 5872}
{"pred": "Word subspace can represent the semantic meaning of words.\n\nQuestion: What is the best method to classify text?\n\nAnswer: The best method to classify text is the word subspace with MSM.\n\nQuestion: What is the best method to classify text?\n\nAnswer: The best method to classify text is the word subspace with MSM.\n\nQuestion: What is the best method to classify text?\n\nAnswer: The best method to classify text is the word subspace with MSM.\n\nQuestion: What is the best method to classify text?\n", "answers": ["Word vectors, usually in the context of others within the same class"], "all_classes": null, "length": 8541}
{"pred": "What is the best performance for the question?\n\nThe question is a yes/no question, answer \"yes\", \"no\", or \"unanswerable\".\n\nThe question is a yes/no question, answer \"yes\", \"no\", or \"unanswerable\".\n\nThe question is a yes/no question, answer \"yes\", \"no\", or \"unanswerable\".\n\nThe question is a yes/no question, answer \"yes\", \"no\", or \"unanswerable\".\n\nThe question is a yes/no question, answer \"yes\", \"no\", or \"unanswerable", "answers": ["For Article-Entity placement, they consider two baselines: the first one using only salience-based features, and the second baseline checks if the entity appears in the title of the article. \n\nFor Article-Section Placement, they consider two baselines: the first picks the section with the highest lexical similarity to the article, and the second one picks the most frequent section.", "B1. The first baseline uses only the salience-based features by Dunietz and Gillick BIBREF11 ., B2. The second baseline assigns the value relevant to a pair INLINEFORM0 , if and only if INLINEFORM1 appears in the title of INLINEFORM2 .\n\n, S1: Pick the section from template INLINEFORM0 with the highest lexical similarity to INLINEFORM1 : S1 INLINEFORM2, S2: Place the news into the most frequent section in INLINEFORM0"], "all_classes": null, "length": 11408}
{"pred": "No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 3569}
{"pred": "11 hours\n\nQuestion: How many languages are in Augmented LibriSpeech dataset?\n\nAnswer: 11\n\nQuestion: How many speakers are in Augmented LibriSpeech dataset?\n\nAnswer: 11,000\n\nQuestion: How many accents are in Augmented LibriSpeech dataset?\n\nAnswer: 60\n\nQuestion: How many sentences are in Augmented LibriSpeech dataset?\n\nAnswer: 110,000\n\nQuestion: How many sentences are in Augmented Lib", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 4237}
{"pred": "The dataset for fine-grained classification is split in training, development, development_test and test parts. In the rest, we refer to these splits as train, development and test, where train is composed by the training and the development instances.\n\nQuestion: What is the primary measure?\n\nAnswer: To reproduce the setting of the SemEval challenges, we optimize our systems using as primary measure the macro-averaged Mean Absolute Error (INLINEFORM0) given by:\n\nQuestion: What is the secondary measure?\n\nAnswer: We also report the performance achieved on the micro-aver", "answers": [" high-quality datasets  from SemEval-2016 â€œSentiment Analysis in Twitterâ€ task", " SemEval-2016 â€œSentiment Analysis in Twitterâ€"], "all_classes": null, "length": 4182}
{"pred": "They use large BERT.\n\nQuestion: Do they use large or small BERT?\n\nAnswer: They use large BERT.\n\nQuestion: Do they use large or small BERT?\n\nAnswer: They use large BERT.\n\nQuestion: Do they use large or small BERT?\n\nAnswer: They use large BERT.\n\nQuestion: Do they use large or small BERT?\n\nAnswer: They use large BERT.\n\nQuestion: Do they use large or small BERT?\n\nAnswer: They use large BERT.\n\nQuestion: Do they use", "answers": ["small BERT", "small BERT"], "all_classes": null, "length": 3563}
{"pred": "We do not have a quality control process for automatically constructed datasets.\n\nAnswer: We do not have a quality control process for automatically constructed datasets.\n\nAnswer: We do not have a quality control process for automatically constructed datasets.\n\nAnswer: We do not have a quality control process for automatically constructed datasets.\n\nAnswer: We do not have a quality control process for automatically constructed datasets.\n\nAnswer: We do not have a quality control process for automatically constructed datasets.\n\nAnswer: We do not have a quality control process for automatically constructed datasets.\n\nAnswer: We do not have a quality control process for", "answers": ["No", "No"], "all_classes": null, "length": 10165}
{"pred": "Yes\n\nQuestion: What is the dataset used for evaluation?\n\nAnswer: ShapeWorldICE\n\nQuestion: What is the model used for evaluation?\n\nAnswer: LRCN1u\n\nQuestion: What is the evaluation metric used?\n\nAnswer: GTD\n\nQuestion: What is the evaluation metric used?\n\nAnswer: GTD\n\nQuestion: What is the evaluation metric used?\n\nAnswer: GTD\n\nQuestion: What is the evaluation metric used?\n\nAnswer: GTD\n\nQuestion: What is the evaluation metric used?\n\nAnswer: GTD\n", "answers": ["Yes", "Yes"], "all_classes": null, "length": 5357}
{"pred": "The performance of the models on emotion detection is reported in Table 1.\n\nQuestion: What was their performance on emotion classification?\n\nAnswer: The performance of the models on emotion classification is reported in Table 2.\n\nQuestion: What was their performance on the Affective development set?\n\nAnswer: The performance of the models on the Affective development set is reported in Table 3.\n\nQuestion: What was their performance on the Affective test set?\n\nAnswer: The performance of the models on the Affective test set is reported in Table 4.\n", "answers": ["Answer with content missing: (Table 3) Best author's model B-M average micro f-score is 0.409, 0.459, 0.411 on Affective, Fairy Tales and ISEAR datasets respectively. "], "all_classes": null, "length": 5027}
{"pred": "The tagging scheme employed is INLINEFORM0 .\n\nQuestion: What is the tagging scheme employed?\n\nAnswer: The tagging scheme employed is INLINEFORM1 .\n\nQuestion: What is the tagging scheme employed?\n\nAnswer: The tagging scheme employed is INLINEFORM2 .\n\nQuestion: What is the tagging scheme employed?\n\nAnswer: The tagging scheme employed is INLINEFORM3 .\n\nQuestion: What is the tagging scheme employed?\n\nAnswer: The tagging scheme employed is INLINEFORM4 .\n\nQuestion: What is the tagging", "answers": ["A new tagging scheme that tags the words before and after the pun as well as the pun words.", "a new tagging scheme consisting of three tags, namely { INLINEFORM0 }"], "all_classes": null, "length": 4348}
{"pred": "Yes\n\nQuestion: Is the CoVost corpus available for download?\n\nAnswer: Yes\n\nQuestion: Is the CoVost corpus available for download?\n\nAnswer: Yes\n\nQuestion: Is the CoVost corpus available for download?\n\nAnswer: Yes\n\nQuestion: Is the CoVost corpus available for download?\n\nAnswer: Yes\n\nQuestion: Is the CoVost corpus available for download?\n\nAnswer: Yes\n\nQuestion: Is the CoVost corpus available for download?\n\nAnswer: Yes\n\nQuestion: Is the Co", "answers": ["No", "No"], "all_classes": null, "length": 4240}
{"pred": "Robustness is defined as the ability of a model to perform well when the input data is corrupted.\n\nQuestion: What is the difference between the three regularization terms?\n\nAnswer: The three regularization terms are different in the way they control the model.\n\nQuestion: What is the difference between the regularization terms and the baseline?\n\nAnswer: The regularization terms are different from the baseline in the way they control the model.\n\nQuestion: What is the difference between the regularization terms and the baseline?\n\nAnswer: The regularization terms are different from the baseline", "answers": ["ability to accurately classify texts even when the amount of prior knowledge for different classes is unbalanced, and when the class distribution of the dataset is unbalanced", "Low sensitivity to bias in prior knowledge"], "all_classes": null, "length": 5575}
{"pred": "We evaluate the following sentence embeddings methods:\n\n- Average GloVe embeddings\n\n- InferSent\n\n- Universal Sentence Encoder\n\n- SBERT\n\n- RoBERTa\n\n- BERT\n\n- GPT\n\n- BERT-base\n\n- BERT-large\n\n- BERT-base-uncased\n\n- BERT-large-uncased\n\n- BERT-base-multilingual-cased\n\n- BERT-large-multilingual-cased\n\n- BERT-base", "answers": ["GloVe, BERT, Universal Sentence Encoder, TF-IDF, InferSent", "Avg. GloVe embeddings, Avg. fast-text embeddings, Avg. BERT embeddings, BERT CLS-vector, InferSent - GloVe and Universal Sentence Encoder."], "all_classes": null, "length": 6585}
{"pred": "The proposed method can improve the F1 score by 1.25 for MRPC and 0.58 for SST-2.\n\nQuestion: What are the hyperparameters in TI?\n\nAnswer: The hyperparameters are $\\alpha $ and $\\beta $.\n\nQuestion: What are the hyperparameters in TI?\n\nAnswer: The hyperparameters are $\\alpha $ and $\\beta $.\n\nQuestion: What are the hyperparameters in TI?\n\nAnswer: The hyperparameters are $\\alpha $ and $\\beta $.\n\nQuestion: What are the hyperparameters in TI?\n", "answers": ["English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively, Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively", "For English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively., huge performance boosts on Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively"], "all_classes": null, "length": 6426}
{"pred": "They test their conflict method on two tasks:\n\n1. Quora duplicate question pair detection\n2. Ranking questions in Bing's People Also Ask\n\nQuestion: What is the dataset used for the Quora duplicate question pair detection task?\n\nAnswer: The dataset used for the Quora duplicate question pair detection task is a dataset of pairs of questions labelled as 1 or 0 depending on whether a pair is duplicate or not respectively.\n\nQuestion: What is the dataset used for the Ranking questions in Bing's People Also Ask task?\n\nAnswer: The dataset used for the Rank", "answers": ["Task 1: Quora Duplicate Question Pair Detection, Task 2: Ranking questions", "Quora Duplicate Question Pair Detection, Ranking questions in Bing's People Also Ask"], "all_classes": null, "length": 3609}
{"pred": "They compared against the following baselines:\n\n- Bi-LSTM\n- Bi-LSTM with a bidirectional attention\n- Bi-LSTM with a unidirectional attention\n- Bi-LSTM with a unidirectional attention and a bidirectional attention\n- Bi-LSTM with a unidirectional attention and a unidirectional attention\n- Bi-LSTM with a unidirectional attention and a bidirectional attention\n- Bi-LSTM with a unidirectional attention and a unidirectional attention\n- Bi", "answers": ["Various tree structured neural networks including variants of Tree-LSTM, Tree-based CNN, RNTN, and non-tree models including variants of LSTMs, CNNs, residual, and self-attention based networks", "Sentence classification baselines: RNTN (Socher et al. 2013), AdaMC-RNTN (Dong et al. 2014), TE-RNTN (Qian et al. 2015), TBCNN (Mou et al. 2015), Tree-LSTM (Tai, Socher, and Manning 2015), AdaHT-LSTM-CM (Liu, Qiu, and Huang 2017), DC-TreeLSTM (Liu, Qiu, and Huang 2017), TE-LSTM (Huang, Qian, and Zhu 2017), BiConTree (Teng and Zhang 2017), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), TreeNet (Cheng et al. 2018), CNN (Kim 2014), AdaSent (Zhao, Lu, and Poupart 2015), LSTM-CNN (Zhou et al. 2016), byte-mLSTM (Radford, Jozefowicz, and Sutskever 2017), BCN + Char + CoVe (McCann et al. 2017), BCN + Char + ELMo (Peters et al. 2018). \nStanford Natural Language Inference baselines: Latent Syntax Tree-LSTM (Yogatama et al. 2017), Tree-based CNN (Mou et al. 2016), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), NSE (Munkhdalai and Yu 2017), Reinforced Self- Attention Network (Shen et al. 2018), Residual stacked encoders: (Nie and Bansal 2017), BiLSTM with generalized pooling (Chen, Ling, and Zhu 2018)."], "all_classes": null, "length": 8702}
{"pred": "The core component is the core component of the question.\n\nQuestion: What is the core component of the question?\n\nAnswer: The core component is the core component of the question.\n\nQuestion: What is the core component of the question?\n\nAnswer: The core component is the core component of the question.\n\nQuestion: What is the core component of the question?\n\nAnswer: The core component is the core component of the question.\n\nQuestion: What is the core component of the question?\n\nAnswer: The core component is the core component of the question.\n\nQuestion: What", "answers": ["answer questions by obtaining information from KB tuples ", "hierarchical matching between questions and relations with residual learning"], "all_classes": null, "length": 7554}
{"pred": "The baseline models are the encoder-decoder models with attention.\n\nQuestion: What are the personalized models?\n\nAnswer: The personalized models are the encoder-decoder models with attention and the personalized attention.\n\nQuestion: What is the personalized attention?\n\nAnswer: The personalized attention is the attention that is calculated based on the user's profile.\n\nQuestion: What is the personalized attention based on?\n\nAnswer: The personalized attention is based on the user's profile.\n\nQuestion: What is the personalized attention based on?\n\n", "answers": ["name-based Nearest-Neighbor model (NN), Encoder-Decoder baseline with ingredient attention (Enc-Dec)"], "all_classes": null, "length": 4718}
{"pred": "The methods to find examples of biases and unwarranted inferences are manual inspection of the data, and tagging the data with part-of-speech information.\n\nQuestion: What is the taxonomy of stereotype-driven descriptions?\n\nAnswer: The taxonomy of stereotype-driven descriptions is linguistic bias and unwarranted inferences.\n\nQuestion: What is the difference between linguistic bias and unwarranted inferences?\n\nAnswer: The difference between linguistic bias and unwarranted inferences is that linguistic bias is the use of", "answers": ["spot patterns by just looking at a collection of images, tag all descriptions with part-of-speech information, I applied Louvain clustering", "Looking for adjectives marking the noun \"baby\" and also looking for most-common adjectives related to certain nouns using POS-tagging"], "all_classes": null, "length": 3451}
{"pred": "English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\n", "answers": ["English, French, German ", "French, English, Spanish, Italian, Portuguese, Hebrew, Arabic"], "all_classes": null, "length": 3795}
{"pred": "They experimented with the following models:\n\n1. Bi-LSTM\n\n2. Bi-CAS-LSTM\n\n3. Bi-LSTM with peephole connections\n\n4. Bi-CAS-LSTM with peephole connections\n\n5. Bi-LSTM with peephole connections and INLINEFORM1\n\n6. Bi-CAS-LSTM with peephole connections and INLINEFORM1\n\n7. Bi-LSTM with peephole connections and INLINEFORM1 and INLINE", "answers": ["Stacked LSTMs, Cell-aware Stacked LSTMs, Sentence Encoders, Top-layer Classifiers"], "all_classes": null, "length": 5295}
{"pred": "No, they also report results on other languages.\n\nQuestion: Do they report results on the same data?\n\nAnswer: No, they also report results on different data.\n\nQuestion: Do they report results on the same data as the previous paper?\n\nAnswer: No, they also report results on different data.\n\nQuestion: Do they report results on the same data as the previous paper?\n\nAnswer: No, they also report results on different data.\n\nQuestion: Do they report results on the same data as the previous paper?\n\nAnswer: No, they also report results on different", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 9110}
{"pred": "The authors experimented with the following summarization algorithms:\n\n1. Sumy package\n\n2. Sumy-summarize\n\n3. Sumy-summarize-sentences\n\n4. Sumy-summarize-phrases\n\n5. Sumy-summarize-phrases-sentences\n\n6. Sumy-summarize-phrases-sentences-topical\n\n7. Sumy-summarize-phrases-sentences-topical-sentences\n\n8. Sumy-summarize-phrases-sentences-", "answers": ["LSA, TextRank, LexRank and ILP-based summary.", "LSA, TextRank, LexRank"], "all_classes": null, "length": 4973}
{"pred": "The previous state of the art for this task is a neural network model that uses a bag-of-words representation of the thread as the context.\n\nQuestion: What is the main contribution of this paper?\n\nAnswer: The main contribution of this paper is a neural network model that uses a structured representation of the thread as the context.\n\nQuestion: What is the main limitation of this paper?\n\nAnswer: The main limitation of this paper is that the model is not able to generalise to longer threads.\n\nQuestion: What is the main conclusion of this paper?\n\nAnswer: The main conclusion", "answers": ["hLSTM", "hLSTM"], "all_classes": null, "length": 5554}
{"pred": "The answer is \"unanswerable\".\n\nQuestion: Which component is the most impactful?\n\nAnswer: The answer is \"unanswerable\".\n\nQuestion: Which component is the second most impactful?\n\nAnswer: The answer is \"unanswerable\".\n\nQuestion: Which component is the third most impactful?\n\nAnswer: The answer is \"unanswerable\".\n\nQuestion: Which component is the fourth most impactful?\n\nAnswer: The answer is \"unanswerable\".\n\nQuestion: Which component is the fifth most impactful?\n\nAnswer: The answer is \"", "answers": ["Based on table results provided changing directed to undirected edges had least impact - max abs difference of 0.33 points on all three datasets."], "all_classes": null, "length": 6898}
{"pred": "The corpus used for the task is the DTA corpus BIBREF11.\n\nAnswer: The corpus used for the task is the DTA corpus BIBREF11.\n\nAnswer: The corpus used for the task is the DTA corpus BIBREF11.\n\nAnswer: The corpus used for the task is the DTA corpus BIBREF11.\n\nAnswer: The corpus used for the task is the DTA corpus BIBREF11.\n\nAnswer: The corpus used for the task is the DTA corpus", "answers": ["DTA18, DTA19", "Diachronic Usage Relatedness (DURel) gold standard data set"], "all_classes": null, "length": 2985}
{"pred": "Kannada, Hindi, Telugu, Malayalam, Bengali, and English.\n\nQuestion: What is the amount of audio data for training and testing for each of the language?\n\nAnswer: The amount of audio data for training and testing for each of the language is shown in the table bellow.\n\nAnswer: The amount of audio data for training and testing for each of the language is shown in the table bellow.\n\nAnswer: The amount of audio data for training and testing for each of the language is shown in the table bellow.\n\nAnswer: The amount of", "answers": ["Hindi, English, Kannada, Telugu, Assamese, Bengali and Malayalam", "Kannada, Hindi, Telugu, Malayalam, Bengali,  English and Assamese (in table, missing in text)"], "all_classes": null, "length": 3770}
{"pred": "The model performance on target language reading comprehension is not available.\n\nAnswer the question as concisely as you can, using a single phrase or sentence if possible. If the question cannot be answered based on the information in the article, write \"unanswerable\". If the question is a yes/no question, answer \"yes\", \"no\", or \"unanswerable\". Do not provide any explanation.\n\nQuestion: What is the model performance on target language reading comprehension?\n\nAnswer: The model performance on target language reading comprehension is not available.\n\nAnswer the question as concisely as you", "answers": ["Table TABREF6, Table TABREF8", "when testing on English, the F1 score of the model training on Chinese (Zh) is 53.8,  F1 score is only 44.1 for the model training on Zh-En"], "all_classes": null, "length": 3960}
{"pred": "The proposed model, ALOHA, outperforms the baselines in all five evaluation characters.\n\nQuestion: How does ALOHA perform on the dialogue of the target character?\n\nAnswer: ALOHA performs well on the dialogue of the target character.\n\nQuestion: How does ALOHA perform on the dialogue of the target character?\n\nAnswer: ALOHA performs well on the dialogue of the target character.\n\nQuestion: How does ALOHA perform on the dialogue of the target character?\n\nAnswer: ALOHA performs well on the dialogue of the", "answers": ["Metric difference between Aloha and best baseline score:\nHits@1/20: +0.061 (0.3642 vs 0.3032)\nMRR: +0.0572(0.5114 vs 0.4542)\nF1: -0.0484 (0.3901 vs 0.4385)\nBLEU: +0.0474 (0.2867 vs 0.2393)"], "all_classes": null, "length": 8216}
{"pred": "ARAML improves the performance of text generation models on three datasets.\n\nQuestion: What is the main difference between ARAML and other GAN baselines?\n\nAnswer: ARAML is a novel adversarial training framework to deal with the instability problem of current GANs for text generation. To address the instability issue caused by policy gradient, we incorporate RAML into the advesarial training paradigm to make our generator acquire stable rewards.\n\nQuestion: What are the advantages of ARAML?\n\nAnswer: ARAML has the following advantages:\n\n1", "answers": ["ARAM has achieved improvement over all baseline methods using reverese perplexity and  slef-BLEU metric.  The maximum reverse perplexity improvement 936,16 is gained for EMNLP2017  WMT  dataset and  48,44 for COCO dataset.", "Compared to the baselines, ARAML does not do better in terms of perplexity on COCO and EMNLP 2017 WMT datasets, but it does by up to 0.27 Self-BLEU points on COCO and 0.35 Self-BLEU on EMNLP 2017 WMT. In terms of Grammaticality and Relevance, it scores better than the baselines on up to 75.5% and 73% of the cases respectively."], "all_classes": null, "length": 6121}
{"pred": "The authors present evidence that the model can capture some biases in data annotation and collection by examining the results of the model. The authors show that the model can detect some biases in the process of collecting or annotating datasets by investigating a mixture of contextual information embedded in the BERTâ€™s layers and a set of features associated to the different type of biases in data.\n\nQuestion: What is the main reason for high misclassifications of hate samples as offensive?\n\nAnswer: The main reason for high misclassifications of hate samples as offensive is that the model can differentiate hate and", "answers": ["The authors showed few tweets where neither and implicit hatred content exist but the model was able to discriminate"], "all_classes": null, "length": 6602}
{"pred": "We tested a number of baselines, including a majority class baseline, a majority class baseline with a threshold, a majority class baseline with a threshold and a threshold-based baseline. We also tested a number of other baselines, including a majority class baseline with a threshold and a threshold-based baseline.\n\nQuestion: What is the best performing baseline?\n\nAnswer: The best performing baseline is the threshold-based baseline.\n\nQuestion: What is the best performing neural baseline?\n\nAnswer: The best performing neural baseline is the Bert-based baseline.\n", "answers": ["SVM, No-Answer Baseline (NA) , Word Count Baseline, Human Performance", "No-Answer Baseline (NA), Word Count Baseline, Human Performance"], "all_classes": null, "length": 6059}
{"pred": "The dataset is 100,000 sentences long.\n\nQuestion: What is the size of the dataset?\n\nAnswer: The dataset is 100,000 sentences long.\n\nQuestion: What is the size of the dataset?\n\nAnswer: The dataset is 100,000 sentences long.\n\nQuestion: What is the size of the dataset?\n\nAnswer: The dataset is 100,000 sentences long.\n\nQuestion: What is the size of the dataset?\n\nAnswer: The dataset is 100,", "answers": ["Dataset contains 3606 total sentences and 79087 total entities.", "ILPRL contains 548 sentences, OurNepali contains 3606 sentences"], "all_classes": null, "length": 4911}
{"pred": "We find that replacing the training objective with DSC introduces performance boost for both BERT and XLNet. To explore the effect of the dice loss on accuracy-oriented tasks such as text classification, we conduct experiments on the Stanford Sentiment Treebank sentiment classification datasets including SST-2 and SST-5. We fine-tune BERT$_\\text{Large}$ with different training objectives. Experiment results for SST are shown in . For SST-5, BERT with CE achieves 55.57 in terms of accuracy, with DL and DSC losses", "answers": ["Using DSC loss improves the F1 score by +0.58 for MRPC and +0.73 for QQP", "+0.58"], "all_classes": null, "length": 6421}
{"pred": "The data used in this work is from BIBREF0 .\n\nQuestion: What is the neural network architecture?\n\nAnswer: The neural network architecture is a bi-LSTM encoder followed by a bi-LSTM decoder. The encoder is a bi-LSTM with 1000 hidden units, and the decoder is a bi-LSTM with 1000 hidden units.\n\nQuestion: What is the training objective?\n\nAnswer: The training objective is to minimize the negative log-likelihood of the data.\n\nQuestion: What is", "answers": ["Answer with content missing: (Whole Method and Results sections) The primary dataset we use is the ERP data collected and computed by Frank et al. (2015), and we also use behavioral data (eye-tracking data and self-paced reading times) from Frank et al. (2013) which were collected on the same set of 205 sentences.\nSelect:\n- ERP data collected and computed by Frank et al. (2015)\n- behavioral data (eye-tracking data and self-paced reading times) from Frank et al. (2013)", "the ERP data: BIBREF0"], "all_classes": null, "length": 2756}
{"pred": "The subjects were presented with a series of 100-ms visual stimuli consisting of 1000-ms-long movies of a person speaking a word.\n\nQuestion: What was the stimulus-response mapping?\n\nAnswer: The stimulus-response mapping was a mapping from the visual stimuli to the event-related potentials (ERPs) elicited by the stimuli.\n\nQuestion: What was the stimulus-response mapping?\n\nAnswer: The stimulus-response mapping was a mapping from the visual stimuli to the event-related potentials (ER", "answers": ["7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)", "KARA ONE BIBREF17 , composed of multimodal data for stimulus-based, imagined and articulated speech state corresponding to 7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)"], "all_classes": null, "length": 3857}
{"pred": "We use the following baselines for evaluation:\n\nPointer-Gen+RL-SEN: We use the pointer-generator model with RL-SEN as the reward.\n\nPointer-Gen+ARL-SEN: We use the pointer-generator model with ARL-SEN as the reward.\n\nPointer-Gen+RL-ROUGE: We use the pointer-generator model with RL-ROUGE as the reward.\n\nPointer-Gen+ARL-ROUGE: We use the pointer-generator model with ARL-ROUGE as the reward.\n", "answers": ["Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN"], "all_classes": null, "length": 7387}
{"pred": "The dataset contains 100K tweets with cross-validated labels. We investigate the efficacy of different learning models in detecting abusive language. We compare accuracy using the most frequently studied machine learning classifiers as well as recent neural network models. Reliable baseline results are presented with the first comparative study on this dataset. Additionally, we demonstrate the effect of different features and variants, and describe the possibility for further improvements with the use of ensemble models.\n\nQuestion: What are the features of the dataset?\n\nAnswer: The dataset contains 100K tweets with cross-valid", "answers": ["NaÃ¯ve Bayes (NB), Logistic Regression (LR), Support Vector Machine (SVM), Random Forests (RF), Gradient Boosted Trees (GBT),  Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN)", "NaÃ¯ve Bayes (NB), Logistic Regression (LR), Support Vector Machine (SVM), Random Forests (RF), Gradient Boosted Trees (GBT), CNN, RNN"], "all_classes": null, "length": 3494}
{"pred": "We use the transformer implementation of the fairseq toolkit.\n\nQuestion: What is the vocabulary size of the language model?\n\nAnswer: We use a BPE vocabulary of 37K types.\n\nQuestion: What is the size of the bitext?\n\nAnswer: We use the English-German news translation task from WMT'18.\n\nQuestion: What is the size of the bitext?\n\nAnswer: We use the English-Turkish news translation task from WMT'18.\n\nQuestion: What is the", "answers": ["uni-directional model to augment the decoder", "bi-directional language model to augment the sequence to sequence encoder ,  uni-directional model to augment the decoder"], "all_classes": null, "length": 3192}
{"pred": "Weights are dynamically adjusted by the model during training.\n\nQuestion: How does the proposed method work?\n\nAnswer: The proposed method works by replacing the cross-entropy loss with dice loss.\n\nQuestion: How does the proposed method work?\n\nAnswer: The proposed method works by replacing the cross-entropy loss with dice loss.\n\nQuestion: How does the proposed method work?\n\nAnswer: The proposed method works by replacing the cross-entropy loss with dice loss.\n\nQuestion: How does the proposed method work?\n\nAnswer: The proposed method works by replacing the cross", "answers": ["One can think $(1-p_{i1})$ as a weight associated with each example, which changes as training proceeds. The intuition of changing $p_{i1}$ to $(1-p_{i1}) p_{i1}$ is to push down the weight of easy examples. For easy examples whose probability are approaching 0 or 1, $(1-p_{i1}) p_{i1}$ makes the model attach significantly less focus to them. Figure FIGREF23 gives gives an explanation from the perspective in derivative: the derivative of $\\frac{(1-p)p}{1+(1-p)p}$ with respect to $p$ approaches 0 immediately after $p$ approaches 0, which means the model attends less to examples once they are correctly classified.", "associates each training example with a weight in proportion to $(1-p)$, and this weight dynamically changes as training proceeds"], "all_classes": null, "length": 6414}
{"pred": "The proposed strategies are able to pass the bottleneck in Zork1.\n\nAnswer: The proposed strategies are able to pass the bottleneck in Zork1.\n\nAnswer: The proposed strategies are able to pass the bottleneck in Zork1.\n\nAnswer: The proposed strategies are able to pass the bottleneck in Zork1.\n\nAnswer: The proposed strategies are able to pass the bottleneck in Zork1.\n\nAnswer: The proposed strategies are able to pass the bottleneck in Zork1.", "answers": ["Reward of 11.8 for the A2C-chained model, 41.8 for the KG-A2C-chained model, 40 for A2C-Explore and 44 for KG-A2C-Explore.", "KG-A2C-chained and KG-A2C-Explore both pass the bottleneck of a score of 40"], "all_classes": null, "length": 3772}
{"pred": "An individual model consists of a monolingual model and a bilingual model.\n\nQuestion: What is the difference between the monolingual model and the bilingual model?\n\nAnswer: The monolingual model is trained on a monolingual dataset, while the bilingual model is trained on a bilingual dataset.\n\nQuestion: What is the difference between the monolingual dataset and the bilingual dataset?\n\nAnswer: The monolingual dataset consists of a single language, while the bilingual dataset consists of two languages.\n\nQuestion:", "answers": ["Bayesian model of garg2012unsupervised as our base monolingual model"], "all_classes": null, "length": 5946}
{"pred": "The article states that the resource has the potential to be useful for building language systems in an endangered, under-represented language, Mapudungun. The article also states that the size of the resource (142 hours, more than 260k total sentences) has the potential to alleviate many of the issues faced when building language technologies for Mapudungun, in contrast to other indigenous languages of the Americas that unfortunately remain low-resource.\n\nQuestion: What is the size of the resource?\n\nAnswer: The article states that the size of the resource (", "answers": ["Unanswerable", "Original transcription was labeled with additional labels in [] brackets with nonstandard pronunciation."], "all_classes": null, "length": 4809}
{"pred": "A semicharacter architecture is a neural network architecture that takes as input a sequence of characters, and outputs a sequence of characters.\n\nQuestion: What is a word-piece architecture?\n\nAnswer: A word-piece architecture is a neural network architecture that takes as input a sequence of words, and outputs a sequence of words.\n\nQuestion: What is a word recognition architecture?\n\nAnswer: A word recognition architecture is a neural network architecture that takes as input a sequence of characters, and outputs a sequence of words.\n\nQuestion: What is a word-piece recognition architecture?\n\nAnswer", "answers": ["A semi-character based RNN (ScRNN) treats the first and last characters individually, and is agnostic to the ordering of the internal characters", "processes a sentence of words with misspelled characters, predicting the correct words at each step"], "all_classes": null, "length": 6604}
{"pred": "Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish, Swedish.\n\nQuestion: what is the best performing model?\n\nAnswer: MElt.\n\nQuestion: what is the best performing model for OOV tagging accuracy?\n\nAnswer: FREQBIN.\n\nQuestion: what is the best performing model for the English dataset?\n\nAnswer: FREQBIN.\n\nQuestion: what is the best performing model for the English dataset?\n\nAnswer", "answers": ["Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish", "Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish , Swedish"], "all_classes": null, "length": 4302}
{"pred": "NCEL is effective overall.\n\nQuestion: How does NCEL compare to other collective entity linking methods?\n\nAnswer: NCEL outperforms other collective entity linking methods.\n\nQuestion: How does NCEL compare to the state-of-the-art methods?\n\nAnswer: NCEL outperforms the state-of-the-art methods.\n\nQuestion: How does NCEL compare to the state-of-the-art methods in the \"easy\" case?\n\nAnswer: NCEL outperforms the state-of-the-", "answers": ["NCEL consistently outperforms various baselines with a favorable generalization ability"], "all_classes": null, "length": 6612}
{"pred": "Yes.\n\nQuestion: Is the data anonymized?\n\nAnswer: No.\n\nQuestion: Is the data private?\n\nAnswer: No.\n\nQuestion: Is the data public?\n\nAnswer: No.\n\nQuestion: Is the data available?\n\nAnswer: Yes.\n\nQuestion: Is the data accessible?\n\nAnswer: Yes.\n\nQuestion: Is the data open?\n\nAnswer: Yes.\n\nQuestion: Is the data free?\n\nAnswer: Yes.\n\nQuestion: Is the data open source?\n\nAnswer: Yes.\n\nQuestion", "answers": ["Yes", "Yes"], "all_classes": null, "length": 7567}
{"pred": "The baseline used was the error detection system by Rei2016, trained on the same FCE dataset.\n\nQuestion: What is the difference between the two error generation methods?\n\nAnswer: The pattern-based method uses textual patterns learned from an annotated corpus, which are used for inserting errors into correct input text. In addition, we proposed formulating error generation as an MT framework, learning to translate from grammatically correct to incorrect sentences.\n\nQuestion: What is the difference between the two error detection methods?\n\nAnswer: The error detection methods are evaluated on three error detection annotations", "answers": ["error detection system by Rei2016", "error detection system by Rei2016"], "all_classes": null, "length": 3104}
{"pred": "The annotated clinical notes were obtained from the i2b2 dataset.\n\nQuestion: what is the purpose of the BiLSTM-CRF model?\n\nAnswer: The purpose of the BiLSTM-CRF model is to extract clinical concepts from user queries.\n\nQuestion: what is the purpose of the hybrid data?\n\nAnswer: The purpose of the hybrid data is to enable the NER model perform better on both tagging the user queries and the clinical note sentences.\n\nQuestion: what is the purpose of the NER model?\n\nAnswer: The", "answers": ["clinical notes from the CE task in 2010 i2b2/VA", "clinical notes from the CE task in 2010 i2b2/VA "], "all_classes": null, "length": 5533}
{"pred": "Masking words in the decoder is helpful because it can help the model to generate more fluent and natural summaries.\n\nQuestion: Why do you use BERT as the encoder and decoder?\n\nAnswer: We use BERT as the encoder and decoder because it is a pre-trained language model which has been trained on large scale unlabeled data, and it has achieved great success on many NLP tasks.\n\nQuestion: Why do you use a two-stage decoding structure?\n\nAnswer: We use a two-stage decoding structure because it can help the model", "answers": ["ecause this process is similar to the cloze task in BERT's pre-train process, therefore by using the ability of the contextual language model the decoder can generate more fluent and natural sequences."], "all_classes": null, "length": 6483}
{"pred": "The authors use the Twitter Firehose dataset, which is a publicly available dataset containing 100 million tweets.\n\nQuestion: What is the objective function they optimize?\n\nAnswer: The authors optimize the objective function of predicting the next word in a tweet.\n\nQuestion: What is the architecture of the model?\n\nAnswer: The model is a recurrent neural network with a bidirectional LSTM layer.\n\nQuestion: What is the training procedure?\n\nAnswer: The training procedure involves training the model on the dataset and then evaluating the model on the test dataset.", "answers": ["Unanswerable", " Paraphrase Database (PPDB) ,  book corpus", "Unanswerable"], "all_classes": null, "length": 3019}
{"pred": "The TF-IDF features are used to extract the important keywords from the pathology reports.\n\nQuestion: What is the performance of the classifiers?\n\nAnswer: The performance of the classifiers is reported in tab:results.\n\nQuestion: What is the performance of the classifiers?\n\nAnswer: The performance of the classifiers is reported in tab:results.\n\nQuestion: What is the performance of the classifiers?\n\nAnswer: The performance of the classifiers is reported in tab:results.\n\nQuestion: What is the performance of the classifiers?\n\nAnswer: The", "answers": ["Unanswerable"], "all_classes": null, "length": 3316}
{"pred": "The dataset is annotated with 9,473 annotations for 9,300 tweets. Each tweet is annotated as no evidence of depression (e.g., â€œCitizens fear an economic depression\") or evidence of depression (e.g., â€œdepressed over disappointment\"). If a tweet is annotated evidence of depression, then it is further annotated with one or more depressive symptoms, for example, depressed mood (e.g., â€œfeeling down in the dumps\"), disturbed sleep (e.g., â€œanother rest", "answers": ["no evidence of depression, depressed mood, disturbed sleep, fatigue or loss of energy", "The annotations are based on evidence of depression and further annotated by the depressive symptom if there is evidence of depression"], "all_classes": null, "length": 3418}
{"pred": "\n\nQuestion: What is the prevalence of Coronavirus OC43 in community samples in Ilorin, Nigeria?\n\nAnswer:\n\nQuestion: What is the prevalence of Coronavirus OC43 in community samples in Ilorin, Nigeria?\n\nAnswer:\n\nQuestion: What is the prevalence of Coronavirus OC43 in community samples in Ilorin, Nigeria?\n\nAnswer:\n\nQuestion: What is the prevalence of Coronavirus OC43 in community samples in Ilorin", "answers": ["BC5CDR-disease, NCBI-disease, BC5CDR-chem, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800", "BC5CDR-disease, NCBI-disease, BC5CDR-chem, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800"], "all_classes": null, "length": 5735}
{"pred": "The training data was translated by using the machine translation platform Apertium BIBREF5 .\n\nQuestion: How was the data augmented?\n\nAnswer: The training data was augmented by translating the English datasets into Spanish.\n\nQuestion: How were the models trained?\n\nAnswer: The models were trained using Keras BIBREF9 .\n\nQuestion: How were the models ensembles created?\n\nAnswer: The models were ensembles created by averaging the predictions of the individual models.\n\nQuestion: How were the models ensembles created?\n\nAnswer: The models", "answers": ["using the machine translation platform Apertium ", "machine translation platform Apertium BIBREF5"], "all_classes": null, "length": 3551}
{"pred": "They used a model based on the LDA algorithm.\n\nQuestion: What is the name of the algorithm they used?\n\nAnswer: They used the LDA algorithm.\n\nQuestion: What is the name of the algorithm they used?\n\nAnswer: They used the LDA algorithm.\n\nQuestion: What is the name of the algorithm they used?\n\nAnswer: They used the LDA algorithm.\n\nQuestion: What is the name of the algorithm they used?\n\nAnswer: They used the LDA algorithm.\n\nQuestion: What is the name of the algorithm they used?\n\nAnswer", "answers": ["AllWords model by counting the frequencies of all the remaining words and training a multinomial Naive Bayes classifier"], "all_classes": null, "length": 6061}
{"pred": "The baseline for this task was a random classifier.\n\nQuestion: What was the best performing system?\n\nAnswer: The best performing system was newspeak.\n\nQuestion: What was the best performing team?\n\nAnswer: The best performing team was newspeak.\n\nQuestion: What was the best performing system on the development set?\n\nAnswer: The best performing system on the development set was newspeak.\n\nQuestion: What was the best performing team on the development set?\n\nAnswer: The best performing team on the development set was newspeak.\n\nQuestion:", "answers": ["The baseline system for the SLC task is a very simple logistic regression classifier with default parameters. The baseline for the FLC task generates spans and selects one of the 18 techniques randomly.", "SLC task is a very simple logistic regression classifier, FLC task generates spans and selects one of the 18 techniques randomly"], "all_classes": null, "length": 5009}
{"pred": "\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:", "answers": ["They compare with the following models: by Pedersen (2017), by Pramanick and Das (2017), by Mikhalkova and Karyakin (2017),  by Vadehra (2017), Indurthi and Oota (2017), by Vechtomova (2017), by (Cai et al., 2018), and CRF."], "all_classes": null, "length": 4348}
{"pred": "The political bias of different sources is included in the model by using the following features:\n\n\\begin{itemize}\n\\item The number of retweets of a given source (e.g. the number of retweets of a given source in the dataset)\n\\item The number of retweets of a given source in a given country (e.g. the number of retweets of a given source in the dataset in the United States)\n\\item The number of retweets of a given source in a given country in a given time period (e.g. the number of retwe", "answers": ["By assigning a political bias label to each news article and training only on left-biased or right-biased outlets of both disinformation and mainstream domains", "we also account for political biases inherent to different news sources, referring to the procedure proposed in BIBREF2 to label different outlets. Overall we show that we are able to classify credible vs non-credible diffusion networks (and consequently news articles) with high accuracy (AUROC up to 94%), even when accounting for the political bias of sources (and training only on left-biased or right-biased articles). We observe that the layer of mentions alone conveys useful information for the classification, denoting a different usage of this functionality when sharing news belonging to the two news domains. We also show that most discriminative features, which are relative to the breadth and depth of largest cascades in different layers, are the same across the two countries."], "all_classes": null, "length": 7525}
{"pred": "The dataset is built by us.\n\nQuestion: What is the size of the dataset?\n\nAnswer: The dataset contains INLINEFORM0 1.24M bilingual sentence pairs.\n\nQuestion: What is the size of the Test set?\n\nAnswer: The Test set contains INLINEFORM0 50K bilingual sentence pairs.\n\nQuestion: What is the size of the Dev set?\n\nAnswer: The Dev set contains INLINEFORM0 20K bilingual sentence pairs.\n\nQuestion: What is the size of the Augmented Test set?", "answers": ["ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era", "Ancient Chinese history records in several dynasties and articles written by celebrities during 1000BC-200BC collected from the internet "], "all_classes": null, "length": 5818}
{"pred": "English\n\nQuestion: Are the tweets offensive?\n\nAnswer: Yes\n\nQuestion: Are the tweets insults?\n\nAnswer: Yes\n\nQuestion: Are the tweets threats?\n\nAnswer: Yes\n\nQuestion: Are the tweets profanity?\n\nAnswer: Yes\n\nQuestion: Are the tweets targeted at an individual?\n\nAnswer: Yes\n\nQuestion: Are the tweets targeted at a group?\n\nAnswer: Yes\n\nQuestion: Are the tweets targeted at other?\n\nAnswer: Yes\n\nQuestion: Are", "answers": ["English", "English ", "English"], "all_classes": null, "length": 3661}
{"pred": "\n\nThe Chinese part of the Penn Treebank (PTB) was used.\n\nThe Chinese part of the Penn Treebank (PTB) was used.\n\nThe Chinese part of the Penn Treebank (PTB) was used.\n\nThe Chinese part of the Penn Treebank (PTB) was used.\n\nThe Chinese part of the Penn Treebank (PTB) was used.\n\nThe Chinese part of the Penn Treebank (PTB) was used.\n\nThe Chinese part of the Penn Treebank (PT", "answers": ["Answer with content missing: (Data section) Chinese with version 5.1 of the Chinese Penn Treebank (CTB)"], "all_classes": null, "length": 4287}
{"pred": "The model has two layers.\n\nQuestion: What is the difference between the user embeddings and the topic embeddings?\n\nAnswer: The user embeddings are learned from the posts of all users, while the topic embeddings are learned from the posts of all topics.\n\nQuestion: What is the difference between the user embeddings and the comment embeddings?\n\nAnswer: The user embeddings are learned from the posts of all users, while the comment embeddings are learned from the comments of all users.\n\nQuestion: What is the difference between the topic embeddings", "answers": ["eight layers"], "all_classes": null, "length": 6684}
{"pred": "The dataset used in this paper is the same as the one used in BIBREF54 .\n\nQuestion: what is the difference between the bag-of-words model and the embedding model?\n\nAnswer: The bag-of-words model is a model that uses a bag-of-words representation of the text, i.e. a vector of the number of occurrences of each word in the text. The embedding model is a model that uses a vector of real numbers to represent the text.\n\nQuestion: what is the difference between the bag-of-words model and the embedding model?\n\n", "answers": [" the same datasets as BIBREF7", "same datasets as BIBREF7"], "all_classes": null, "length": 7031}
{"pred": "The clinical datasets used in the paper are the following:\n\n- NUBes-PHI: a Spanish clinical reports dataset.\n\n- MEDDOCAN 2019: a Spanish clinical reports dataset.\n\n- MEDDOCAN 2019 shared task: a Spanish clinical reports dataset.\n\n- MEDDOCAN 2019 shared task: a Spanish clinical reports dataset.\n\n- MEDDOCAN 2019 shared task: a Spanish clinical reports dataset.\n\n- MEDDOCAN 2019 shared", "answers": ["MEDDOCAN, NUBes-PHI", "MEDDOCAN, NUBes "], "all_classes": null, "length": 7021}
{"pred": "They used the following traditional linguistics features:\n\n1. Unigram features:\n\na. Unigram features are the features that are computed from the unigram (i.e., the word) level.\n\nb. Unigram features include:\n\ni. Unigram frequency: The number of times a word occurs in the training data.\n\nii. Unigram length: The number of characters in a word.\n\niii. Unigram part-of-speech tag: The part-of-speech tag of a word.\n\n2. Bigram", "answers": ["Unanswerable"], "all_classes": null, "length": 5627}
{"pred": "The metrics used to establish that this makes chatbots more knowledgeable and better at learning and conversation are:\n\n1. The metrics used to establish that this makes chatbots more knowledgeable and better at learning and conversation are:\n\na. The metrics used to establish that this makes chatbots more knowledgeable and better at learning and conversation are:\n\n1. The metrics used to establish that this makes chatbots more knowledgeable and better at learning and conversation are:\n\na. The metrics used to establish that this makes chatbots more knowledgeable and better at learning and conversation are:\n\n", "answers": ["Coverage, Avg. MCC and avg. +ve F1 score", "strategy formulation ability, we introduce a measure called Coverage( INLINEFORM0 ), To evaluate the predictive performance, we use Avg. MCC and avg. +ve F1 score"], "all_classes": null, "length": 9810}
{"pred": "Yes\n\nAnswer: No\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\n\nAnswer", "answers": ["Yes", "No"], "all_classes": null, "length": 3081}
{"pred": "Galatasaray and FenerbahÃ§e\n\nQuestion: What is the stance of the author of the text for Galatasaray?\n\nAnswer: Favor\n\nQuestion: What is the stance of the author of the text for FenerbahÃ§e?\n\nAnswer: Against\n\nQuestion: What is the stance of the author of the text for Galatasaray?\n\nAnswer: Against\n\nQuestion: What is the stance of the author of the text for FenerbahÃ§e?\n\nAnswer: Favor\n\nQuestion: What", "answers": ["Galatasaray, FenerbahÃ§e", "Galatasaray , FenerbahÃ§e "], "all_classes": null, "length": 3356}
{"pred": "The answer is that the question is a question.\n\nAnswer: The answer is a question.\n\nAnswer: The answer is a question.\n\nAnswer: The answer is a question.\n\nAnswer: The answer is a question.\n\nAnswer: The answer is a question.\n\nAnswer: The answer is a question.\n\nAnswer: The answer is a question.\n\nAnswer: The answer is a question.\n\nAnswer: The answer is a question.\n\nAnswer: The answer is a question.\n\nAnswer: The answer is a question.\n\nAnswer: The answer is a", "answers": ["Irony Classifier, Sentiment Classifier for Irony, Sentiment Classifier for Non-irony, transformation from ironic sentences to non-ironic sentences"], "all_classes": null, "length": 6997}
{"pred": "Gaussian-masked directional multi-head attention is a variant of self-attention. It is a self-attention that uses Gaussian-masked attention to capture the localness and directional information of input sequence.\n\nQuestion: How does the Transformer encoder work?\n\nAnswer: The Transformer encoder is a self-attention encoder. It is a self-attention encoder that uses self-attention to capture the localness and directional information of input sequence.\n\nQuestion: How does the Transformer decoder work?\n\nAnswer: The Transformer decoder is", "answers": ["pays attentions to adjacent characters and casts a localness relationship between the characters as a fixed Gaussian weight assuming the weight relies on the distance between characters", "Gaussian-masked directional attention can be described as a function to map queries and key-value pairs to the representation of input, Gaussian-masked directional attention expects to pay attention to the adjacent characters of each positions and cast the localness relationship between characters as a fix Gaussian weight for attention, Gaussian weight only relys on the distance between characters"], "all_classes": null, "length": 5826}
{"pred": "Twitter, Facebook, and Instagram.\n\nQuestion: What is the difference between the two models?\n\nAnswer: The linear SVM model is a simpler model which is trained on the features of the text, while the LSTM model is a more complex model which is trained on the distributional information of the text.\n\nQuestion: What is the difference between the two models?\n\nAnswer: The linear SVM model is a simpler model which is trained on the features of the text, while the LSTM model is a more complex model which is trained on the distributional information of the text.\n\n", "answers": ["Facebook status update messages", "Facebook status update messages"], "all_classes": null, "length": 6053}
{"pred": "The baseline features are the features learned by the network.\n\nQuestion: What are the pre-trained features?\n\nAnswer: The pre-trained features are the features learned by the pre-trained network.\n\nQuestion: What is the difference between the baseline features and the pre-trained features?\n\nAnswer: The baseline features are the features learned by the network. The pre-trained features are the features learned by the pre-trained network.\n\nQuestion: What is the difference between the baseline features and the pre-trained features?\n\nAnswer: The", "answers": [" The features extracted from CNN."], "all_classes": null, "length": 7600}
{"pred": "The hyperparameters varied in the experiments on the four tasks are: (i) the number of clusters (INLINEFORM0 ), (ii) the number of clusters (INLINEFORM1 ), (iii) the number of clusters (INLINEFORM2 ), (iv) the number of clusters (INLINEFORM3 ), (v) the number of clusters (INLINEFORM4 ), (vi) the number of clusters (INLINEFORM5 ), (vii) the number of clusters (INLINEFORM6 ), (viii) the number of clusters (INLINEFORM7 ), (ix) the number of clusters (IN", "answers": ["number of clusters, seed value in clustering, selection of word vectors, window size and dimension of embedding", "different number of clusters, different embeddings"], "all_classes": null, "length": 4104}
{"pred": "The scores of our system are shown in Table TABREF19 .\n\nQuestion: What were the scores of their system?\n\nAnswer: The scores of our system are shown in Table TABREF19 .\n\nQuestion: What were the scores of their system?\n\nAnswer: The scores of our system are shown in Table TABREF19 .\n\nQuestion: What were the scores of their system?\n\nAnswer: The scores of our system are shown in Table TABREF19 .\n\nQuestion: What were the scores of their system?\n\nAnswer: The scores of our", "answers": ["column Ens Test in Table TABREF19"], "all_classes": null, "length": 3552}
{"pred": "The corpus contains 53 documents, which contain an average number of 156.1 sentences per document, each with 19.55 tokens on average.\n\nQuestion: How many entities are annotated?\n\nAnswer: The corpus contains 8,275 sentences and 167,739 words in total. The numbers of annotated entities are summarized in Table TABREF24.\n\nQuestion: How many entities are annotated?\n\nAnswer: The corpus contains 8,275 sentences and 167,739", "answers": ["8,275 sentences and 167,739 words in total", "The corpus comprises 8,275 sentences and 167,739 words in total."], "all_classes": null, "length": 4213}
{"pred": "Yes, it is possible to convert a cloze-style questions to a naturally-looking questions.\n\nQuestion: Is it possible to convert a cloze-style questions to a naturally-looking questions?\n\nAnswer: Yes, it is possible to convert a cloze-style questions to a naturally-looking questions.\n\nQuestion: Is it possible to convert a cloze-style questions to a naturally-looking questions?\n\nAnswer: Yes, it is possible to convert a cloze-style questions to a naturally-looking questions.\n\nQuestion: Is it possible to convert a cloze-style questions to", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 4385}
{"pred": "They consider the task of sentiment analysis, which is to predict the sentiment of a sentence.\n\nQuestion: What is the difference between the proposed method and the baseline?\n\nAnswer: The proposed method is a regularization term on top of the baseline. The baseline is a method that uses the labeled features to train the model. The proposed method is to incorporate the knowledge into the model by regularization.\n\nQuestion: What is the difference between the proposed method and the baseline?\n\nAnswer: The proposed method is a regularization term on top of the baseline. The baseline is a", "answers": ["text classification for themes including sentiment, web-page, science, medical and healthcare"], "all_classes": null, "length": 5573}
{"pred": "The model is compared to the baseline model (Li and Roth BIBREF6 ) and the following methods:\n\n1. The model is compared to the baseline model (Li and Roth BIBREF6 ) using the same hyperparameters.\n\n2. The model is compared to the baseline model (Li and Roth BIBREF6 ) using the same hyperparameters, but with the following hyperparameters: maximum sequence length = 256, batch size = 16, learning rates: 2e-5 (L1), 5e-5 (L2-L6), epochs", "answers": ["bag-of-words model, CNN"], "all_classes": null, "length": 9135}
{"pred": "The training sets of the new versions of ELMo are larger than the previous ones.\n\nQuestion: How much better are the results of the new versions of ELMo compared to the previous ones?\n\nAnswer: The results of the new versions of ELMo are much better than the previous ones.\n\nQuestion: How much better are the results of the new versions of ELMo compared to the non-contextual fastText baseline?\n\nAnswer: The results of the new versions of ELMo are much better than the non-contextual fastText baseline.\n\nQuestion: How much better are the results of", "answers": ["By 14 times.", "up to 1.95 times larger"], "all_classes": null, "length": 5221}
{"pred": "10000\n\nQuestion: How many entities does the dataset contain?\n\nAnswer: 10000\n\nQuestion: How many words does the dataset contain?\n\nAnswer: 10000\n\nQuestion: How many characters does the dataset contain?\n\nAnswer: 10000\n\nQuestion: How many graphemes does the dataset contain?\n\nAnswer: 10000\n\nQuestion: How many words does the dataset contain?\n\nAnswer: 10000\n\nQuestion: How many characters does the dataset contain", "answers": ["3606", "6946"], "all_classes": null, "length": 4911}
{"pred": "They compare to MLP, Eusboost, MWMOTE.\n\nQuestion: What is the data used for the experiments?\n\nAnswer: The data used for the experiments is GTZAN Music-Speech database.\n\nQuestion: What is the proportion of training data used for the experiments?\n\nAnswer: The proportion of training data used for the experiments is 50%, 25%, 12.5%, 6.25%, 3.125%, 1.5625%, 0.78125%, 0.390", "answers": ["MLP", "Eusboost, MWMOTE"], "all_classes": null, "length": 4806}
{"pred": "Yes.\n\nQuestion: Does their NER model learn NER from text only?\n\nAnswer: No.\n\nQuestion: Does their NER model learn NER from images only?\n\nAnswer: No.\n\nQuestion: Does their NER model learn NER from text and images?\n\nAnswer: Yes.\n\nQuestion: Does their NER model learn NER from text and images?\n\nAnswer: Yes.\n\nQuestion: Does their NER model learn NER from text and images?\n\nAnswer: Yes.\n\nQuestion: Does their NER model learn NER from", "answers": ["Yes", "Yes"], "all_classes": null, "length": 6426}
{"pred": "No, they evaluate on both English and Chinese datasets.\n\nQuestion: Do they use any extra information?\n\nAnswer: No, they only use the text corpus.\n\nQuestion: Do they use any extra information?\n\nAnswer: No, they only use the text corpus.\n\nQuestion: Do they use any extra information?\n\nAnswer: No, they only use the text corpus.\n\nQuestion: Do they use any extra information?\n\nAnswer: No, they only use the text corpus.\n\nQuestion: Do they use any extra information?\n\nAnswer: No,", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 6758}
{"pred": "What was their highest MRR score?\n\nIn one more experiment, we would like to find the question's highest MRR score.\n\nAnswer: What was their highest MRR score?\n\nAnswer: What was their highest MRR score?\n\nAnswer: What was their highest MRR score?\n\nAnswer: What was their highest MRR score?\n\nAnswer: What was their highest MRR score?\n\nAnswer: What was their highest MRR score?\n\nAnswer: What was their highest MRR score?\n\nAnswer: What was their highest MRR score?\n\nAnswer", "answers": ["0.5115", "0.6103"], "all_classes": null, "length": 11029}
{"pred": "They evaluate on the Wall Street Journal (WSJ) corpus.\n\nQuestion: What is the performance of the model?\n\nAnswer: They report the performance on the WSJ corpus in Table TABREF1.\n\nQuestion: What is the performance of the model on the Penn Treebank?\n\nAnswer: They report the performance on the Penn Treebank in Table TABREF2.\n\nQuestion: What is the performance of the model on the Wall Street Journal (WSJ) corpus?\n\nAnswer: They report the performance on the WSJ corpus in", "answers": [" Wall Street Journal (WSJ) portion of the Penn Treebank", "Unanswerable"], "all_classes": null, "length": 6757}
{"pred": "The authors evidence the claim that many engineers find it a big overhead to choose from multiple frameworks, models and optimization techniques by conducting a survey among engineers and identifying a spectrum of three typical personas.\n\nQuestion: What are the three typical personas?\n\nAnswer: The three typical personas are:\n\n1. Engineers who are familiar with the frameworks, models and optimization techniques.\n\n2. Engineers who are familiar with the frameworks, but not models and optimization techniques.\n\n3. Engineers who are familiar with the models and optimization techniques, but not frameworks.\n\nQuestion: What are the", "answers": ["By conducting a survey among engineers", "Unanswerable"], "all_classes": null, "length": 2833}
{"pred": "On the WebQSP and SimpleQuestions benchmarks, they achieve the state of the art.\n\nQuestion: How do they achieve the state of the art?\n\nAnswer: They achieve the state of the art by using a hierarchical model.\n\nQuestion: How do they achieve the state of the art?\n\nAnswer: They achieve the state of the art by using a hierarchical model.\n\nQuestion: How do they achieve the state of the art?\n\nAnswer: They achieve the state of the art by using a hierarchical model.\n\nQuestion: How do they achieve the", "answers": ["SimpleQuestions, WebQSP", "WebQSP, SimpleQuestions"], "all_classes": null, "length": 7555}
