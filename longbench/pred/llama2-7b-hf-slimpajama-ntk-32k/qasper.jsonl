{"pred": "The dataset BIBREF8 , manually labelled by an expert, has been publicly released and is available to researchers and interested parties.\n\nQuestion: What is the difference between the number of followers and the number of friends?\n\nAnswer: The number of followers is the number of users that follow an account. The number of friends is the number of users that an account follows.\n\nQuestion: What is the difference between the number of mentions and the number of URLs?\n\nAnswer: The number of mentions is the number of users that are mentioned in a tweet. The number of URLs is the", "answers": ["Ground truth is not established in the paper"], "all_classes": null, "length": 3141}
{"pred": "GhostVLAD is an extension of the NetVLAD approach, which we discussed in the previous section. The GhostVLAD model was proposed for face recognition by Y. Zhong [10]. GhostVLAD works exactly similar to NetVLAD except it adds Ghost clusters along with the NetVLAD clusters. So, now we will have a K+G number of clusters instead of K clusters. Where G is the number of ghost clusters, we want to add (typically 2-4). The Ghost clusters are added to map any noisy or irrelevant content into ghost clusters and are", "answers": ["extension of the NetVLAD, adds Ghost clusters along with the NetVLAD clusters", "An extension of NetVLAD which replaces hard assignment-based clustering with soft assignment-based clustering with the additon o fusing Ghost clusters to deal with noisy content."], "all_classes": null, "length": 2454}
{"pred": "The proposed model outperforms the state of the art results by 68.8% to 71.8%.\n\nQuestion: What is the name of the dataset used in the experiments?\n\nAnswer: The Interactive Emotional Dyadic Motion Capture (IEMOCAP) dataset.\n\nQuestion: What is the name of the toolkit used to extract the MFCC features?\n\nAnswer: The OpenSMILE toolkit.\n\nQuestion: What is the name of the toolkit used to extract the prosodic features?\n\nAnswer: The OpenSMI", "answers": ["the attention model, MDREA, also outperforms the best existing research results (WAP 0.690 to 0.688)"], "all_classes": null, "length": 3207}
{"pred": "The article proposes the use of context tweets, which are tweets that the user has replied to or quoted.\n\nQuestion: What is the most accurate model?\n\nAnswer: The most accurate model is the bidirectional GRU network with LTC modules.\n\nQuestion: What is the most accurate feature?\n\nAnswer: The most accurate feature is the use of context tweets.\n\nQuestion: What is the most accurate variant?\n\nAnswer: The most accurate variant is the bidirectional GRU network with LTC modules.\n\nQuestion: What is the most accurate ensemble model?\n", "answers": ["using tweets that one has replied or quoted to as contextual information", "text sequences of context tweets"], "all_classes": null, "length": 2060}
{"pred": "They looked at FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney.\n\nQuestion: What is the name of the dataset they used?\n\nAnswer: Affective Text dataset\n\nQuestion: What is the name of the dataset they used?\n\nAnswer: Fairy Tales dataset\n\nQuestion: What is the name of the dataset they used?\n\nAnswer: ISEAR\n", "answers": ["FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney", "FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney."], "all_classes": null, "length": 3411}
{"pred": "The hashtag dataset contains only English data. The SemEval dataset contains both English and non-English data.\n\nQuestion: How many hashtags are in the hashtag dataset?\n\nAnswer: The hashtag dataset contains 12,594 unique hashtags.\n\nQuestion: How many hashtags are in the SemEval dataset?\n\nAnswer: The SemEval dataset contains 12,284 unique hashtags.\n\nQuestion: How many hashtags are in the hashtag dataset that are multi-token?\n\nAnswer:", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3735}
{"pred": "The evaluation protocol is described in ยง SECREF7 .\n\nQuestion: What is the size of the corpus?\n\nAnswer: The corpus consists of 30 topics, each with around 40 documents and a summarizing concept map.\n\nQuestion: What is the average cluster size?\n\nAnswer: The average cluster size is 97,880 tokens.\n\nQuestion: What is the average length of documents?\n\nAnswer: The average length of documents is 2413 tokens.\n\nQuestion: What is the average length of concepts?\n\nAnswer: The", "answers": ["Answer with content missing: (Evaluation Metrics section) Precision, Recall, F1-scores, Strict match, METEOR, ROUGE-2"], "all_classes": null, "length": 4263}
{"pred": "We evaluated our model on three benchmark datasets, namely the CNN/DailyMail news highlights dataset BIBREF24, the New York Times Annotated Corpus (NYT; BIBREF25), and XSum BIBREF22. These datasets represent different summary styles ranging from highlights to very brief one sentence summaries. The summaries also vary with respect to the type of rewriting operations they exemplify (e.g., some showcase more cut and paste operations while others are genuinely abstractive). Table TABREF12 presents statistics on these datasets (test set);", "answers": ["CNN/DailyMail news highlights, New York Times Annotated Corpus, XSum", "the CNN/DailyMail news highlights dataset BIBREF24, the New York Times Annotated Corpus (NYT; BIBREF25), XSum BIBREF22"], "all_classes": null, "length": 4369}
{"pred": "The proposed approach is compared with other WSD approaches employing word embeddings. The comparison is done on the basis of the Spearman correlation between the human scores and the model scores. The Spearman correlation is computed for various metrics such as MaxCos, AvgCos, KL$\\_$approx, KL$\\_$comp, and KL$\\_$comp. The results are shown in Table TABREF17. It is evident from Table TABREF17 that GM$\\_$KL achieves better correlation than existing approaches for various metrics on SCWS dataset.\n", "answers": ["GM$\\_$KL achieves better correlation than existing approaches for various metrics on SCWS dataset."], "all_classes": null, "length": 2189}
{"pred": "The ensemble method is a technique that combines the predictions of multiple models to improve the overall accuracy of the system. In their ensemble method, they use a greedy algorithm to select the best performing model from a set of models, and then combine the predictions of the selected model with the predictions of the other models in the ensemble. This ensures that the ensemble is able to take advantage of the strengths of each individual model, while also compensating for the weaknesses of each model.\n\nQuestion: What is the BookTest dataset?\n\nAnswer: The BookTest dataset is a new dataset similar to the Children's", "answers": ["simply averaging the predictions from the constituent single models"], "all_classes": null, "length": 4212}
{"pred": "The source of the Friends dataset is the scripts of the Friends TV sitcom. The source of the EmotionPush dataset is Facebook messenger chats.\n\nQuestion: What are the properties of the datasets?\n\nAnswer: The Friends dataset is speech-based dataset which is annotated dialogues from the TV sitcom. The EmotionPush dataset is chat-based dataset which is made up of Facebook messenger chats.\n\nQuestion: What are the properties of the datasets?\n\nAnswer: The Friends dataset is speech-based dataset which is annotated dialogues from the TV sitcom.", "answers": ["Friends TV sitcom, Facebook messenger chats"], "all_classes": null, "length": 3185}
{"pred": "The paper focuses on English.\n\nQuestion: what is the main idea of the paper?\n\nAnswer: The main idea of the paper is to use neural machine translation to improve text simplification.\n\nQuestion: what is the main contribution of the paper?\n\nAnswer: The main contribution of the paper is to use neural machine translation to improve text simplification.\n\nQuestion: what is the main conclusion of the paper?\n\nAnswer: The main conclusion of the paper is that neural machine translation can be used to improve text simplification.\n\nQuestion: what is the main finding of the paper?\n", "answers": ["English", "Simple English"], "all_classes": null, "length": 2243}
{"pred": "IMDb dataset of movie reviews.\n\nQuestion: What is the name of the dataset used for named entity recognition?\n\nAnswer: Groningen Meaning Bank (GMB)\n\nQuestion: What is the name of the dataset used for sentiment analysis?\n\nAnswer: IMDb dataset of movie reviews.\n\nQuestion: What is the name of the dataset used for named entity recognition?\n\nAnswer: Groningen Meaning Bank (GMB)\n\nQuestion: What is the name of the dataset used for sentiment analysis?\n\nAnswer: IMDb dataset of movie reviews.\n\nQuestion: What is the name", "answers": ["IMDb dataset of movie reviews", "IMDb"], "all_classes": null, "length": 2327}
{"pred": "The proposed system achieves 92.3% accuracy on the DL-PS dataset, 91.2% on the EC-MT dataset, and 92.1% on the EC-UQ dataset.\n\nQuestion: What is the difference between the proposed system and the baseline system?\n\nAnswer: The proposed system uses adversarial training to learn worker independent features from crowd annotations. The baseline system uses the crowd annotations directly.\n\nQuestion: What is the difference between the proposed system and the baseline system?\n\nAnswer: The proposed system uses adversarial training to learn worker", "answers": ["F1 scores of 85.99 on the DL-PS data,  75.15 on the EC-MT data and 71.53 on the EC-UQ data ", "F1 of 85.99 on the DL-PS dataset (dialog domain); 75.15 on EC-MT and 71.53 on EC-UQ (e-commerce domain)"], "all_classes": null, "length": 5310}
{"pred": "Yes.\n\nQuestion: Did they experiment with this new dataset? ::: Answer\n\nAnswer: Yes.\n\nQuestion: Did they experiment with this new dataset? ::: Answer ::: Answer\n\nAnswer: Yes.\n\nQuestion: Did they experiment with this new dataset? ::: Answer ::: Answer ::: Answer\n\nAnswer: Yes.\n\nQuestion: Did they experiment with this new dataset? ::: Answer ::: Answer ::: Answer ::: Answer\n\nAnswer: Yes.\n\nQuestion: Did they experiment with this new dataset? ::: Answer ::: Answer ::: Answer ::: Answer", "answers": ["No"], "all_classes": null, "length": 3445}
{"answers": "", "length": 13401}
{"answers": "", "length": 10349}
{"pred": "The RNN-based NMT model is based on BIBREF0 which is introduced above. Both the encoder and decoder used 2-layer RNN with 1024 LSTM cells, and the encoder is a bi-directional RNN. The batch size, threshold of element-wise gradient clipping and initial learning rate of Adam optimizer BIBREF21 were set to 128, 5.0 and 0.001. When trained the model on augmented dataset, we used 4-layer RNN. Several techniques were investigated to train", "answers": ["RNN-based NMT model, Transformer-NMT"], "all_classes": null, "length": 3708}
{"pred": "The three regularization terms are neutral features, maximum entropy, and KL divergence.\n\nQuestion: What is the difference between the three regularization terms?\n\nAnswer: The difference between the three regularization terms is that neutral features are features that are not informative indicator of any classes, maximum entropy is the maximum entropy principle, and KL divergence is the KL divergence between the predicted and reference class distributions.\n\nQuestion: What is the difference between the regularization terms and the objective function?\n\nAnswer: The regularization terms are added to the objective function to make the model more robust.\n", "answers": ["a regularization term associated with neutral features, the maximum entropy of class distribution regularization term, the KL divergence between reference and predicted class distribution", "a regularization term associated with neutral features,  the maximum entropy of class distribution, KL divergence between reference and predicted class distribution"], "all_classes": null, "length": 3604}
{"pred": "The baselines are: 1) SVM with unigram, bigram, and trigram features, which is a standard yet rather strong classifier for text features; 2) SVM with average word embedding, where a document is represented as a continuous representation by averaging the embeddings of the composite words; 3) SVM with average transformed word embeddings (the INLINEFORM0 in equation EQREF6 ), where a document is represented as a continuous representation by averaging the transformed embeddings of the composite words; 4) two mature deep learning models on text classification, CNN", "answers": ["SVM with unigram, bigram, and trigram features, SVM with average word embedding, SVM with average transformed word embeddings, CNN, ecurrent Convolutional Neural Networks, SVM and deep learning models with comment information", "SVM with unigram, bigram, trigram features, with average word embedding, with average transformed word embeddings, CNN and RCNN, SVM, CNN, RCNN with comment information"], "all_classes": null, "length": 4512}
{"pred": "They improved by 1.2%.\n\nQuestion: How many people were in the study?\n\nAnswer: 100 people.\n\nQuestion: How many people were in the study?\n\nAnswer: 100 people.\n\nQuestion: How many people were in the study?\n\nAnswer: 100 people.\n\nQuestion: How many people were in the study?\n\nAnswer: 100 people.\n\nQuestion: How many people were in the study?\n\nAnswer: 100 people.\n\nQuestion: How many people were in the", "answers": ["They decrease MAE in 0.34"], "all_classes": null, "length": 2735}
{"pred": "The authors of the article propose a novel Transformer architecture wherein they simply replace softmax with $\\alpha$-entmax in the attention heads. Concretely, they replace the row normalization $\\mathbf{\\pi}$ in Equation DISPLAY_FORM7 by\nThis change leads to sparse attention weights, as long as $\\alpha >1$; in particular, $\\alpha =1.5$ is a sensible starting point BIBREF14.\nAdaptively Sparse Transformers with $\\alpha$-entmax ::: Different $\\alpha$ per head.\nUnlike LSTM-based seq2", "answers": ["the attention heads in the proposed adaptively sparse Transformer can specialize more and with higher confidence", "We introduce sparse attention into the Transformer architecture"], "all_classes": null, "length": 4902}
{"pred": "the baseline is the sentence-level MT system.\n\nQuestion: what is the DocRepair model?\n\nAnswer: the DocRepair model is a monolingual sequence-to-sequence model that maps inconsistent groups of sentences into consistent ones. Consistent groups come from the original training data; the inconsistent groups are obtained by sampling round-trip translations for each isolated sentence.\n\nQuestion: what is the difference between the baseline and the DocRepair model?\n\nAnswer: the baseline is a sentence-level MT system. The DocRepair model is a mon", "answers": [" MT system on the data released by BIBREF11", "Transformer base, two-pass CADec model"], "all_classes": null, "length": 3716}
{"pred": "We use the following metrics for evaluation:\n\n*\n\n*Zero-shot XNLI test accuracy\n\n*Zero-shot UD parsing LAS\n\n*Supervised UD parsing LAS\n\n\nQuestion: What is the difference between RAMEN$_{\\textsc {base}}$ and RAMEN$_{\\textsc {large}}$?\n\nAnswer: RAMEN$_{\\textsc {base}}$ and RAMEN$_{\\textsc {large}}$ are two variants of RAMEN. RAMEN$_{\\textsc {base}}$ is a bilingual LM with 12 Transformer layers and RAMEN$_{\\textsc {", "answers": ["translation probabilities, Labeled Attachment Scores (LAS)", "accuracy, Labeled Attachment Scores (LAS)"], "all_classes": null, "length": 3402}
{"pred": "The attention module is pre-trained on the MT dataset.\n\nQuestion: What is the attention module pretrained on?\n\nAnswer: The attention module is pre-trained on the MT dataset.\n\nQuestion: What is the attention module pretrained on?\n\nAnswer: The attention module is pre-trained on the MT dataset.\n\nQuestion: What is the attention module pretrained on?\n\nAnswer: The attention module is pre-trained on the MT dataset.\n\nQuestion: What is the attention module pretrained on?\n\nAnswer:", "answers": ["the model is pre-trained on CTC-based ASR task and MT task in the pre-training stage."], "all_classes": null, "length": 4656}
{"pred": "The stylistic features are obtained from the textual features.\n\nQuestion: What is the purpose of the article?\n\nAnswer: The purpose of the article is to explain the importance of cognitive features in sarcasm detection.\n\nQuestion: What is the purpose of the article?\n\nAnswer: The purpose of the article is to explain the importance of cognitive features in sarcasm detection.\n\nQuestion: What is the purpose of the article?\n\nAnswer: The purpose of the article is to explain the importance of cognitive features in sarcasm detection.\n\nQuestion: What is the", "answers": ["Unanswerable"], "all_classes": null, "length": 3543}
{"pred": "The encoder has a single LSTM layer.\n\nQuestion: What is the size of the LSTM layer?\n\nAnswer: The LSTM layer has 100 units.\n\nQuestion: What is the size of the attention layer?\n\nAnswer: The attention layer has 100 units.\n\nQuestion: What is the size of the embedding layer?\n\nAnswer: The embedding layer has 100 units.\n\nQuestion: What is the size of the dropout layer?\n\nAnswer: The dropout layer has 0.3 units.\n\nQuestion", "answers": ["LSTM", "LSTM"], "all_classes": null, "length": 2289}
{"pred": "Yes. WordNet is a comprehensive lexical ontology that contains a wide range of taxonomic relations, including hypernymy, hyponymy, and synonymy.\n\nQuestion: Is WordNet useful for definition-based reasoning for this task?\n\nAnswer: Yes. WordNet contains definitions for many concepts, which can be used to answer questions about the definitions of those concepts.\n\nQuestion: Is WordNet useful for ISA reasoning for this task?\n\nAnswer: Yes. WordNet contains ISA relations between concepts, which can be used to answer questions about the ISA relations between concepts", "answers": ["Unanswerable", "Yes"], "all_classes": null, "length": 6391}
{"pred": "The baselines are:\n\n*\n\n*Jasper DR 10x5\n\n*Jasper DR 10x4\n\n*Jasper DR 10x3\n\n*Jasper DR 10x2\n\n*Jasper DR 10x1\n\n*Jasper DR 5x3\n\n*Jasper DR 5x2\n\n*Jasper DR 5x1\n\n*Jasper DR 3x3\n\n*Jasper DR 3x2\n\n*Jas", "answers": ["Unanswerable", "LF-MMI Attention\nSeq2Seq \nRNN-T \nChar E2E LF-MMI \nPhone E2E LF-MMI \nCTC + Gram-CTC"], "all_classes": null, "length": 1856}
{"pred": "Unanswerable\n\nQuestion: How many users do they look at?\n\nAnswer: Unanswerable\n\nQuestion: How many users do they look at?\n\nAnswer: Unanswerable\n\nQuestion: How many users do they look at?\n\nAnswer: Unanswerable\n\nQuestion: How many users do they look at?\n\nAnswer: Unanswerable\n\nQuestion: How many users do they look at?\n\nAnswer: Unanswerable\n\nQuestion: How many users do they look at?\n\nAnswer: Unanswerable\n\nQuestion: How many users do they look at", "answers": ["22,880 users", "20,000"], "all_classes": null, "length": 4160}
{"pred": "We use BLEU-1/4, ROUGE-L, Distinct-1/2, and coherence metrics.\n\nQuestion: What is the dataset used for evaluation?\n\nAnswer: We use the Food.com dataset.\n\nQuestion: What is the size of the dataset?\n\nAnswer: The dataset consists of 180K recipes and 700K user reviews.\n\nQuestion: What is the size of the training set?\n\nAnswer: The training set consists of 130K recipes and 500K user reviews.\n", "answers": ["Byte-Pair Encoding perplexity  (BPE PPL),\nBLEU-1,\nBLEU-4,\nROUGE-L,\npercentage of distinct unigram (D-1),\npercentage of distinct bigrams(D-2),\nuser matching accuracy(UMA),\nMean Reciprocal Rank(MRR)\nPairwise preference over baseline(PP)", "BLEU-1/4 and ROUGE-L, likelihood of generated recipes using identical input specifications but conditioned on ten different user profiles, user matching accuracy (UMA), Mean Reciprocal Rank (MRR), neural scoring model from BIBREF33 to measure recipe-level coherence", " Distinct-1/2, UMA = User Matching Accuracy, MRR\n= Mean Reciprocal Rank, PP = Pairwise preference over baseline (evaluated for 310 recipe pairs per model)"], "all_classes": null, "length": 2673}
{"pred": "The labels are \"Yes\", \"No\", \"Unanswerable\", and \"No Answer\".\n\nQuestion: What is the purpose of the simulated data?\n\nAnswer: The purpose of the simulated data is to bootstrap a prototype QA system for extracting key clinical information from spoken conversations between nurses and patients.\n\nQuestion: What is the purpose of the simulated data?\n\nAnswer: The purpose of the simulated data is to bootstrap a prototype QA system for extracting key clinical information from spoken conversations between nurses and patients.\n\nQuestion: What is the", "answers": ["(1) the time the patient has been experiencing the symptom, (2) activities that trigger the symptom (to occur or worsen), (3) the extent of seriousness, (4) the frequency occurrence of the symptom, and (5) the location of symptom, No Answer", "the time the patient has been experiencing the symptom, activities that trigger the symptom, the extent of seriousness, the frequency occurrence of the symptom, the location of symptom, 9 symptoms"], "all_classes": null, "length": 3424}
{"pred": "The task-specific encoder is trained on the same data as the universal encoder.\n\nQuestion: How many sentences are annotated by experts?\n\nAnswer: 2000 sentences are annotated by experts.\n\nQuestion: How many sentences are annotated by crowd workers?\n\nAnswer: 57,505 sentences are annotated by crowd workers.\n\nQuestion: How many sentences are annotated by both experts and crowd workers?\n\nAnswer: 2,428 sentences are annotated by both experts and crowd workers.\n\nQuestion: How many", "answers": ["57,505 sentences", "57,505 sentences"], "all_classes": null, "length": 4371}
{"pred": "The Transformer architecture BIBREF0 for deep neural networks has quickly risen to prominence in NLP through its efficiency and performance, leading to improvements in the state of the art of Neural Machine Translation BIBREF1, BIBREF2, as well as inspiring other powerful general-purpose models like BERT BIBREF3 and GPT-2 BIBREF4. At the heart of the Transformer lie multi-head attention mechanisms: each word is represented by multiple different weighted averages of its relevant context. As suggested by recent works on interpreting attention head roles, separate attention", "answers": ["four machine translation tasks: German -> English, Japanese -> English, Romanian -> English, English -> German", " four machine translation tasks, IWSLT 2017 German $\\rightarrow $ English BIBREF27, KFTT Japanese $\\rightarrow $ English BIBREF28, WMT 2016 Romanian $\\rightarrow $ English BIBREF29, WMT 2014 English $\\rightarrow $ German BIBREF30"], "all_classes": null, "length": 4898}
{"pred": "The improvement in performance for Estonian in the NER task is 1.2% for the first layer, 1.4% for the second layer, and 1.5% for the third layer.\n\nQuestion: What is the improvement in performance for Latvian in the NER task?\n\nAnswer: The improvement in performance for Latvian in the NER task is 1.2% for the first layer, 1.4% for the second layer, and 1.5% for the third layer.\n\nQuestion: What is the improvement in performance for Slovenian in the", "answers": ["5 percent points.", "0.05 F1"], "all_classes": null, "length": 3290}
{"pred": "They have a background in computational linguistics, social science, and the humanities.\n\nQuestion: What is the research question?\n\nAnswer: The research question is: Does eliminating these \"echo chambers\" diminish the amount of hate speech overall?\n\nQuestion: What is the research process?\n\nAnswer: The research process is:\n\n\n*\n\n*Identifying the research questions\n\n*Data acquisition\n\n*Data compilation\n\n*Conceptualization\n\n*Operationalization\n\n*Modeling considerations\n\n*Annotation\n\n*Data pre-processing", "answers": ["Unanswerable"], "all_classes": null, "length": 8506}
{"pred": "No. The paper is introducing a supervised approach to spam detection.\n\nQuestion: What is the difference between the two datasets?\n\nAnswer: The Honeypot dataset is a public dataset, while the Weibo dataset is a self-collected dataset.\n\nQuestion: What is the difference between the two features?\n\nAnswer: The Global Outlier Standard Score (GOSS) measures the degree that a user's tweet content is related to a certain topic compared to the other users. Specifically, the \"GOSS\" score of user $i$ on topic $k$ can", "answers": ["No", "No"], "all_classes": null, "length": 2239}
{"pred": "The Nguni languages are similar to each other and harder to distinguish. The same is true of the Sotho languages.\n\nQuestion: Which languages are disjunctively written?\n\nAnswer: The Nguni languages are similar to each other and harder to distinguish. The same is true of the Sotho languages.\n\nQuestion: Which languages are conjunctively written?\n\nAnswer: The Nguni languages are similar to each other and harder to distinguish. The same is true of the Sotho languages.\n\nQuestion: Which languages are disjunctively written and conjunctively written?\n\n", "answers": ["Nguni languages (zul, xho, nbl, ssw), Sotho languages (nso, sot, tsn)", "The Nguni languages are similar to each other, The same is true of the Sotho languages"], "all_classes": null, "length": 1877}
{"pred": "The comparison is between 2-layers LSTM and 2-layers LSTM with soft target.\n\nQuestion: what is the difference between the two lstm models?\n\nAnswer: The 2-layers LSTM with soft target is trained with sMBR loss function, while the 2-layers LSTM is trained with CE loss function.\n\nQuestion: what is the difference between the two lstm models?\n\nAnswer: The 2-layers LSTM with soft target is trained with sMBR loss function, while the 2-layers LSTM is trained", "answers": ["Unidirectional LSTM networks with 2, 6, 7, 8, and  9 layers."], "all_classes": null, "length": 4286}
{"pred": "The data set is large.\n\nQuestion: How many documents are in the data set?\n\nAnswer: The data set contains 29,794 documents.\n\nQuestion: How many documents are in the Wikipedia data set?\n\nAnswer: The Wikipedia data set contains 29,794 documents.\n\nQuestion: How many documents are in the arXiv data set?\n\nAnswer: The arXiv data set contains 29,794 documents.\n\nQuestion: How many documents are in the Wikipedia data set?\n\nAnswer: The Wikipedia data set contains ", "answers": ["a sample of  29,794 wikipedia articles and 2,794 arXiv papers "], "all_classes": null, "length": 4187}
{"pred": "The human judgements were assembled by a group of 50 native people who were well-versed in both English and Tamil languages.\n\nQuestion: What is the difference between the RNNMorph and RNNSearch models?\n\nAnswer: The RNNMorph model uses morphological segmentation to split the words into morphemes. The RNNSearch model uses word2vec embeddings to embed semantic understanding.\n\nQuestion: What is the difference between the RNNMorph and RNNMorph + Word2Vec models?\n\nAnswer: The RNNMorph +", "answers": ["50 human annotators ranked a random sample of 100 translations by Adequacy, Fluency and overall ranking on a 5-point scale.", "adequacy, precision and ranking values"], "all_classes": null, "length": 5344}
{"pred": "Yes, they do. They test their framework performance on commonly used language pairs, such as English-to-German.\n\nQuestion: Do they use the same encoder and decoder for all languages?\n\nAnswer: No, they do not. They use the same encoder and decoder for all languages.\n\nQuestion: Do they use the same encoder and decoder for all languages?\n\nAnswer: No, they do not. They use the same encoder and decoder for all languages.\n\nQuestion: Do they use the same encoder and decoder for all languages?\n\nAnswer", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4472}
{"pred": "The models are evaluated by measuring the accuracy of the decoding strategy $p_{\\beta }(x\\mid z)$ and the efficiency of the encoding strategy $q_{\\alpha }(z\\mid x)$. The accuracy is measured as the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence. The efficiency is measured as the fraction of tokens that are kept in the keywords.\n\nQuestion: What is the intuition behind the assumption that humans have an intuitive sense of retaining important keywords?\n\nAnswer: We assume that humans have an intuitive sense of retaining important keywords because", "answers": ["by training an autocomplete system on 500K randomly sampled sentences from Yelp reviews", "efficiency of a communication scheme $(q_{\\alpha },p_{\\beta })$ by the retention rate of tokens, which is measured as the fraction of tokens that are kept in the keywords, accuracy of a scheme is measured as the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence"], "all_classes": null, "length": 1873}
{"pred": "The evaluation metrics are accuracy, precision, recall, and F-measure.\n\nQuestion: What is the difference between accuracy and precision?\n\nAnswer: Accuracy is the ratio of correct predictions to the total number of predictions. Precision is the ratio of correct predictions to the total number of positive predictions.\n\nQuestion: What is the difference between recall and F-measure?\n\nAnswer: Recall is the ratio of correct predictions to the total number of positive predictions. F-measure is the harmonic mean of precision and recall.\n\nQuestion: What is the difference between precision and recall?\n\n", "answers": ["Precision, Recall, F-measure, accuracy", "Precision, Recall and F-measure"], "all_classes": null, "length": 3044}
{"pred": "The source domain is the domain from which the labeled data is available, and the target domain is the domain from which the unlabeled data is available.\n\nQuestion: What is the main difference between the source and target domains?\n\nAnswer: The main difference between the source and target domains is that the vocabulary of the target domain is different from that of the source domain.\n\nQuestion: What is the main difference between the source and target domains in terms of sentiment expressions?\n\nAnswer: The main difference between the source and target domains in terms of sentiment expressions is that the target domain contains more domain", "answers": ["Book, electronics, beauty, music, IMDB, Yelp, cell phone, baby, DVDs, kitchen", "we use set 1 of the source domain as the only source with sentiment label information during training, and we evaluate the trained model on set 1 of the target domain, Book (BK), Electronics (E), Beauty (BT), and Music (M)"], "all_classes": null, "length": 5061}
{"pred": "The PRU is compared with LSTM, RNN, and CNN.\n\nQuestion: what is the difference between the pyramidal transformation and the linear transformation?\n\nAnswer: The pyramidal transformation uses subsampling to effect multiple views of the input vector. The subsampled representations are combined in a pyramidal fusion structure, resulting in richer interactions between the individual dimensions of the input vector than is possible with a linear transformation.\n\nQuestion: what is the difference between the grouped linear transformation and the linear transformation?\n\nAnswer: The weights learned using the linear transformation (Eq. EQREF", "answers": ["Variational LSTM, CharCNN, Pointer Sentinel-LSTM, RHN, NAS Cell, SRU, QRNN, RAN, 4-layer skip-connection LSTM, AWD-LSTM, Quantized LSTM"], "all_classes": null, "length": 3319}
{"pred": "The NeuronBlocks includes the following modules:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "answers": ["Embedding Layer, Neural Network Layers, Loss Function, Metrics", "Embedding Layer, Neural Network Layers, Loss Function, Metrics"], "all_classes": null, "length": 1678}
{"pred": "The data used for training and testing is the multilingual pronunciation corpus collected by deri2016grapheme. It consists of spellingโpronunciation pairs extracted from Wiktionary. It is already partitioned into training and test sets. Corpus statistics are presented in Table TABREF10 .\n\nQuestion: what is the difference between the high resource and low resource results?\n\nAnswer: The high resource results are for languages for which BIBREF13 used non-adapted wFST models. The low resource results are for languages for which they built adapted models", "answers": ["the Carnegie Mellon Pronouncing Dictionary BIBREF12, the multilingual pronunciation corpus collected by deri2016grapheme , ranscriptions extracted from Wiktionary", "multilingual pronunciation corpus collected by deri2016grapheme"], "all_classes": null, "length": 3244}
{"pred": "The baselines were:\n\n\n*\n\n*Bibref12: BERT\n\n*Bibref12: RoBERTa\n\n*Bibref12: XLNet\n\n\nQuestion: What were the results?\n\nAnswer: The results are as follows:\n\n\n*\n\n*Bibref12: BERT: 0.82 F1\n\n*Bibref12: RoBERTa: 0.82 F1\n\n*Bibref12: XLNet: 0.82 F", "answers": ["varied from Maximum Entropy Classifiers (BIBREF4) to Support Vector Machines (BIBREF5,BIBREF6,BIBREF7,BIBREF8), Recursive Neural Networks (BIBREF9,BIBREF10), Convolutional Neural Networks (BIBREF11) and most recently transfer learning-based architectures like Bidirectional Encoder Representation from Transformers (BERT) (BIBREF12)"], "all_classes": null, "length": 2215}
{"pred": "The languages they use in their experiment are English, Spanish, Finnish, and Basque.\n\nQuestion: What is the purpose of the article?\n\nAnswer: The purpose of the article is to analyze the effect of both human and machine translation in cross-lingual models.\n\nQuestion: What are the models used in the experiments?\n\nAnswer: The models used in the experiments are Roberta and XLM-R.\n\nQuestion: What are the training variants explored?\n\nAnswer: The training variants explored are the original training set in English (Orig), an English paraphrase", "answers": ["English\nFrench\nSpanish\nGerman\nGreek\nBulgarian\nRussian\nTurkish\nArabic\nVietnamese\nThai\nChinese\nHindi\nSwahili\nUrdu\nFinnish", "English, Spanish, Finnish"], "all_classes": null, "length": 4086}
{"pred": "They test their method on the task of predicting hashtags for a post from its latent representation.\n\nQuestion: What is the objective function they use to optimize?\n\nAnswer: They optimize the categorical cross-entropy loss between predicted and true hashtags.\n\nQuestion: What is the difference between the word-level and character-level models?\n\nAnswer: The word-level model uses a lookup table of word vectors, while the character-level model uses a lookup table of character vectors.\n\nQuestion: What is the difference between the word-level and character-level models in terms", "answers": ["None"], "all_classes": null, "length": 2473}
{"pred": "Yes, they use pretrained embeddings.\n\nQuestion: What is the vocabulary size?\n\nAnswer: The vocabulary size is 400K words.\n\nQuestion: What is the number of fields in the infobox?\n\nAnswer: The number of fields in the infobox is 10.\n\nQuestion: What is the number of values in each field?\n\nAnswer: The number of values in each field is 1.\n\nQuestion: What is the number of timesteps in the decoder?\n\nAnswer: The number of tim", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4457}
{"pred": "PolyReponse was evaluated against a baseline system that uses a slot-value ontology to generate responses. The baseline system was trained on a dataset of 100K conversations, and was able to achieve a 70% success rate on the test set. PolyReponse was trained on 727M conversations, and achieved a 90% success rate on the test set.\n\nQuestion: What is the size of the PolyReponse dataset?\n\nAnswer: The PolyReponse dataset is a large dataset of conversations, with over 727M", "answers": ["No", "No"], "all_classes": null, "length": 2738}
{"pred": "They use the Linguistic Inquiry and Word Count (LIWC) BIBREF12 , which is a lexical resource that groups words into categories.\n\nQuestion: What is the correlation between Money and Positive Feelings?\n\nAnswer: The correlation is -0.49.\n\nQuestion: What is the correlation between Religion and Hard Work?\n\nAnswer: The correlation is -0.49.\n\nQuestion: What is the correlation between Money and Hard Work?\n\nAnswer: The correlation is 0.49.\n\nQuestion: What is the", "answers": ["using the Meaning Extraction Method", "Unanswerable"], "all_classes": null, "length": 1440}
{"answers": "", "length": 14472}
{"pred": "The length of n-grams aligned using PARENT is 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31,", "answers": ["Unanswerable", "Answer with content missing: (Parent subsections) combine precisions for n-gram orders 1-4"], "all_classes": null, "length": 3827}
{"pred": "The Twitter dataset consists of 1,873 conversation threads, roughly 14k tweets.\n\nQuestion: How many conversations are in the OSG dataset?\n\nAnswer: The OSG dataset consists of 295 thousand conversations, each conversation contains on average 6 comments.\n\nQuestion: How many users are in the Twitter dataset?\n\nAnswer: The Twitter dataset consists of 1,873 conversation threads, roughly 14k tweets.\n\nQuestion: How many users are in the OSG dataset?\n\nAnswer: The OSG dataset consists of ", "answers": ["1,873 Twitter conversation threads, roughly 14k tweets", "1,873 Twitter conversation threads, roughly 14k tweets"], "all_classes": null, "length": 3721}
{"answers": "", "length": 14660}
{"pred": "Wikipedia conversations and Reddit conversations.\n\nQuestion: What is the main insight of the model?\n\nAnswer: The model can capture the dynamics of a conversation as it develops.\n\nQuestion: What is the generative component of the model?\n\nAnswer: It is a hierarchical recurrent encoder-decoder (HRED) architecture.\n\nQuestion: What is the prediction component of the model?\n\nAnswer: It is a multilayer perceptron (MLP) with 3 fully-connected layers, leaky ReLU activations between layers, and sigm", "answers": [" `Conversations Gone Awry' dataset, subreddit ChangeMyView", "An expanded version of the existing 'Conversations Gone Awry' dataset and the ChangeMyView dataset, a subreddit whose only annotation is whether the conversation required action by the Reddit moderators. "], "all_classes": null, "length": 4718}
{"pred": "No.\n\nQuestion: What is the name of the pipeline?\n\nAnswer: Agatha.\n\nQuestion: What is the name of the ontology?\n\nAnswer: Criminal Law Ontology.\n\nQuestion: What is the name of the thesaurus?\n\nAnswer: Eurovoc.\n\nQuestion: What is the name of the terminology database?\n\nAnswer: IATE.\n\nQuestion: What is the name of the language detection module?\n\nAnswer: Freeling.\n\nQuestion: What is the name of the part-of-speech tagging", "answers": ["No", "No"], "all_classes": null, "length": 2276}
{"pred": "We provide baselines using the official train-development-test split on the following tasks: automatic speech recognition (ASR), machine translation (MT) and speech translation (ST).\n\nQuestion: How many languages are covered in the corpus?\n\nAnswer: CoVoST is diversified with a rich set of speakers and accents. We further inspect the speaker demographics in terms of sample distributions with respect to speaker counts, accent counts and age groups, which is shown in Figure FIGREF6, FIGREF7 and FIGREF8. We observe that for 8 of the 1", "answers": ["Validated transcripts were sent to professional translators., various sanity checks to the translations,  sanity check the overlaps of train, development and test sets", "computed sentence-level BLEU, We manually inspected examples where the source transcript was identical to the translation, measured the perplexity of the translations, computed the ratio of English characters in the translations, calculate similarity scores between transcripts and translations"], "all_classes": null, "length": 2435}
{"pred": "The audio-RNN encodes MFCC features from the audio signal using equation EQREF2 . The last hidden state of the audio-RNN is concatenated with the prosodic features to form the final vector representation INLINEFORM0 , and this vector is then passed through a fully connected neural network layer to form the audio encoding vector A. On the other hand, the text-RNN encodes the word sequence of the transcript using equation EQREF2 . The final hidden states of the text-RNN are also passed through another fully connected neural network layer to form a textual encoding vector T.", "answers": ["combines the information from these sources using a feed-forward neural model", "encodes the information from audio and text sequences using dual RNNs and then combines the information from these sources using a feed-forward neural model"], "all_classes": null, "length": 3201}
{"pred": "The model improved by 2.11 BLEU, 1.7 FKGL and 1.07 SARI.\n\nQuestion: by how much did their model improve?\n\nAnswer: The model improved by 2.11 BLEU, 1.7 FKGL and 1.07 SARI.\n\nQuestion: by how much did their model improve?\n\nAnswer: The model improved by 2.11 BLEU, 1.7 FKGL and 1.07 SARI.\n\nQuestion: by how much did their", "answers": ["For the WikiLarge dataset, the improvement over baseline NMT is 2.11 BLEU, 1.7 FKGL and 1.07 SARI.\nFor the WikiSmall dataset, the improvement over baseline NMT is  8.37 BLEU.", "6.37 BLEU"], "all_classes": null, "length": 2271}
{"pred": "10 humans evaluated the results.\n\nQuestion: how many humans evaluated the results?\n\nAnswer: 10 humans evaluated the results.\n\nQuestion: how many humans evaluated the results?\n\nAnswer: 10 humans evaluated the results.\n\nQuestion: how many humans evaluated the results?\n\nAnswer: 10 humans evaluated the results.\n\nQuestion: how many humans evaluated the results?\n\nAnswer: 10 humans evaluated the results.\n\nQuestion: how many humans evaluated the results?\n\nAnswer: 10 humans evaluated the results.\n\nQuestion: how", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3711}
{"pred": "Tweets that were retweeted more than 1000 times.\n\nQuestion: What is the difference between the number of retweets of tweets containing fake news and the number of retweets of tweets not containing fake news?\n\nAnswer: There is no difference.\n\nQuestion: What is the difference between the number of favourites of tweets containing fake news and the number of favourites of tweets not containing fake news?\n\nAnswer: There is no difference.\n\nQuestion: What is the difference between the number of hashtags used in tweets containing fake news and the", "answers": ["Viral tweets are the ones that are retweeted more than 1000 times", "those that contain a high number of retweets"], "all_classes": null, "length": 3144}
{"pred": "CNN\n\nQuestion: Which basic neural architecture perform best by itself?\n\nAnswer: CNN\n\nQuestion: Which basic neural architecture perform best by itself?\n\nAnswer: CNN\n\nQuestion: Which basic neural architecture perform best by itself?\n\nAnswer: CNN\n\nQuestion: Which basic neural architecture perform best by itself?\n\nAnswer: CNN\n\nQuestion: Which basic neural architecture perform best by itself?\n\nAnswer: CNN\n\nQuestion: Which basic neural architecture perform best by itself?\n\nAnswer: CNN\n\nQuestion: Which basic neural architecture perform best by itself?\n\nAnswer: CNN\n", "answers": ["BERT"], "all_classes": null, "length": 1507}
{"pred": "The data was collected using crowdsourcing.\n\nQuestion: what is the size of the database?\n\nAnswer: The database consists of 3 parts. Part1 contains 5 Persian phrases and 3 English phrases. Part2 contains 3 random sequences of Persian month names. Part3 contains 8 Persian phrases and 5 English phrases.\n\nQuestion: what is the number of speakers in the database?\n\nAnswer: There are 1969 speakers in the database.\n\nQuestion: what is the number of utterances in the database?\n\n", "answers": ["Android application"], "all_classes": null, "length": 3795}
{"pred": "We used two methods for our empirical study:\n\n1) Deep Learning Model: To recognize entailment between two questions INLINEFORM0 (premise) and INLINEFORM1 (hypothesis), we adapted the neural network proposed by Bowman et al. BIBREF13 . Our DL model, presented in Figure FIGREF20 , consists of three 600d ReLU layers, with a bottom layer taking the concatenated sentence representations as input and a top layer feeding a softmax classifier. The sentence embedding model sums the Recurrent neural network (RNN", "answers": ["Logistic Regression, neural networks"], "all_classes": null, "length": 7257}
{"pred": "The benchmark dataset is the Social Honeypot dataset. It is a dataset collected by Lee et al. lee2010devils. The quality of the dataset is high.\n\nQuestion: What is the difference between the two types of spammers?\n\nAnswer: The two types of spammers are content polluters and fake accounts. Content polluters are those who post only malicious links. Fake accounts are those who post random copies of others' tweets.\n\nQuestion: What is the difference between legitimate users and spammers?\n\nAnswer: Legitimate", "answers": ["Social Honeypot dataset (public) and Weibo dataset (self-collected); yes", "Social Honeypot, which is not of high quality"], "all_classes": null, "length": 2242}
{"pred": "The decoder has a single LSTM layer.\n\nQuestion: What is the size of the LSTM layer?\n\nAnswer: The LSTM layer has 100 units.\n\nQuestion: What is the size of the attention layer?\n\nAnswer: The attention layer has 100 units.\n\nQuestion: What is the size of the embedding layer?\n\nAnswer: The embedding layer has 100 units.\n\nQuestion: What is the size of the dropout layer?\n\nAnswer: The dropout layer has 0.3 units.\n\nQuestion", "answers": ["LSTM", "LSTM"], "all_classes": null, "length": 2289}
{"pred": "Yes, they report results only on English data.\n\nQuestion: Do they report results only on English data?\n\nAnswer: Yes, they report results only on English data.\n\nQuestion: Do they report results only on English data?\n\nAnswer: Yes, they report results only on English data.\n\nQuestion: Do they report results only on English data?\n\nAnswer: Yes, they report results only on English data.\n\nQuestion: Do they report results only on English data?\n\nAnswer: Yes, they report results only on English data.\n\nQuestion: Do they report results only on", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3838}
{"pred": "The best performing model among the author's submissions is the ensemble of Logistic Regression, CNN and BERT. It achieved F1 score of 0.673 on dev (external) set.\n\nQuestion: What is the best performing model among all the submissions, what performance it had?\n\nAnswer: The best performing model among all the submissions is the ensemble of Logistic Regression, CNN and BERT. It achieved F1 score of 0.673 on dev (external) set.\n\nQuestion: What is the best performing model among all the submissions, what", "answers": ["For SLC task, the \"ltuorp\" team  has the best performing  model (0.6323/0.6028/0.6649 for F1/P/R  respectively) and for FLC task the \"newspeak\" team  has the best performing  model (0.2488/0.2863/0.2201 for F1/P/R respectively)."], "all_classes": null, "length": 1541}
{"pred": "The baseline is the M2M Transformer NMT model (b3) in Section SECREF4 .\n\nQuestion: what is the difference between the baseline and the final model?\n\nAnswer: The final model (VII) is better than the baseline (b3) in all the translation directions.\n\nQuestion: what is the difference between the final model and the model trained on the mixture of in-domain and out-of-domain data?\n\nAnswer: The final model (VII) is better than the model trained on the mixture of in-domain and out-of-", "answers": ["pivot-based translation relying on a helping language BIBREF10, nduction of phrase tables from monolingual data BIBREF14 , attentional RNN-based model (RNMT) BIBREF2, Transformer model BIBREF18, bi-directional model BIBREF11, multi-to-multi (M2M) model BIBREF8, back-translation BIBREF17", "M2M Transformer"], "all_classes": null, "length": 4542}
{"pred": "0.7033\n\nQuestion: What was their highest F-measure score?\n\nAnswer: 0.2862\n\nQuestion: What was their highest MRR score?\n\nAnswer: 0.6103\n\nQuestion: What was their highest precision score?\n\nAnswer: 0.1119\n\nQuestion: What was their highest recall score?\n\nAnswer: 0.7033\n\nQuestion: What was their highest F-measure score?\n\nAnswer: 0.2862\n\nQuestion: What was", "answers": ["0.7033", "0.7033"], "all_classes": null, "length": 6810}
{"pred": "The paper explores the use of word embeddings in the biomedical domain. Word embeddings are a type of neural network that learns a representation of a word by word coโoccurrence matrix. The basic idea is that the neural network learns a series of weights (the hidden layer within the neural network) that either maximizes the probability of a word given its context, referred to as the continuous bag of words (CBOW) approach, or that maximizes the probability of the context given a word, referred to as the Skipโgram approach. These approaches have been used in numerous recent papers.", "answers": ["Skipโgram, CBOW", "integrated vector-res, vector-faith, Skipโgram, CBOW"], "all_classes": null, "length": 4259}
{"pred": "The pre-ordering system matches words based on their morphological and syntactic properties.\n\nQuestion: What is the difference between the two pre-ordering systems?\n\nAnswer: The generic pre-ordering system matches words based on their morphological and syntactic properties. The Hindi-tuned pre-ordering system improves the generic rules by incorporating improvements found through an error analysis of English-Hindi reordering.\n\nQuestion: What is the difference between the two pre-ordering systems?\n\nAnswer: The generic pre-ordering system matches words based on their", "answers": ["Unanswerable", "CFILT-preorder system"], "all_classes": null, "length": 2231}
{"pred": "No. The paper does not explore extraction from electronic health records.\n\nQuestion: Does the paper explore extraction from biological literature?\n\nAnswer: No. The paper does not explore extraction from biological literature.\n\nQuestion: Does the paper explore extraction from medical literature?\n\nAnswer: No. The paper does not explore extraction from medical literature.\n\nQuestion: Does the paper explore extraction from electronic health records, biological literature and medical literature?\n\nAnswer: No. The paper does not explore extraction from electronic health records, biological literature and medical literature.\n\nQuestion", "answers": ["Yes"], "all_classes": null, "length": 3035}
{"pred": "We recruited seven experts with legal training to construct answers to Turker questions. Experts identify relevant evidence within the privacy policy, as well as provide meta-annotation on the question's relevance, subjectivity, OPP-115 category BIBREF49, and how likely any privacy policy is to contain the answer to the question asked.\n\nQuestion: What is the distribution of questions in the corpus across OPP-115 categories?\n\nAnswer: Table.TABREF16 presents the distribution of questions in the corpus across OPP-115 categories", "answers": ["Individuals with legal training", "Yes"], "all_classes": null, "length": 3846}
{"pred": "The models used for painting embedding are a CNN-RNN generative model and a seq2seq model with parallel text corpus for language style transfer.\n\nQuestion: What is the name of the poem dataset used for training the seq2seq model?\n\nAnswer: The name of the poem dataset used for training the seq2seq model is MultiM-Poem.\n\nQuestion: What is the name of the poem dataset used for training the seq2seq model?\n\nAnswer: The name of the poem dataset used for training the seq2seq model is UniM-Poem.\n\nQuestion:", "answers": ["generating a poem from images we use an existing actor-critic architecture, various types of sequence to sequence models"], "all_classes": null, "length": 1653}
{"pred": "The transformer layer works better.\n\nQuestion: Does the positional embeddings help?\n\nAnswer: No.\n\nQuestion: Does the RoBERT or ToBERT work better?\n\nAnswer: ToBERT works better.\n\nQuestion: Does the RoBERT or ToBERT work better for CSAT?\n\nAnswer: ToBERT works better.\n\nQuestion: Does the RoBERT or ToBERT work better for 20newsgroups?\n\nAnswer: ToBERT works better.\n\nQuestion: Does the RoBERT or ToBERT work better", "answers": ["Transformer over BERT (ToBERT)", "The transformer layer"], "all_classes": null, "length": 2655}
{"pred": "Yes.\n\nQuestion: What is the name of the data enrichment method?\n\nAnswer: WordNet-based data enrichment method.\n\nQuestion: What is the name of the attention mechanism in the coarse memory layer?\n\nAnswer: Knowledge aided mutual attention.\n\nQuestion: What is the name of the attention mechanism in the refined memory layer?\n\nAnswer: Knowledge aided self attention.\n\nQuestion: What is the name of the MRC model?\n\nAnswer: Knowledge Aided Reader (KAR).\n\nQuestion: What is the", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4127}
{"pred": "They addressed three topics of cyberbullying: personal attack, racism, and sexism.\n\nQuestion: What is the size of a post in terms of the number of words in the post?\n\nAnswer: The size of a post is the number of words in the post.\n\nQuestion: What is the size of the largest post in the Formspring dataset?\n\nAnswer: The largest post in the Formspring dataset has 2846 words.\n\nQuestion: What is the size of the largest post in the Twitter dataset?\n\nAnswer: The largest post in the Twitter dataset has ", "answers": ["personal attack, racism, and sexism", "racism, sexism, personal attack, not specifically about any single topic"], "all_classes": null, "length": 3244}
{"pred": "The contexts are split into three disjoint regions based on the two relation arguments: the left context, the middle context and the right context. Since in most cases the middle context contains the most relevant information for the relation, we want to focus on it but not ignore the other regions completely. Hence, we propose to use two contexts: (1) a combination of the left context, the left entity and the middle context; and (2) a combination of the middle context, the right entity and the right context. Due to the repetition of the middle context, we force the network to pay special attention to it. The", "answers": ["They use two independent convolutional and max-pooling layers on (1) a combination of the left context, the left entity and the middle context; and (2) a combination of the middle context, the right entity and the right context. They concatenated the two results after pooling to get the new context representation."], "all_classes": null, "length": 2435}
{"pred": "There are three different types of entities in the dataset.\n\nQuestion: What is the total number of entities in the dataset?\n\nAnswer: There are 10000 entities in the dataset.\n\nQuestion: What is the total number of sentences in the dataset?\n\nAnswer: There are 10000 sentences in the dataset.\n\nQuestion: What is the total number of words in the dataset?\n\nAnswer: There are 100000 words in the dataset.\n\nQuestion: What is the total number of characters in the dataset?\n\nAnswer: There", "answers": ["OurNepali contains 3 different types of entities, ILPRL contains 4 different types of entities", "three"], "all_classes": null, "length": 2851}
{"pred": "The resulting annotated data is of higher quality.\n\nQuestion: How much higher quality is the resulting annotated data?\n\nAnswer: The resulting annotated data is of higher quality.\n\nQuestion: How much higher quality is the resulting annotated data?\n\nAnswer: The resulting annotated data is of higher quality.\n\nQuestion: How much higher quality is the resulting annotated data?\n\nAnswer: The resulting annotated data is of higher quality.\n\nQuestion: How much higher quality is the resulting annotated data?\n\nAnswer: The resulting annotated data is of higher quality.\n\n", "answers": ["improvement when the difficult subset with expert annotations is mixed with the remaining crowd annotation is 3.5 F1 score, much larger than when a random set of expert annotations are added"], "all_classes": null, "length": 4399}
{"pred": "The gender imbalance is 65% men and 35% women.\n\nQuestion: What is the impact of gender imbalance on ASR performance?\n\nAnswer: The gender imbalance leads to a WER increase of 24% for women compared to men.\n\nQuestion: Is this as simple as a problem of gender proportion in the training data or are other factors entangled?\n\nAnswer: The gender imbalance is not as simple as a problem of gender proportion in the training data.\n\nQuestion: What is the impact of speaker's role on ASR", "answers": ["Women represent 33.16% of the speakers"], "all_classes": null, "length": 4055}
{"pred": "The English-German dataset.\n\nQuestion: What is the main metric used to evaluate the models?\n\nAnswer: Meteor.\n\nQuestion: What is the main metric used to evaluate the models?\n\nAnswer: Meteor.\n\nQuestion: What is the main metric used to evaluate the models?\n\nAnswer: Meteor.\n\nQuestion: What is the main metric used to evaluate the models?\n\nAnswer: Meteor.\n\nQuestion: What is the main metric used to evaluate the models?\n\nAnswer: Meteor.\n\nQuestion: What is the main metric used to evaluate the models?\n", "answers": ["the English-German dataset"], "all_classes": null, "length": 1833}
{"pred": "The strong baselines model is compared to is the Transformer model.\n\nQuestion: What is the Transformer model?\n\nAnswer: The Transformer model is a neural network model that is used to perform sequence labeling tasks.\n\nQuestion: What is the Transformer model composed of?\n\nAnswer: The Transformer model is composed of an encoder and a decoder. The encoder is used to generate the representation of sentences, and the decoder is used to perform segmentation according to the encoder scoring.\n\nQuestion: What are the two parts of the Transformer model?\n\nAnswer:", "answers": ["Baseline models are:\n- Chen et al., 2015a\n- Chen et al., 2015b\n- Liu et al., 2016\n- Cai and Zhao, 2016\n- Cai et al., 2017\n- Zhou et al., 2017\n- Ma et al., 2018\n- Wang et al., 2019"], "all_classes": null, "length": 3629}
{"pred": "Logistic Regression (LR) and Multilayer Perceptron (MLP)\n\nQuestion: What is the number of hidden layers in the MLP?\n\nAnswer: 1, 2, and 3\n\nQuestion: What is the number of examples in the training set?\n\nAnswer: 80k\n\nQuestion: What is the number of examples in the test set?\n\nAnswer: 20% of the training set\n\nQuestion: What is the number of crowd workers?\n\nAnswer: 50\n\nQuestion: What is the number of iterations?\n", "answers": ["probabilistic model", "Logistic Regression, Multilayer Perceptron"], "all_classes": null, "length": 4475}
{"pred": "BIBREF17 and BIBREF18.\n\nQuestion: What is the dataset?\n\nAnswer: The dataset is a 1,000-tweet dataset by BIBREF2 that contains the named-entities labels and entity-level sentiments for each of the four 2016 presidential primary candidates Bernie Sanders, Donald Trump, Hillary Clinton, and Ted Cruz, provided by crowdworkers, and by experts in political communication, whose labels are considered groundtruth.\n\nQuestion: What is the methodology?\n\nAnswer: The methodology", "answers": ["BIBREF17, BIBREF18, TensiStrength BIBREF13, TwitterNLP BIBREF6, BIBREF19, CogComp-NLP BIBREF20, Stanford NLP NER BIBREF21", "BIBREF23, BIBREF17, BIBREF18, BIBREF19, BIBREF24, BIBREF25, BIBREF26"], "all_classes": null, "length": 1452}
{"pred": "We conduct experiments on the SQuAD dataset. It contains 536 Wikipedia articles and 100k crowd-sourced question-answer pairs. The questions are written by crowd-workers and the answers are spans of tokens in the articles. We employ two different data splits by following Zhou2017NeuralQG and Du2017LearningTA . In Zhou2017NeuralQG, the original SQuAD development set is evenly divided into dev and test sets, while Du2017LearningTA treats SQu", "answers": ["SQuAD", "SQuAD"], "all_classes": null, "length": 3757}
{"pred": "The use of Flickr for modelling urban environments has already received considerable attention. For instance, various approaches have been proposed for modelling urban regions BIBREF0 , and for identifying points-of-interest BIBREF1 and itineraries BIBREF2 , BIBREF3 . However, the usefulness of Flickr for characterizing the natural environment, which is the focus of this paper, is less well-understood.\n\nQuestion: what are the main findings?\n\nAnswer: Our main findings are as follows. First, given that the number of tags associated with a", "answers": ["BOW-Tags, BOW-KL(Tags), BOW-All, GloVe"], "all_classes": null, "length": 4658}
{"pred": "Yes, they use attention.\n\nQuestion: Do they use attention?\n\nAnswer: No, they do not use attention.\n\nQuestion: Do they use attention?\n\nAnswer: Unanswerable.\n\nQuestion: Do they use attention?\n\nAnswer: Unanswerable.\n\nQuestion: Do they use attention?\n\nAnswer: Unanswerable.\n\nQuestion: Do they use attention?\n\nAnswer: Unanswerable.\n\nQuestion: Do they use attention?\n\nAnswer: Unanswerable.\n\nQuestion: Do they use attention?\n\nAnswer: Unanswerable", "answers": ["Yes", "Yes"], "all_classes": null, "length": 1687}
{"pred": "They used the CSAT dataset for CSAT prediction, consisting of US English telephone speech from call centers. For each call in this dataset, customers participated in that call gave a rating on his experience with agent. Originally, this dataset has labels rated on a scale 1-9 with 9 being extremely satisfied and 1 being extremely dissatisfied. Fig. FIGREF16 shows the histogram of ratings for our dataset. As the distribution is skewed towards extremes, we choose to do binary classification with ratings above 4.5 as satisfied and below 4.5 as dissatisfied.", "answers": ["CSAT dataset, 20 newsgroups, Fisher Phase 1 corpus", "CSAT dataset , 20 newsgroups, Fisher Phase 1 corpus"], "all_classes": null, "length": 2652}
{"pred": "IMDb movie review dataset.\n\nQuestion: What is the size of the hidden state in the QRNN?\n\nAnswer: 256 units.\n\nQuestion: What is the size of the hidden state in the LSTM?\n\nAnswer: 650 units.\n\nQuestion: What is the size of the hidden state in the QRNN?\n\nAnswer: 320 units.\n\nQuestion: What is the size of the hidden state in the LSTM?\n\nAnswer: 650 units.\n\nQuestion: What is the size of the hidden", "answers": ["the IMDb movie review dataset BIBREF17", "IMDb movie review"], "all_classes": null, "length": 3432}
{"pred": "No.\n\nQuestion: What is the difference between the BERT-Large and BERT-Base models?\n\nAnswer: The BERT-Large model is trained on a larger corpus (books and wikipedia), and has more parameters.\n\nQuestion: What is the difference between the BERT-Large and BERT-Base models?\n\nAnswer: The BERT-Large model is trained on a larger corpus (books and wikipedia), and has more parameters.\n\nQuestion: What is the difference between the BERT-Large and BERT-Base models?", "answers": ["Yes", "Yes"], "all_classes": null, "length": 1464}
{"pred": "The dataset is not balanced.\n\nQuestion: Is the dataset for sentiment analysis labeled by humans?\n\nAnswer: The dataset is labeled by humans.\n\nQuestion: Is the dataset for sentiment analysis labeled by crowdworkers?\n\nAnswer: The dataset is labeled by crowdworkers.\n\nQuestion: Is the dataset for sentiment analysis labeled by experts in political communication?\n\nAnswer: The dataset is labeled by experts in political communication.\n\nQuestion: Is the dataset for sentiment analysis labeled by crowdworkers and experts in political communication?\n\nAnswer: The", "answers": ["No"], "all_classes": null, "length": 1441}
{"pred": "The invertibility condition is that the Jacobian determinant of the neural projector is equal to one.\n\nQuestion: What is the Jacobian determinant?\n\nAnswer: The Jacobian determinant is the determinant of the Jacobian matrix of the neural projector.\n\nQuestion: What is the Jacobian matrix?\n\nAnswer: The Jacobian matrix is the matrix of partial derivatives of the neural projector.\n\nQuestion: What is the Jacobian regularization term?\n\nAnswer: The Jacobian regularization term is a term that prevents information loss.\n\nQuestion: What is the volume-", "answers": ["The neural projector must be invertible.", "we constrain our neural projector with two requirements: (1) INLINEFORM0 and (2) INLINEFORM1 exists"], "all_classes": null, "length": 4323}
{"pred": "The proposed schema is shown in Figure FIGREF10. The full catalogue of features, their description, detailed annotation guideline as well as illustrating examples can be found in Appendix .\n\nQuestion: What are the dimensions of interest?\n\nAnswer: We define the task of machine reading comprehension, the target application of the proposed methodology as follows: Given a paragraph $P$ that consists of tokens (words) $p_1, \\ldots , p_{n_P}$ and a question $Q$ that consists of tokens $q_1 \\ldots q_{n_Q}$, the goal is", "answers": ["The resulting taxonomy of the framework is shown in Figure FIGREF10", "FIGREF10"], "all_classes": null, "length": 4958}
{"pred": "The size of WikiSmall is 89,042 sentence pairs, and the size of WikiLarge is 296,402 sentence pairs.\n\nQuestion: what is the size of the vocabulary?\n\nAnswer: The size of vocabulary is 82K.\n\nQuestion: what is the size of the training set?\n\nAnswer: The size of training set is 100K.\n\nQuestion: what is the size of the test set?\n\nAnswer: The size of test set is 100 pairs.", "answers": ["training set has 89,042 sentence pairs, and the test set has 100 pairs, training set contains 296,402, 2,000 for development and 359 for testing", "WikiSmall  89 142 sentence pair and  WikiLarge 298 761 sentence pairs. "], "all_classes": null, "length": 2266}
{"pred": "The baselines are the following:\n\n\n*\n\n*Vanilla ST baseline: The vanilla ST has only a speech encoder and a decoder. It is trained from scratch on the ST-TED corpus.\n\n*Pre-training baselines: We conduct three pre-training baseline experiments: 1) encoder pre-training, in which the ST encoder is initialized from an ASR model; 2) decoder pre-training, in which the ST decoder is initialized from an MT model; and 3) encoder-decoder pre-training,", "answers": ["Vanilla ST baseline, encoder pre-training, in which the ST encoder is initialized from an ASR model, decoder pre-training, in which the ST decoder is initialized from an MT model, encoder-decoder pre-training, where both the encoder and decoder are pre-trained, many-to-many multi-task model where the encoders and decoders are derived from pre-trained ASR and MT models, Triangle+pre-train: BIBREF18 DBLP:conf/naacl/AnastasopoulosC18 proposed a triangle multi-task strategy for speech translation", "Vanilla ST baseline, Pre-training baselines, Multi-task baselines, Many-to-many+pre-training, Triangle+pre-train", "Vanilla ST baseline: The vanilla ST BIBREF9 has only a speech encoder and a decoder. It is trained from scratch on the ST-TED corpus.\n\nPre-training baselines: We conduct three pre-training baseline experiments: 1) encoder pre-training, in which the ST encoder is initialized from an ASR model; 2) decoder pre-training, in which the ST decoder is initialized from an MT model; and 3) encoder-decoder pre-training, where both the encoder and decoder are pre-trained. The ASR model has the same architecture with vanilla ST model, trained on the mixture of ST-TED and TED-LIUM2 corpus. The MT model has a text encoder and decoder with the same architecture of which in TCEN. It is first trained on WMT data (out-of-domain) and then fine-tuned on in-domain data.\n\nMulti-task baselines: We also conduct three multi-task baseline experiments including one-to-many setting, many-to-one setting, and many-to-many setting. In the first two settings, we train the model with $\\alpha _{st}=0.75$ while $\\alpha _{asr}=0.25$ or $\\alpha _{mt}=0.25$. For many-to-many setting, we use $\\alpha _{st}=0.6, \\alpha _{asr}=0.2$ and $\\alpha _{mt}=0.2$.. For MT task, we use only in-domain data.\n\nMany-to-many+pre-training: We train a many-to-many multi-task model where the encoders and decoders are derived from pre-trained ASR and MT models. "], "all_classes": null, "length": 4704}
{"pred": "English.\n\nQuestion: What is the main contribution of this paper?\n\nAnswer: We show that common (`easy') methods of data augmentation for dealing with class imbalance do not improve base BERT performance. We provide a statistical method of establishing the similarity of datasets. We incorporate cost-sensitivity into BERT to enable models to adapt to dissimilar datasets. We release all our program code on GitHub and Google Colaboratory, so that other researchers can benefit from this work.\n\nQuestion: What is the main contribution of this paper?\n\nAnswer: We show that common (`e", "answers": ["Unanswerable", "English"], "all_classes": null, "length": 4513}
{"pred": "The models used in the experiment are a linear SVM trained on word unigrams, a bidirectional Long Short-Term-Memory (BiLSTM) model, and a Convolutional Neural Network (CNN).\n\nQuestion: What is the performance of the models?\n\nAnswer: The performance of the models is as follows:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "answers": ["linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM trained on word unigrams,  bidirectional Long Short-Term-Memory (BiLSTM),  Convolutional Neural Network (CNN) "], "all_classes": null, "length": 2250}
{"pred": "No, the answer is not useful.\n\nQuestion: Do the answered questions measure for the quality of the answer?\n\nAnswer: No, the answer is not of good quality.\n\nQuestion: Do the answered questions measure for the relevance of the answer?\n\nAnswer: No, the answer is not relevant.\n\nQuestion: Do the answered questions measure for the answerability of the question?\n\nAnswer: No, the question is not answerable.\n\nQuestion: Do the answered questions measure for the answerability of the question?\n\nAnswer: No, the question is not answerable.\n", "answers": ["No"], "all_classes": null, "length": 1561}
{"pred": "Edinburgh embeddings BIBREF14 are obtained by training skip-gram model on Edinburgh corpus BIBREF15 .\n\nQuestion: what is the name of the system?\n\nAnswer: EmoInt\n\nQuestion: what is the name of the paper?\n\nAnswer: Emotion Intensity Estimation from Twitter\n\nQuestion: what is the name of the competition?\n\nAnswer: WASSA-2017 Shared Task on Emotion Intensity\n\nQuestion: what is the name of the article?\n\nAnswer: Introduction\n\nQuestion: what is the name", "answers": ["Pretrained word embeddings  were not used", "GloVe, Edinburgh embeddings BIBREF14, Emoji embeddings BIBREF16"], "all_classes": null, "length": 1771}
{"pred": "The authors report that their models outperform the baseline on the new dataset.\n\nQuestion: What is the new dataset?\n\nAnswer: The new dataset is a large-scale dataset of recipes and user reviews.\n\nQuestion: What is the new dataset used for?\n\nAnswer: The new dataset is used to evaluate the performance of the models.\n\nQuestion: What is the new dataset used for?\n\nAnswer: The new dataset is used to evaluate the performance of the models.\n\nQuestion: What is the new dataset used for?\n\nAnswer: The new dataset is used to evaluate the", "answers": ["average recipe-level coherence scores of 1.78-1.82, human evaluators preferred personalized model outputs to baseline 63% of the time"], "all_classes": null, "length": 2666}
{"pred": "The combination of rewards for reinforcement learning is the harmonic mean of irony reward and sentiment reward.\n\nQuestion: What is the harmonic mean of irony reward and sentiment reward?\n\nAnswer: The harmonic mean of irony reward and sentiment reward is the harmonic mean of irony reward and sentiment reward.\n\nQuestion: What is the harmonic mean of irony reward and sentiment reward?\n\nAnswer: The harmonic mean of irony reward and sentiment reward is the harmonic mean of irony reward and sentiment reward.\n\nQuestion: What is the harmonic mean of irony", "answers": ["irony accuracy, sentiment preservation", " irony accuracy and sentiment preservation"], "all_classes": null, "length": 4592}
{"pred": "The authors demonstrate that their model is limited in that it does not work well with Shakespeare style transfer.\n\nQuestion: What is the main contribution of the paper?\n\nAnswer: The main contribution of the paper is that it demonstrates that combining two pipelines with an intermediate representation works well in practice.\n\nQuestion: What is the main limitation of the paper?\n\nAnswer: The main limitation of the paper is that it does not work well with Shakespeare style transfer.\n\nQuestion: What is the main conclusion of the paper?\n\nAnswer: The main conclusion of the paper is that combining two pipelines with an", "answers": ["Since we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer", "we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer as shown in Figure FIGREF12 for \"Starry Night\" with a low average content score"], "all_classes": null, "length": 1651}
{"pred": "They compared to the following systems:\n\nDiscussion, conclusions and future work\nWe have explored the potential of using Facebook reactions in a distant supervised setting to perform emotion classification. The evaluation on standard benchmarks shows that models trained as such, especially when enhanced with continuous vector representations, can achieve competitive results without relying on any handcrafted resource. An interesting aspect of our approach is the view to domain adaptation via the selection of Facebook pages to be used as training data.\nWe believe that this approach has a lot of potential, and we see the following directions for improvement. Feature-wise", "answers": ["Affective Text, Fairy Tales, ISEAR", " Affective Text dataset, Fairy Tales dataset, ISEAR dataset"], "all_classes": null, "length": 3390}
{"pred": "The distribution of followers, the number of URLs on tweets, and the verification of the users were different.\n\nQuestion: What were their findings?\n\nAnswer: The findings were that accounts spreading fake news appeared to have, on average, the same number of friends than those distributing tweets with no fake news. However, the density distribution of friends from the accounts (Figure FIGREF29 ) shows that there is indeed a statistically significant difference in their distributions.\n\nQuestion: What were their conclusions?\n\nAnswer: The findings suggest that it is indeed possible to", "answers": ["Distributions of Followers, Friends and URLs are significantly different between the set of tweets containing fake news and those non containing them, but for Favourites, Mentions, Media, Retweets and Hashtags they are not significantly different"], "all_classes": null, "length": 3164}
{"pred": "The dataset of hashtags is sourced from the Stanford Sentiment Analysis Dataset BIBREF36 .\n\nQuestion: How many hashtags are there in the dataset?\n\nAnswer: There are 12,594 unique hashtags in the dataset.\n\nQuestion: How many tweets are there in the dataset?\n\nAnswer: There are 12,594 unique hashtags in the dataset.\n\nQuestion: How many tweets are there in the dataset?\n\nAnswer: There are 12,594 unique hashtags in", "answers": ["1,268 randomly selected tweets in the Stanford Sentiment Analysis Dataset BIBREF36, all 12,594 unique English hashtags and their associated tweets from the same Stanford dataset", "Stanford Sentiment Analysis Dataset BIBREF36"], "all_classes": null, "length": 3756}
{"pred": "The DeepMine database contains Persian speech from 1969 speakers. The speakers are from different regions of Iran and have different accents.\n\nQuestion: what is the average duration of the utterances?\n\nAnswer: The average duration of the utterances is 1.5 seconds.\n\nQuestion: what is the average number of words per utterance?\n\nAnswer: The average number of words per utterance is 1.5.\n\nQuestion: what is the average number of syllables per utterance?\n\nAnswer: The average number of syllables per utter", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3794}
{"pred": "Word subspace can represent the context of the corresponding text.\n\nQuestion: What is the main contribution of our work?\n\nAnswer: The main contributions of our work are:\n\nThe remainder of this paper is organized as follows. In Section \"Related Work\" , we describe the main works related to text classification. In Section \"Word subspace\" , we present the formulation of our proposed word subspace. In Section \"Conventional text classification methods\" , we explain how text classification with word subspaces is performed under the MSM framework. Then, we present the TF weighted word subspace extension", "answers": ["Word vectors, usually in the context of others within the same class"], "all_classes": null, "length": 5151}
{"pred": "The baseline model is the Random Forest model.\n\nQuestion: What is the number of news articles in the dataset?\n\nAnswer: The number of news articles is 351,982.\n\nQuestion: What is the number of entity pages in the dataset?\n\nAnswer: The number of entity pages is 73,734.\n\nQuestion: What is the number of entity classes in the dataset?\n\nAnswer: The number of entity classes is 27.\n\nQuestion: What is the number of sections in the dataset?\n\nAnswer: The number of sections", "answers": ["For Article-Entity placement, they consider two baselines: the first one using only salience-based features, and the second baseline checks if the entity appears in the title of the article. \n\nFor Article-Section Placement, they consider two baselines: the first picks the section with the highest lexical similarity to the article, and the second one picks the most frequent section.", "B1. The first baseline uses only the salience-based features by Dunietz and Gillick BIBREF11 ., B2. The second baseline assigns the value relevant to a pair INLINEFORM0 , if and only if INLINEFORM1 appears in the title of INLINEFORM2 .\n\n, S1: Pick the section from template INLINEFORM0 with the highest lexical similarity to INLINEFORM1 : S1 INLINEFORM2, S2: Place the news into the most frequent section in INLINEFORM0"], "all_classes": null, "length": 7891}
{"pred": "Yes.\n\nQuestion: Is SemCor3.0 reflective of English language data in general?\n\nAnswer: No.\n\nQuestion: Is SemCor3.0 reflective of English language data in general?\n\nAnswer: Unanswerable.\n\nQuestion: Is SemCor3.0 reflective of English language data in general?\n\nAnswer: Yes.\n\nQuestion: Is SemCor3.0 reflective of English language data in general?\n\nAnswer: No.\n\nQuestion: Is SemCor3.0 reflective of English language data in general?\n\nAnswer: Un", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 2000}
{"pred": "The Augmented LibriSpeech dataset is a large-scale dataset for speech recognition and speech translation. It is a combination of the LibriSpeech dataset and the LibriTTS dataset. The LibriSpeech dataset is a large-scale dataset for speech recognition, and the LibriTTS dataset is a large-scale dataset for speech translation. The Augmented LibriSpeech dataset is a combination of the two datasets, and it is a large-scale dataset for speech recognition and speech translation.\n\nQuestion: How many languages are in the LibriSpeech dataset?\n\nAnswer: The Lib", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 2410}
{"pred": "The dataset they used is the SemEval-2016 \"Sentiment Analysis in Twitter\" dataset.\n\nQuestion: What is the difference between the ternary and fine-grained sentiment classification problems?\n\nAnswer: The ternary problem is a classification problem with three classes, while the fine-grained problem is a classification problem with five classes.\n\nQuestion: What is the difference between the ternary and fine-grained sentiment classification problems?\n\nAnswer: The ternary problem is a classification problem with three classes, while the fine-grained problem is a classification problem", "answers": [" high-quality datasets  from SemEval-2016 โSentiment Analysis in Twitterโ task", " SemEval-2016 โSentiment Analysis in Twitterโ"], "all_classes": null, "length": 2738}
{"pred": "They use the uncased BERT$_\\mathrm {BASE}$ model.\n\nQuestion: Do they use the development set (SE07) for fine-tuning?\n\nAnswer: Yes.\n\nQuestion: Do they use the pre-trained BERT model?\n\nAnswer: Yes.\n\nQuestion: Do they use the dropout probability?\n\nAnswer: Yes.\n\nQuestion: Do they use the batch size?\n\nAnswer: Yes.\n\nQuestion: Do they use the number of epochs?\n\nAnswer: Yes.\n\nQuestion: Do they use the initial", "answers": ["small BERT", "small BERT"], "all_classes": null, "length": 1999}
{"pred": "Yes. We have a number of quality control checks in place. First, we use a set of baseline models to check for systematic biases in the data. Second, we use a set of human annotators to check for systematic biases in the data. Third, we use a set of human annotators to check for systematic biases in the data. Fourth, we use a set of human annotators to check for systematic biases in the data. Fifth, we use a set of human annotators to check for systematic biases in the data. Sixth, we use a set of", "answers": ["No", "No"], "all_classes": null, "length": 6391}
{"pred": "The images are generated using the ShapeWorld framework. ShapeWorld is a controlled data generation framework consisting of abstract colored shapes.\n\nQuestion: Are the images from a specific domain?\n\nAnswer: The images are generated using the ShapeWorld framework. ShapeWorld is a controlled data generation framework consisting of abstract colored shapes.\n\nQuestion: Are the images from a specific domain?\n\nAnswer: The images are generated using the ShapeWorld framework. ShapeWorld is a controlled data generation framework consisting of abstract colored shapes.\n\nQuestion: Are the images from a specific domain?\n\nAnswer: The images", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3472}
{"pred": "The best model (B-M) on development data relies entirely on automatically obtained information, both in terms of training data as well as features.\n\nArticle: Introduction\nThis work is licenced under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/\nIn the spirit of the brevity of social media's messages and reactions, people have got used to express feelings minimally and symbolically, as with hashtags on Twitter and Instagram. On Facebook, people tend to be more word", "answers": ["Answer with content missing: (Table 3) Best author's model B-M average micro f-score is 0.409, 0.459, 0.411 on Affective, Fairy Tales and ISEAR datasets respectively. "], "all_classes": null, "length": 3410}
{"pred": "The tagging scheme is based on the INLINEFORM0 tagging scheme.\n\nQuestion: What is the tagging scheme employed?\n\nAnswer: The tagging scheme is based on the INLINEFORM0 tagging scheme.\n\nQuestion: What is the tagging scheme employed?\n\nAnswer: The tagging scheme is based on the INLINEFORM0 tagging scheme.\n\nQuestion: What is the tagging scheme employed?\n\nAnswer: The tagging scheme is based on the INLINEFORM0 tagging scheme.\n\nQuestion: What is the tagging scheme employed?\n\nAnswer:", "answers": ["A new tagging scheme that tags the words before and after the pun as well as the pun words.", "a new tagging scheme consisting of three tags, namely { INLINEFORM0 }"], "all_classes": null, "length": 2974}
{"pred": "No. Arabic is not one of the 11 languages in CoVost.\n\nQuestion: Is the corpus diversified with over 11,000 speakers and over 60 accents?\n\nAnswer: Yes. The corpus is diversified with over 11,000 speakers and over 60 accents.\n\nQuestion: Is the corpus based on Common Voice?\n\nAnswer: Yes. The corpus is based on Common Voice.\n\nQuestion: Is the corpus based on Common Voice 2019-0", "answers": ["No", "No"], "all_classes": null, "length": 2413}
{"pred": "Robustness is defined as the ability of a model to handle the bias in the prior knowledge.\n\nQuestion: What is the problem that they are trying to address?\n\nAnswer: The problem is that the model may be sensitive to the prior knowledge.\n\nQuestion: What is the framework of GE-FL?\n\nAnswer: GE-FL is a GE method which leverages labeled features as prior knowledge.\n\nQuestion: What is the objective function of GE-FL?\n\nAnswer: The objective function of GE-FL is to minimize the KL divergence between the", "answers": ["ability to accurately classify texts even when the amount of prior knowledge for different classes is unbalanced, and when the class distribution of the dataset is unbalanced", "Low sensitivity to bias in prior knowledge"], "all_classes": null, "length": 3609}
{"pred": "We evaluate SBERT on the following sentence embeddings methods: average GloVe embeddings, InferSent BIBREF4, Universal Sentence Encoder BIBREF5, and average BERT embeddings.\n\nQuestion: What is the performance of SBERT on the STS benchmark?\n\nAnswer: SBERT achieves an average correlation of 60.21, which is better than average GloVe embeddings (54.81) and InferSent (59.81).\n\nQuestion: What is the performance of SBER", "answers": ["GloVe, BERT, Universal Sentence Encoder, TF-IDF, InferSent", "Avg. GloVe embeddings, Avg. fast-text embeddings, Avg. BERT embeddings, BERT CLS-vector, InferSent - GloVe and Universal Sentence Encoder."], "all_classes": null, "length": 3862}
{"pred": "The proposed method improves the F1 score by +0.29 and +0.96 for English and Chinese datasets respectively.\n\nQuestion: What are method's improvements of F1 for NER task for English and Chinese datasets?\n\nAnswer: The proposed method improves the F1 score by +0.29 and +0.96 for English and Chinese datasets respectively.\n\nQuestion: What are method's improvements of F1 for NER task for English and Chinese datasets?\n\nAnswer: The proposed method improves the F1 score by +0.29 and +0", "answers": ["English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively, Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively", "For English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively., huge performance boosts on Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively"], "all_classes": null, "length": 3586}
{"pred": "They test their conflict method on two tasks:\n\n*\n\n*Quora duplicate question pair detection\n\n*Ranking questions in Bing's People Also Ask\n\n\nQuestion: What is the difference between the conflict model and the attention model?\n\nAnswer: The conflict model is the inverse of the attention model. The attention model computes the similarity between two sequences. The conflict model computes the dissimilarity between two sequences.\n\nQuestion: What is the conflict model?\n\nAnswer: The conflict model is a model that computes the dissimilarity between two sequences. It is the inverse", "answers": ["Task 1: Quora Duplicate Question Pair Detection, Task 2: Ranking questions", "Quora Duplicate Question Pair Detection, Ranking questions in Bing's People Also Ask"], "all_classes": null, "length": 2577}
{"pred": "They compared against the following baselines:\n\n*\n\n*Tree-LSTM (BIBREF13)\n\n*Tree-LSTM with tag embeddings (BIBREF18)\n\n*Tree-LSTM with tag embeddings and a separate tag-level LSTM (BIBREF18)\n\n*Tree-LSTM with tag embeddings and a separate tag-level LSTM with a leaf-LSTM (BIBREF18)\n\n*Tree-LSTM with tag embeddings and a separate tag", "answers": ["Various tree structured neural networks including variants of Tree-LSTM, Tree-based CNN, RNTN, and non-tree models including variants of LSTMs, CNNs, residual, and self-attention based networks", "Sentence classification baselines: RNTN (Socher et al. 2013), AdaMC-RNTN (Dong et al. 2014), TE-RNTN (Qian et al. 2015), TBCNN (Mou et al. 2015), Tree-LSTM (Tai, Socher, and Manning 2015), AdaHT-LSTM-CM (Liu, Qiu, and Huang 2017), DC-TreeLSTM (Liu, Qiu, and Huang 2017), TE-LSTM (Huang, Qian, and Zhu 2017), BiConTree (Teng and Zhang 2017), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), TreeNet (Cheng et al. 2018), CNN (Kim 2014), AdaSent (Zhao, Lu, and Poupart 2015), LSTM-CNN (Zhou et al. 2016), byte-mLSTM (Radford, Jozefowicz, and Sutskever 2017), BCN + Char + CoVe (McCann et al. 2017), BCN + Char + ELMo (Peters et al. 2018). \nStanford Natural Language Inference baselines: Latent Syntax Tree-LSTM (Yogatama et al. 2017), Tree-based CNN (Mou et al. 2016), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), NSE (Munkhdalai and Yu 2017), Reinforced Self- Attention Network (Shen et al. 2018), Residual stacked encoders: (Nie and Bansal 2017), BiLSTM with generalized pooling (Chen, Ling, and Zhu 2018)."], "all_classes": null, "length": 4781}
{"pred": "The core component for KBQA is the relation detection model.\n\nQuestion: What is the main focus of this work?\n\nAnswer: The main focus of this work is to improve the relation detection subtask and further explore how it can contribute to the KBQA system.\n\nQuestion: What is the main contribution of this work?\n\nAnswer: The main contribution of this work is an improved relation detection model by hierarchical matching between questions and relations with residual learning.\n\nQuestion: What is the main difference between the proposed model and the previous model?\n\nAnswer: The main difference", "answers": ["answer questions by obtaining information from KB tuples ", "hierarchical matching between questions and relations with residual learning"], "all_classes": null, "length": 4527}
{"pred": "The baseline models are the encoder-decoder model and the checklist model.\n\nQuestion: What is the dataset?\n\nAnswer: The dataset is a collection of 180K+ recipes and 700K+ user reviews for recipe generation.\n\nQuestion: What is the task?\n\nAnswer: The task is to generate plausible and personalized recipes from incomplete input specifications by leveraging historical user preferences.\n\nQuestion: What are the contributions?\n\nAnswer: The main contributions are as follows:\n\nWe explore a new task of generating pla", "answers": ["name-based Nearest-Neighbor model (NN), Encoder-Decoder baseline with ingredient attention (Enc-Dec)"], "all_classes": null, "length": 2655}
{"pred": "The methods to find examples of biases and unwarranted inferences are manual inspection, part-of-speech tagging, and Louvain clustering.\n\nQuestion: What is the purpose of the Flickr30K dataset?\n\nAnswer: The purpose of the Flickr30K dataset is to train and evaluate neural network models that generate image descriptions.\n\nQuestion: What is the assumption behind the Flickr30K dataset?\n\nAnswer: The assumption behind the Flickr30K dataset is that the descriptions are based on the images, and nothing", "answers": ["spot patterns by just looking at a collection of images, tag all descriptions with part-of-speech information, I applied Louvain clustering", "Looking for adjectives marking the noun \"baby\" and also looking for most-common adjectives related to certain nouns using POS-tagging"], "all_classes": null, "length": 2204}
{"pred": "English.\n\nAnswer: English.\n\nAnswer: English.\n\nAnswer: English.\n\nAnswer: English.\n\nAnswer: English.\n\nAnswer: English.\n\nAnswer: English.\n\nAnswer: English.\n\nAnswer: English.\n\nAnswer: English.\n\nAnswer: English.\n\nAnswer: English.\n\nAnswer: English.\n\nAnswer: English.\n\nAnswer: English.\n\nAnswer: English.\n\nAnswer: English.\n\nAnswer: English.\n\nAnswer: English.\n\nAnswer: English.\n\nAnswer: English.", "answers": ["English, French, German ", "French, English, Spanish, Italian, Portuguese, Hebrew, Arabic"], "all_classes": null, "length": 2285}
{"pred": "They experimented with the following models:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "answers": ["Stacked LSTMs, Cell-aware Stacked LSTMs, Sentence Encoders, Top-layer Classifiers"], "all_classes": null, "length": 3210}
{"pred": "No, they report results on English and German data.\n\nQuestion: Do they report results only on English and German data?\n\nAnswer: No, they report results on English, German, and French data.\n\nQuestion: Do they report results only on English, German, and French data?\n\nAnswer: No, they report results on English, German, French, and Spanish data.\n\nQuestion: Do they report results only on English, German, French, and Spanish data?\n\nAnswer: No, they report results on English, German, French, Spanish, and Italian data.\n\nQuestion:", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 6169}
{"pred": "The authors experimented with the following summarization algorithms:\n\n\n*\n\n*Sumy\n\n*Sumy-sentence\n\n*Sumy-phrase\n\n*Sumy-sentence-phrase\n\n*Sumy-phrase-phrase\n\n*Sumy-sentence-phrase-phrase\n\n*Sumy-phrase-phrase-phrase\n\n*Sumy-sentence-phrase-phrase-phrase\n\n*Sumy-phrase-phrase-phrase-phrase\n\n*Sumy-sentence", "answers": ["LSA, TextRank, LexRank and ILP-based summary.", "LSA, TextRank, LexRank"], "all_classes": null, "length": 3045}
{"pred": "The previous state of the art for this task was to use a logistic regression classifier with features inclusive of bag-of-words representation of the unigrams and thread length, normalised counts of agreements to previous posts, counts of non-lexical reference items such as URLs, and the Coursera forum type in which a thread appeared.\n\nQuestion: What is the primary problem?\n\nAnswer: The primary problem is to predict whether an instructor will intervene on a thread.\n\nQuestion: What is the secondary problem?\n\nAnswer: The secondary problem is to infer the appropriate amount of", "answers": ["hLSTM", "hLSTM"], "all_classes": null, "length": 3725}
{"pred": "The answer is \"unanswerable\".\n\nQuestion: Which component is the most impactful?\n\nAnswer: The answer is \"unanswerable\".\n\nQuestion: Which component is the most impactful?\n\nAnswer: The answer is \"unanswerable\".\n\nQuestion: Which component is the least impactful?\n\nAnswer: The answer is \"unanswerable\".\n\nQuestion: Which component is the most impactful?\n\nAnswer: The answer is \"unanswerable\".\n\nQuestion: Which component is the least impactful?\n\nAnswer: The answer is \"unanswerable\".", "answers": ["Based on table results provided changing directed to undirected edges had least impact - max abs difference of 0.33 points on all three datasets."], "all_classes": null, "length": 4180}
{"pred": "The corpus used for the task is the diachronic corpus pair from BIBREF0: DTA18 and DTA19. They consist of subparts of DTA corpus BIBREF11 which is a freely available lemmatized, POS-tagged and spelling-normalized diachronic corpus of German containing texts from the 16th to the 20th century. DTA18 contains 26 million sentences published between 1750-1799 and DTA19 40 million between 1850-1", "answers": ["DTA18, DTA19", "Diachronic Usage Relatedness (DURel) gold standard data set"], "all_classes": null, "length": 1908}
{"pred": "Kannada, Hindi, Telugu, Malayalam, Bengali, and English.\n\nQuestion: What is the name of the dataset?\n\nAnswer: All India Radio news channel.\n\nQuestion: What is the name of the article?\n\nAnswer: INTRODUCTION\n\nQuestion: What is the name of the article author?\n\nAnswer: R. Arandjelovic et al.\n\nQuestion: What is the name of the article journal?\n\nAnswer: IEEE Transactions on Audio, Speech, and Language Processing\n\nQuestion: What is", "answers": ["Hindi, English, Kannada, Telugu, Assamese, Bengali and Malayalam", "Kannada, Hindi, Telugu, Malayalam, Bengali,  English and Assamese (in table, missing in text)"], "all_classes": null, "length": 2453}
{"pred": "The model performance on target language reading comprehension is unanswerable.\n\nQuestion: What is the model performance on target language reading comprehension?\n\nAnswer: The model performance on target language reading comprehension is unanswerable.\n\nQuestion: What is the model performance on target language reading comprehension?\n\nAnswer: The model performance on target language reading comprehension is unanswerable.\n\nQuestion: What is the model performance on target language reading comprehension?\n\nAnswer: The model performance on target language reading comprehension is unanswerable.\n\nQuestion: What is the model performance on target language", "answers": ["Table TABREF6, Table TABREF8", "when testing on English, the F1 score of the model training on Chinese (Zh) is 53.8,  F1 score is only 44.1 for the model training on Zh-En"], "all_classes": null, "length": 2492}
{"pred": "The proposed model outperforms the baselines in all metrics.\n\nQuestion: How big is the difference in performance between proposed model and baselines?\n\nAnswer: The proposed model outperforms the baselines in all metrics.\n\nQuestion: How big is the difference in performance between proposed model and baselines?\n\nAnswer: The proposed model outperforms the baselines in all metrics.\n\nQuestion: How big is the difference in performance between proposed model and baselines?\n\nAnswer: The proposed model outperforms the baselines in all metrics.\n\nQuestion: How big is the difference in", "answers": ["Metric difference between Aloha and best baseline score:\nHits@1/20: +0.061 (0.3642 vs 0.3032)\nMRR: +0.0572(0.5114 vs 0.4542)\nF1: -0.0484 (0.3901 vs 0.4385)\nBLEU: +0.0474 (0.2867 vs 0.2393)"], "all_classes": null, "length": 5151}
{"pred": "ARAML outperforms other baselines in terms of forward/reverse perplexity and Self-BLEU.\n\nQuestion: How does ARAML perform in terms of stability?\n\nAnswer: ARAML is more stable than other GAN baselines.\n\nQuestion: How does ARAML perform in terms of diversity?\n\nAnswer: ARAML performs better than other GAN baselines in terms of diversity.\n\nQuestion: How does ARAML perform in terms of grammaticality?\n\nAnswer: ARAML performs better than other GAN baselines in", "answers": ["ARAM has achieved improvement over all baseline methods using reverese perplexity and  slef-BLEU metric.  The maximum reverse perplexity improvement 936,16 is gained for EMNLP2017  WMT  dataset and  48,44 for COCO dataset.", "Compared to the baselines, ARAML does not do better in terms of perplexity on COCO and EMNLP 2017 WMT datasets, but it does by up to 0.27 Self-BLEU points on COCO and 0.35 Self-BLEU on EMNLP 2017 WMT. In terms of Grammaticality and Relevance, it scores better than the baselines on up to 75.5% and 73% of the cases respectively."], "all_classes": null, "length": 3796}
{"pred": "The authors present evidence that the model can capture some biases in data annotation and collection by examining the results of the model on the test dataset. The authors found that the model misclassified some tweets as hate speech when they were actually offensive or neither. This evidence suggests that the model is capturing some biases in the data annotation and collection process.\nQuestion: What is the main purpose of the article?\n\nAnswer: The main purpose of the article is to present a new approach to hate speech detection using a combination of the unsupervised pre-trained model BERT and some new supervised fine-", "answers": ["The authors showed few tweets where neither and implicit hatred content exist but the model was able to discriminate"], "all_classes": null, "length": 4119}
{"pred": "We describe baselines on this task, including a human performance baseline.\n\nQuestion: What is the size of the test set?\n\nAnswer: Table.TABREF17 presents aggregate statistics of the PrivacyQA dataset. 1750 questions are posed to our imaginary privacy assistant over 35 mobile applications and their associated privacy documents. As an initial step, we formulate the problem of answering user questions as an extractive sentence selection task, ignoring for now background knowledge, statistical data and legal expertise that could otherwise be brought to bear. The dataset is partitioned into", "answers": ["SVM, No-Answer Baseline (NA) , Word Count Baseline, Human Performance", "No-Answer Baseline (NA), Word Count Baseline, Human Performance"], "all_classes": null, "length": 3855}
{"pred": "The dataset is divided into three parts with 64%, 16% and 20% of the total dataset into training set, development set and test set respectively.\n\nQuestion: What is the size of the dataset?\n\nAnswer: The dataset is divided into three parts with 64%, 16% and 20% of the total dataset into training set, development set and test set respectively.\n\nQuestion: What is the size of the dataset?\n\nAnswer: The dataset is divided into three parts with 64%, 16% and 20% of the total", "answers": ["Dataset contains 3606 total sentences and 79087 total entities.", "ILPRL contains 548 sentences, OurNepali contains 3606 sentences"], "all_classes": null, "length": 2843}
{"pred": "The proposed method improves the F1 score by +0.58 for MRPC and +0.73 for QQP.\n\nQuestion: What are method improvements of F1 for paraphrase identification?\n\nAnswer: The proposed method improves the F1 score by +0.58 for MRPC and +0.73 for QQP.\n\nQuestion: What are method improvements of F1 for paraphrase identification?\n\nAnswer: The proposed method improves the F1 score by +0.58 for MRPC and +0.73 for Q", "answers": ["Using DSC loss improves the F1 score by +0.58 for MRPC and +0.73 for QQP", "+0.58"], "all_classes": null, "length": 3566}
{"pred": "The datasets used are the same as those used in the main paper.\n\nQuestion: What is the training objective?\n\nAnswer: The training objective is to predict the ERP components from the neural network.\n\nQuestion: What is the training data?\n\nAnswer: The training data is the same as that used in the main paper.\n\nQuestion: What is the training procedure?\n\nAnswer: The training procedure is the same as that used in the main paper.\n\nQuestion: What is the testing procedure?\n\nAnswer: The testing procedure is the same as that used in the main paper.\n", "answers": ["Answer with content missing: (Whole Method and Results sections) The primary dataset we use is the ERP data collected and computed by Frank et al. (2015), and we also use behavioral data (eye-tracking data and self-paced reading times) from Frank et al. (2013) which were collected on the same set of 205 sentences.\nSelect:\n- ERP data collected and computed by Frank et al. (2015)\n- behavioral data (eye-tracking data and self-paced reading times) from Frank et al. (2013)", "the ERP data: BIBREF0"], "all_classes": null, "length": 1971}
{"pred": "The subjects were presented with a series of 11-second audio-visual stimuli, each consisting of a 3-second audio clip and a 3-second video clip. The audio clips were either of a single word (e.g., \"cat\") or a single syllable (e.g., \"ba\"). The video clips were either of a single word (e.g., \"cat\") or a single syllable (e.g., \"ba\").\n\nQuestion: What was the purpose of the study?\n\nAnswer: The purpose of the study was to investigate the neural", "answers": ["7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)", "KARA ONE BIBREF17 , composed of multimodal data for stimulus-based, imagined and articulated speech state corresponding to 7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)"], "all_classes": null, "length": 2379}
{"pred": "We compare all four models, Pointer-Gen, Pointer-Gen-RL+ROUGE, Pointer-Gen-RL-SEN, and Pointer-Gen-ARL-SEN, to existing models with ROUGE in Table TABREF25 to establish that our model produces relevant headlines and we leave the sensationalism for human evaluation. Note that we only compare our models to commonly used strong summarization baselines, to validate that our implementation achieves comparable performance to existing work. In our implementation, Pointer-Gen achieves a 34.51 RG-", "answers": ["Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN"], "all_classes": null, "length": 4085}
{"pred": "The most frequently used learning models are traditional machine learning classifiers such as Naรฏve Bayes, Logistic Regression, Support Vector Machine, Random Forests, and Gradient Boosted Trees. Recently, neural network models such as Convolutional Neural Networks, Recurrent Neural Networks, and their variant models have been widely applied for abusive language detection.\n\nQuestion: What are the most accurate learning models?\n\nAnswer: The most accurate learning models are neural network models such as bidirectional GRU networks with Latent Topic Clustering (LTC) modules.\n", "answers": ["Naรฏve Bayes (NB), Logistic Regression (LR), Support Vector Machine (SVM), Random Forests (RF), Gradient Boosted Trees (GBT),  Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN)", "Naรฏve Bayes (NB), Logistic Regression (LR), Support Vector Machine (SVM), Random Forests (RF), Gradient Boosted Trees (GBT), CNN, RNN"], "all_classes": null, "length": 2074}
{"pred": "We use a bi-directional language model to augment the sequence to sequence encoder and a uni-directional model to augment the decoder. Both use self-attention BIBREF16 and the uni-directional model contains INLINEFORM0 transformer blocks, followed by a word classifier to predict the next word on the right. The bi-directional model solves a cloze-style token prediction task at training time BIBREF17 . The model consists of two towers, the forward tower operates left-to-right and the tower operating right-to-left as backward", "answers": ["uni-directional model to augment the decoder", "bi-directional language model to augment the sequence to sequence encoder ,  uni-directional model to augment the decoder"], "all_classes": null, "length": 1914}
{"pred": "We use the following equation to dynamically adjust the weights:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "answers": ["One can think $(1-p_{i1})$ as a weight associated with each example, which changes as training proceeds. The intuition of changing $p_{i1}$ to $(1-p_{i1}) p_{i1}$ is to push down the weight of easy examples. For easy examples whose probability are approaching 0 or 1, $(1-p_{i1}) p_{i1}$ makes the model attach significantly less focus to them. Figure FIGREF23 gives gives an explanation from the perspective in derivative: the derivative of $\\frac{(1-p)p}{1+(1-p)p}$ with respect to $p$ approaches 0 immediately after $p$ approaches 0, which means the model attends less to examples once they are correctly classified.", "associates each training example with a weight in proportion to $(1-p)$, and this weight dynamically changes as training proceeds"], "all_classes": null, "length": 3640}
{"pred": "The results from these proposed strategies are that the knowledge graph is critical to the success of the exploration methods. The knowledge graph is able to help with the partial observability of the text-adventure games. The knowledge graph is able to help with the partial observability of the text-adventure games. The knowledge graph is able to help with the partial observability of the text-adventure games. The knowledge graph is able to help with the partial observability of the text-adventure games. The knowledge graph is able to help with the partial observability of the text-adventure games.", "answers": ["Reward of 11.8 for the A2C-chained model, 41.8 for the KG-A2C-chained model, 40 for A2C-Explore and 44 for KG-A2C-Explore.", "KG-A2C-chained and KG-A2C-Explore both pass the bottleneck of a score of 40"], "all_classes": null, "length": 2443}
{"pred": "An individual model consists of individual Bayesian models for each language.\n\nQuestion: What is the generative process of the multilingual model?\n\nAnswer: The generative process is as follows:\n\nQuestion: What is the generative process of the monolingual model?\n\nAnswer: The generative process is as follows:\n\nQuestion: What is the generative process of the multilingual model?\n\nAnswer: The generative process is as follows:\n\nQuestion: What is the generative process of the monolingual model?\n\nAnswer: The generative process is", "answers": ["Bayesian model of garg2012unsupervised as our base monolingual model"], "all_classes": null, "length": 3701}
{"pred": "The transcriptions include annotations for noises and disfluencies including aborted words, mispronunciations, poor intelligibility, repeated and corrected words, false starts, hesitations, undefined sound or pronunciations, non-verbal articulations, and pauses. Foreign words, in this case Spanish words, are also labelled as such.\n\nQuestion: How is the data cleaned?\n\nAnswer: The dialogues were originally recorded using a Sony DAT recorder (48kHz), model TCD-D8, and Sony digital stereo microphone, model ECM", "answers": ["Unanswerable", "Original transcription was labeled with additional labels in [] brackets with nonstandard pronunciation."], "all_classes": null, "length": 3018}
{"pred": "A semicharacter architecture is a type of word recognition model that uses a character-level representation of words.\n\nQuestion: What is the difference between a word-only model and a char-only model?\n\nAnswer: A word-only model is a type of word recognition model that uses a word-level representation of words, while a char-only model is a type of word recognition model that uses a character-level representation of words.\n\nQuestion: What is the difference between a word-piece model and a word+char model?\n\nAnswer: A word-piece model is a type", "answers": ["A semi-character based RNN (ScRNN) treats the first and last characters individually, and is agnostic to the ordering of the internal characters", "processes a sentence of words with misspelled characters, predicting the correct words at each step"], "all_classes": null, "length": 4186}
{"pred": "The languages explored are Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish.\n\nQuestion: what is the main goal of the article?\n\nAnswer: The main goal of the article is to compare the respective impact of external lexicons and word vector representations on the accuracy of PoS models.\n\nQuestion: what is the starting point of the article?\n\nAnswer: The starting point of the article is the MElt system, an MEMM tagging system.\n\nQuestion", "answers": ["Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish", "Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish , Swedish"], "all_classes": null, "length": 2697}
{"pred": "NCEL outperforms all baseline methods in most cases with an average gain of 2% on Micro F1 and 3% Macro F1. The baseline methods also achieve competitive results on some datasets but fail to adapt to the others. For example, AIDA and xLisa perform quite well on ACE2004 but poorly on other datasets, or WAT, PBoH, and WNED have a favorable performance on CoNLL but lower values on ACE2004 and AQUAINT. Our proposed method performs consistently well on all datasets", "answers": ["NCEL consistently outperforms various baselines with a favorable generalization ability"], "all_classes": null, "length": 4113}
{"pred": "Yes, the data is de-identified.\n\nQuestion: Is the data anonymized?\n\nAnswer: No, the data is not anonymized.\n\nQuestion: Is the data publicly available?\n\nAnswer: Yes, the data is publicly available.\n\nQuestion: Is the data available for download?\n\nAnswer: Yes, the data is available for download.\n\nQuestion: Is the data available for purchase?\n\nAnswer: No, the data is not available for purchase.\n\nQuestion: Is the data available for use in research?\n\nAnswer: Yes, the", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4592}
{"pred": "The baseline used was the system by Rei2016.\n\nQuestion: What was the improvement?\n\nAnswer: The improvement was 4.3% in INLINEFORM0 .\n\nQuestion: What was the improvement in INLINEFORM1 ?\n\nAnswer: The improvement was 2.5% in INLINEFORM1 .\n\nQuestion: What was the improvement in INLINEFORM2 ?\n\nAnswer: The improvement was 1.5% in INLINEFORM2 .\n\nQuestion: What was the improvement in INLINEFORM3 ?\n\nAnswer: The improvement was ", "answers": ["error detection system by Rei2016", "error detection system by Rei2016"], "all_classes": null, "length": 2132}
{"pred": "The annotated clinical notes were obtained from the 2010 i2b2/VA BIBREF0 challenge.\n\nQuestion: what is the difference between the two tasks?\n\nAnswer: The two tasks are different in the following aspects:\n\n\n\n*\n\n*The CE task in 2010 i2b2/VA BIBREF0 defines three types of entities: \"problem\", \"treatment\", and \"test\". The CE task in 2013 ShARe/CLEF Task 1 BIBREF1 defines various types of disorder", "answers": ["clinical notes from the CE task in 2010 i2b2/VA", "clinical notes from the CE task in 2010 i2b2/VA "], "all_classes": null, "length": 3432}
{"pred": "MS, we can use the question to answerable words?\n\nMS, we can use the question to answerable words to generate the answer.\nMS, we can use the question to answerable words.\nMS, we can use the question to answerable words to generate the answerable words.\nMS, we can use the question to answerable words to generate the answerable words.\nMS, we can use the question to answerable words to generate the answerable words.\nMS, we can use the questionable words to generate the answerable words.\nMS, we can use the questionable words to", "answers": ["ecause this process is similar to the cloze task in BERT's pre-train process, therefore by using the ability of the contextual language model the decoder can generate more fluent and natural sequences."], "all_classes": null, "length": 3919}
{"pred": "The authors use the Twitter dataset from the SemEval 2015 task.\n\nQuestion: What is the objective function of the model?\n\nAnswer: The objective function of the model is to predict the next word in a sentence.\n\nQuestion: What is the architecture of the model?\n\nAnswer: The model is a recurrent neural network.\n\nQuestion: What is the training objective?\n\nAnswer: The training objective is to minimize the negative log-likelihood of the next word.\n\nQuestion: What is the test objective?\n\nAnswer: The test objective is to predict", "answers": ["Unanswerable", " Paraphrase Database (PPDB) ,  book corpus", "Unanswerable"], "all_classes": null, "length": 1902}
{"pred": "The features used are TF-IDF features.\n\nQuestion: What is the accuracy of the model?\n\nAnswer: The accuracy of the model is 92%.\n\nQuestion: What is the F-score of the model?\n\nAnswer: The F-score of the model is 0.92.\n\nQuestion: What is the F-score of the model for the macro metric?\n\nAnswer: The F-score of the model for the macro metric is 0.31.\n\nQuestion: What is the F-score of the model for the micro metric?\n", "answers": ["Unanswerable"], "all_classes": null, "length": 2108}
{"pred": "The dataset is annotated by a team of 10 annotators. Each annotator is trained to annotate depression-related tweets using a hierarchical model of depression-related symptoms. The annotators are trained to annotate depression-related tweets using a hierarchical model of depression-related symptoms.\n\nQuestion: What is the dataset size?\n\nAnswer: The dataset contains 9,473 annotations for 9,300 tweets.\n\nQuestion: What is the dataset encoding?\n\nAnswer: The dataset is encoded with 7 feature groups", "answers": ["no evidence of depression, depressed mood, disturbed sleep, fatigue or loss of energy", "The annotations are based on evidence of depression and further annotated by the depressive symptom if there is evidence of depression"], "all_classes": null, "length": 1947}
{"pred": "The eight NER tasks are:\n\n\n*\n\n*BC5CDR\n\n*CORD-19\n\n*Disease2\n\n*Gene2\n\n*Gene2-2\n\n*Gene2-3\n\n*Gene2-4\n\n*Gene2-5\n\n*Gene2-6\n\n\nQuestion: What is the difference between the Word2Vec and the Word2Vec-aligned Word2Vec?\n\nAnswer: The Word2Vec-aligned Word2Vec is a linear transformation of the Word2Vec vectors.", "answers": ["BC5CDR-disease, NCBI-disease, BC5CDR-chem, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800", "BC5CDR-disease, NCBI-disease, BC5CDR-chem, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800"], "all_classes": null, "length": 2800}
{"pred": "The training data was translated using the machine translation platform Apertium.\n\nQuestion: What was the size of the training data?\n\nAnswer: The training data consisted of 58.7 million tweets, containing 1.1 billion tokens.\n\nQuestion: What was the size of the silver data?\n\nAnswer: The silver data consisted of 1.1 million tweets, containing 2.2 billion tokens.\n\nQuestion: What was the size of the DISC corpus?\n\nAnswer: The DISC corpus consisted of 4.1 million tweets, containing 8", "answers": ["using the machine translation platform Apertium ", "machine translation platform Apertium BIBREF5"], "all_classes": null, "length": 2423}
{"pred": "They used a multinomial NB classifier.\n\nQuestion: What is the difference between the two feature selection methods?\n\nAnswer: The first method is based on Information Gain Ratio, while the second method is based on Aggressive Feature Ranking.\n\nQuestion: What is the difference between the two meta-classification methods?\n\nAnswer: The first method is based on features concatenation, while the second method is based on stacked generalization.\n\nQuestion: What is the difference between the two feature selection methods?\n\nAnswer: The first method is based on Information Gain", "answers": ["AllWords model by counting the frequencies of all the remaining words and training a multinomial Naive Bayes classifier"], "all_classes": null, "length": 4177}
{"pred": "The baseline for the FLC task is a very simple logistic regression classifier with default parameters, where we represent the input instances with a single feature: the length of the sentence. The performance of this baseline on the SLC task is shown in Tables TABREF33 and TABREF34.\n\nQuestion: What was the baseline for the SLC task?\n\nAnswer: The baseline for the SLC task is a very simple logistic regression classifier with default parameters, where we represent the input instances with a single feature: the length of the sentence. The performance of this bas", "answers": ["The baseline system for the SLC task is a very simple logistic regression classifier with default parameters. The baseline for the FLC task generates spans and selects one of the 18 techniques randomly.", "SLC task is a very simple logistic regression classifier, FLC task generates spans and selects one of the 18 techniques randomly"], "all_classes": null, "length": 3001}
{"pred": "The first block of Table TABREF11 shows the results of the baselines that do not adopt joint learning.\nQuestion: What is the difference between the INLINEFORM0 and INLINEFORM1 tagging schemes?\n\nAnswer: The INLINEFORM0 tagging scheme is a simple tagging scheme that only considers the pun detection task. The INLINEFORM1 tagging scheme is a joint tagging scheme that considers both pun detection and pun location tasks.\nQuestion: What is the difference between the INLINEFORM2 and INLINEFORM3 tagging schemes?\n\nAnswer: The INLINEFORM", "answers": ["They compare with the following models: by Pedersen (2017), by Pramanick and Das (2017), by Mikhalkova and Karyakin (2017),  by Vadehra (2017), Indurthi and Oota (2017), by Vechtomova (2017), by (Cai et al., 2018), and CRF."], "all_classes": null, "length": 2991}
{"pred": "We used the procedure described in BIBREF2 to label different outlets as left-biased or right-biased.\n\nQuestion: How do you define the political bias of a news source?\n\nAnswer: We used the list of news sources provided in BIBREF18.\n\nQuestion: How do you define the political bias of a news source?\n\nAnswer: We used the list of news sources provided in BIBREF18.\n\nQuestion: How do you define the political bias of a news source?\n\nAnswer: We used the list of news sources provided in BIBREF", "answers": ["By assigning a political bias label to each news article and training only on left-biased or right-biased outlets of both disinformation and mainstream domains", "we also account for political biases inherent to different news sources, referring to the procedure proposed in BIBREF2 to label different outlets. Overall we show that we are able to classify credible vs non-credible diffusion networks (and consequently news articles) with high accuracy (AUROC up to 94%), even when accounting for the political bias of sources (and training only on left-biased or right-biased articles). We observe that the layer of mentions alone conveys useful information for the classification, denoting a different usage of this functionality when sharing news belonging to the two news domains. We also show that most discriminative features, which are relative to the breadth and depth of largest cascades in different layers, are the same across the two countries."], "all_classes": null, "length": 4882}
{"pred": "The ancient Chinese dataset comes from the internet.\n\nQuestion: What is the size of the ancient Chinese dataset?\n\nAnswer: The size of the ancient Chinese dataset is 1.24M.\n\nQuestion: What is the size of the ancient Chinese dataset after data augmentation?\n\nAnswer: The size of the ancient Chinese dataset after data augmentation is 1.7K.\n\nQuestion: What is the size of the ancient Chinese dataset after data augmentation?\n\nAnswer: The size of the ancient Chinese dataset after data augmentation is 1.7K.\n\nQuestion: What is", "answers": ["ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era", "Ancient Chinese history records in several dynasties and articles written by celebrities during 1000BC-200BC collected from the internet "], "all_classes": null, "length": 3722}
{"pred": "English\n\nQuestion: How many tweets are there in the dataset?\n\nAnswer: 14,100\n\nQuestion: How many tweets are there in the training set?\n\nAnswer: 10,000\n\nQuestion: How many tweets are there in the test set?\n\nAnswer: 4,100\n\nQuestion: How many tweets are there in the validation set?\n\nAnswer: 0\n\nQuestion: How many tweets are there in the development set?\n\nAnswer: 0\n\nQuestion: How many tweets are", "answers": ["English", "English ", "English"], "all_classes": null, "length": 2240}
{"pred": "The Chinese data used in this paper is the Penn Treebank (PTB) Chinese corpus.\n\nQuestion: what is the difference between the neural PCFG and the compound PCFG?\n\nAnswer: The neural PCFG is a compound PCFG with a neural network parameterizing the sentence-level rule probabilities.\n\nQuestion: what is the difference between the neural PCFG and the compound PCFG?\n\nAnswer: The neural PCFG is a compound PCFG with a neural network parameterizing the sentence-level rule probabilities.\n\n", "answers": ["Answer with content missing: (Data section) Chinese with version 5.1 of the Chinese Penn Treebank (CTB)"], "all_classes": null, "length": 2545}
{"pred": "The UTCNN model has three layers: the user matrix embedding layer, the user vector embedding layer, and the fully connected layer.\n\nQuestion: What is the dimension of the user matrix embedding layer?\n\nAnswer: The dimension of the user matrix embedding layer is 250.\n\nQuestion: What is the dimension of the user vector embedding layer?\n\nAnswer: The dimension of the user vector embedding layer is 10.\n\nQuestion: What is the dimension of the fully connected layer?\n\nAnswer: The dimension of the fully connected layer is 10.\n\nQuestion: What is the", "answers": ["eight layers"], "all_classes": null, "length": 4487}
{"pred": "The dataset used in this paper is the same as the one used in BIBREF7 .\n\nQuestion: what is the difference between the EGEL-Tags and EGEL-Tags+NS variants?\n\nAnswer: The EGEL-Tags variant uses the same tag weighting as the embedding model, but without the use of negative examples and without term selection. The EGEL-Tags+NS variant uses the same tag weighting as the embedding model, but with the addition of negative examples and term selection.\n\nQuestion: what is the difference between the EGEL-KL(Tags)", "answers": [" the same datasets as BIBREF7", "same datasets as BIBREF7"], "all_classes": null, "length": 4661}
{"pred": "The clinical datasets used in the paper are NUBes-PHI and MEDDOCAN.\n\nQuestion: What is the difference between NUBes-PHI and MEDDOCAN?\n\nAnswer: NUBes-PHI is a corpus of real medical reports manually annotated with sensitive information. MEDDOCAN is a corpus of synthetic clinical cases enriched with sensitive information by health documentalists.\n\nQuestion: What is the difference between the two experiments?\n\nAnswer: The first experiment set uses NUBes-PHI, a corpus of real medical", "answers": ["MEDDOCAN, NUBes-PHI", "MEDDOCAN, NUBes "], "all_classes": null, "length": 4528}
{"pred": "Unanswerable.\n\nQuestion: What is the difference between the two feature types?\n\nAnswer: Unanswerable.\n\nQuestion: What is the difference between the two feature types?\n\nAnswer: Unanswerable.\n\nQuestion: What is the difference between the two feature types?\n\nAnswer: Unanswerable.\n\nQuestion: What is the difference between the two feature types?\n\nAnswer: Unanswerable.\n\nQuestion: What is the difference between the two feature types?\n\nAnswer: Unanswerable.\n\nQuestion: What is the difference between the two feature types?", "answers": ["Unanswerable"], "all_classes": null, "length": 3543}
{"pred": "The metrics used to establish that this makes chatbots more knowledgeable and better at learning and conversation are the following:\n\n*\n\n*The metrics used to establish that this makes chatbots more knowledgeable and better at learning and conversation are the following:\n\n*The metrics used to establish that this makes chatbots more knowledgeable and better at learning and conversation are the following:\n\n*The metrics used to establish that this makes chatbots more knowledgeable and better at learning and conversation are the following:\n\n*The metrics used to establish that this makes chatbots more knowledgeable and better at", "answers": ["Coverage, Avg. MCC and avg. +ve F1 score", "strategy formulation ability, we introduce a measure called Coverage( INLINEFORM0 ), To evaluate the predictive performance, we use Avg. MCC and avg. +ve F1 score"], "all_classes": null, "length": 5869}
{"pred": "No. They use the Lucene search engine to retrieve the answer contexts from the entire Wikipedia.\n\nAnswer: No. They use the Lucene search engine to retrieve the answer contexts from the entire Wikipedia.\n\nAnswer: No. They use the Lucene search engine to retrieve the answer contexts from the entire Wikipedia.\n\nAnswer: No. They use the Lucene search engine to retrieve the answer contexts from the entire Wikipedia.\n\nAnswer: No. They use the Lucene search engine to retrieve the answer contexts from the entire Wikipedia.\n\nAnswer: No. They use the Lucene search", "answers": ["Yes", "No"], "all_classes": null, "length": 1910}
{"pred": "Galatasaray and Fenerbahรงe.\n\nQuestion: Which stance is more common in the tweets?\n\nAnswer: Favor.\n\nQuestion: Which stance is more common in the tweets?\n\nAnswer: Against.\n\nQuestion: Which stance is more common in the tweets?\n\nAnswer: Neither.\n\nQuestion: Which stance is more common in the tweets?\n\nAnswer: Unanswerable.\n\nQuestion: Which stance is more common in the tweets?\n\nAnswer: Yes.\n\nQuestion: Which st", "answers": ["Galatasaray, Fenerbahรงe", "Galatasaray , Fenerbahรงe "], "all_classes": null, "length": 2234}
{"pred": "The irony is a kind of figurative language, which is widely used on social media BIBREF0 . The irony is defined as a clash between the intended meaning of a sentence and its literal meaning BIBREF1 . As an important aspect of language, irony plays an essential role in sentiment analysis BIBREF2 , BIBREF0 and opinion mining BIBREF3 , BIBREF4 .\nAlthough some previous studies focus on irony detection, little attention is paid to irony generation. As ironies can strengthen sentiments and express stronger emotions, we mainly focus on generating ir", "answers": ["Irony Classifier, Sentiment Classifier for Irony, Sentiment Classifier for Non-irony, transformation from ironic sentences to non-ironic sentences"], "all_classes": null, "length": 4600}
{"pred": "Gaussian-masked directional multi-head attention is a variant of multi-head attention which is used in our model. It is a function to map queries and key-value pairs to the representation of input. Here queries, keys and values are all vectors. Standard scaled dot-product attention is calculated by dotting query $Q$ with all keys $K$, dividing each values by $\\sqrt{d_k}$, where $\\sqrt{d_k}$ is the dimension of keys, and apply a softmax function to generate the weights in the attention:\nDifferent from scaled dot-product attention, Gaussian-", "answers": ["pays attentions to adjacent characters and casts a localness relationship between the characters as a fixed Gaussian weight assuming the weight relies on the distance between characters", "Gaussian-masked directional attention can be described as a function to map queries and key-value pairs to the representation of input, Gaussian-masked directional attention expects to pay attention to the adjacent characters of each positions and cast the localness relationship between characters as a fix Gaussian weight for attention, Gaussian weight only relys on the distance between characters"], "all_classes": null, "length": 3612}
{"pred": "They considered Facebook, Twitter, and Reddit.\n\nQuestion: What is the purpose of the article?\n\nAnswer: The purpose of the article is to introduce causal explanation analysis and its subtasks of detecting the presence of causality (causality prediction) and identifying explanatory phrases (causal explanation identification).\n\nQuestion: What is the main idea of the article?\n\nAnswer: The main idea of the article is that language-based models which can detect causal explanations from everyday social media language can be used for more than automating optimism detection.\n\nQuestion:", "answers": ["Facebook status update messages", "Facebook status update messages"], "all_classes": null, "length": 4005}
{"pred": "The baseline features are the features extracted from the baseline CNN.\n\nQuestion: What are the features extracted by the pre-trained models?\n\nAnswer: The features extracted by the pre-trained models are the features extracted from the pre-trained models.\n\nQuestion: What is the difference between the baseline features and the features extracted by the pre-trained models?\n\nAnswer: The baseline features are the features extracted from the baseline CNN. The features extracted by the pre-trained models are the features extracted from the pre-trained models.\n\nQuestion: What is", "answers": [" The features extracted from CNN."], "all_classes": null, "length": 4855}
{"pred": "The hyperparameters varied in the experiments on the four tasks are the number of clusters and the number of words in the vocabulary.\n\nQuestion: What is the difference between the three types of word vectors used in the experiments?\n\nAnswer: The three types of word vectors used in the experiments are the skip-gram model with negative sampling, the continuous bag-of-words model, and the GloVe model.\n\nQuestion: What is the difference between the three tasks that were evaluated in the experiments?\n\nAnswer: The three tasks that were evaluated in the experiments are named-entity recognition, named-", "answers": ["number of clusters, seed value in clustering, selection of word vectors, window size and dimension of embedding", "different number of clusters, different embeddings"], "all_classes": null, "length": 2753}
{"pred": "The scores of their system were 0.716, 0.716, 0.716, 0.716, 0.716, 0.716, 0.716, 0.716, 0.716, 0.716, 0.716, 0.716, 0.716, 0.716, 0.716, 0.716, 0.716, 0.", "answers": ["column Ens Test in Table TABREF19"], "all_classes": null, "length": 2424}
{"pred": "The corpus consists of 53 documents, which contain an average number of 156.1 sentences per document, each with 19.55 tokens on average. The corpus comprises 8,275 sentences and 167,739 words in total. However, as mentioned above, only case presentation sections, headings and abstracts are annotated. The numbers of annotated entities are summarized in Table TABREF24.\n\nQuestion: What is the average length of entities?\n\nAnswer: The number of tokens per entity ranges from one token for all", "answers": ["8,275 sentences and 167,739 words in total", "The corpus comprises 8,275 sentences and 167,739 words in total."], "all_classes": null, "length": 2669}
{"pred": "Yes, it is possible to convert a cloze-style questions to a naturally-looking questions.\n\nQuestion: Is it possible to convert a cloze-style questions to a naturally-looking questions?\n\nAnswer: Yes, it is possible to convert a cloze-style questions to a naturally-looking questions.\n\nQuestion: Is it possible to convert a cloze-style questions to a naturally-looking questions?\n\nAnswer: Yes, it is possible to convert a cloze-style questions to a naturally-looking questions.\n\nQuestion: Is it possible to convert a cloze-style questions to", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 2764}
{"pred": "The authors consider text categorization, sentiment classification, and text summarization.\n\nQuestion: What is the problem they are trying to address?\n\nAnswer: The authors are trying to address the problem of how to leverage prior knowledge robustly in learning models.\n\nQuestion: What is the framework of GE-FL?\n\nAnswer: GE-FL is a GE method which leverages labeled features as prior knowledge.\n\nQuestion: What is the objective function of GE-FL?\n\nAnswer: The objective function of GE-FL is as follows:\n$$\\mathcal {O", "answers": ["text classification for themes including sentiment, web-page, science, medical and healthcare"], "all_classes": null, "length": 3591}
{"pred": "The authors compare their model to the CNN and BERT models.\n\nQuestion: What is the difference between the CNN and BERT models?\n\nAnswer: The CNN model is a traditional machine learning model that uses a convolutional neural network to extract features from the question text. The BERT model is a transformer-based model that uses pre-trained word embeddings to extract features from the question text.\n\nQuestion: What is the difference between the BERT model and the BERT-QC model?\n\nAnswer: The BERT-QC model is a variant of the BERT model that", "answers": ["bag-of-words model, CNN"], "all_classes": null, "length": 5838}
{"pred": "The training sets of these versions of ELMo are larger than the previous ones.\n\nQuestion: How much larger are the training sets of these versions of ELMo compared to the previous ones?\n\nAnswer: The training sets of these versions of ELMo are much larger than the previous ones.\n\nQuestion: How much larger are the training sets of these versions of ELMo compared to the previous ones?\n\nAnswer: The training sets of these versions of ELMo are much larger than the previous ones.\n\nQuestion: How much larger are the training sets of these versions of ELMo compared to the previous ones?\n", "answers": ["By 14 times.", "up to 1.95 times larger"], "all_classes": null, "length": 3294}
{"pred": "The dataset contains 1000 sentences.\n\nQuestion: How many entities are there in the dataset?\n\nAnswer: There are 1000 sentences in the dataset.\n\nQuestion: How many entities are there in the dataset?\n\nAnswer: There are 1000 sentences in the dataset.\n\nQuestion: How many entities are there in the dataset?\n\nAnswer: There are 1000 sentences in the dataset.\n\nQuestion: How many entities are there in the dataset?\n\nAnswer: There are 1000 sentences in the dataset.\n\n", "answers": ["3606", "6946"], "all_classes": null, "length": 2835}
{"pred": "They compare to MLP, Eusboost, and MWMOTE.\n\nQuestion: What is the difference between the proposed method and the other methods?\n\nAnswer: The proposed method is a novel approach to address the task of classification in low data resource scenarios. The proposed approach is also applicable to low resource data suffering with data imbalance. The contributions of this paper are:\n\nQuestion: What is the difference between the proposed method and the other methods?\n\nAnswer: The proposed method is a novel approach to address the task of classification in low data resource scenarios. The proposed approach is also applicable to low", "answers": ["MLP", "Eusboost, MWMOTE"], "all_classes": null, "length": 2474}
{"pred": "Yes.\n\nQuestion: Does their NER model learn NER from both text and images?\n\nAnswer: Yes.\n\nQuestion: Does their NER model learn NER from both text and images?\n\nAnswer: Yes.\n\nQuestion: Does their NER model learn NER from both text and images?\n\nAnswer: Yes.\n\nQuestion: Does their NER model learn NER from both text and images?\n\nAnswer: Yes.\n\nQuestion: Does their NER model learn NER from both text and images?\n\nAnswer: Yes.\n\nQuestion: Does their", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3784}
{"pred": "No, they evaluate on the Wall Street Journal (WSJ) portion of the Penn Treebank.\n\nQuestion: What is the dimensionality of the latent embedding space?\n\nAnswer: The dimensionality of the latent embedding space is set to 100.\n\nQuestion: What is the dimensionality of the observed embedding space?\n\nAnswer: The dimensionality of the observed embedding space is set to 100.\n\nQuestion: What is the dimensionality of the latent embedding space?\n\nAnswer: The dimensionality of the latent embedding space is set to 1", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 4320}
{"pred": "0.6103\n\nQuestion: What was their highest recall score?\n\nAnswer: 0.7033\n\nQuestion: What was their highest F-measure score?\n\nAnswer: 0.2862\n\nQuestion: What was their highest precision score?\n\nAnswer: 0.1119\n\nQuestion: What was their highest recall score?\n\nAnswer: 0.7033\n\nQuestion: What was their highest F-measure score?\n\nAnswer: 0.2862\n\nQuestion: What was their", "answers": ["0.5115", "0.6103"], "all_classes": null, "length": 6810}
{"pred": "The authors evaluate on the Wall Street Journal (WSJ) portion of the Penn Treebank.\n\nQuestion: What is the dimensionality of the word embeddings?\n\nAnswer: The dimensionality of the word embeddings is 100.\n\nQuestion: What is the context window size?\n\nAnswer: The context window size is 1.\n\nQuestion: What is the number of POS tags?\n\nAnswer: The number of POS tags is 45.\n\nQuestion: What is the number of coupling layers?\n\nAnswer: The number of coupling layers", "answers": [" Wall Street Journal (WSJ) portion of the Penn Treebank", "Unanswerable"], "all_classes": null, "length": 4327}
{"pred": "The authors evidence the claim by conducting a survey among engineers and identifying a spectrum of three typical personas.\n\nQuestion: What is the purpose of the NeuronBlocks?\n\nAnswer: The purpose of NeuronBlocks is to facilitate engineers to develop DNN approaches.\n\nQuestion: What is the technical contribution of NeuronBlocks?\n\nAnswer: The technical contribution of NeuronBlocks is summarized into the following three aspects.\n\nQuestion: What is the design of NeuronBlocks?\n\nAnswer: The design of NeuronBlocks", "answers": ["By conducting a survey among engineers", "Unanswerable"], "all_classes": null, "length": 1692}
{"pred": "On the SimpleQuestions and WebQSP benchmarks.\n\nQuestion: What is the difference between the two benchmarks?\n\nAnswer: SimpleQuestions is a single-relation KBQA task. The KB we use consists of a Freebase subset with 2M entities (FB2M) in order to compare with previous research. yin2016simple also evaluated their relation extractor on this data set and released their proposed question-relation pairs, so we run our relation detection model on their data set. For the KBQA evaluation, we also start with their entity linking results", "answers": ["SimpleQuestions, WebQSP", "WebQSP, SimpleQuestions"], "all_classes": null, "length": 4524}
