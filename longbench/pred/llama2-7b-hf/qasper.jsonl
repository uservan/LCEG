{"pred": "The ground truth for fake news is established by a human annotator.\n\nQuestion: What is the dataset used in this study?\n\nAnswer: The dataset used in this study is the Twitter's public API.\n\nQuestion: What is the dataset used in this study?\n\nAnswer: The dataset used in this study is the Twitter's public API.\n\nQuestion: What is the number of tweets used in this study?\n\nAnswer: The number of tweets used in this study is 13200.\n\nQuestion: What is the number of tweets used in this study", "answers": ["Ground truth is not established in the paper"], "all_classes": null, "length": 4096}
{"pred": "GhostVLAD is an extension of the NetVLAD approach, which we discussed in the previous section. The GhostVLAD model was proposed for face recognition by Y. Zhong [10]. GhostVLAD works exactly similar to NetVLAD except it adds Ghost clusters along with the NetVLAD clusters. So, now we will have a K+G number of clusters instead of K clusters. Where G is the number of ghost clusters, we want to add (typically 2-4). The Ghost clusters are added to map any noisy or irrelevant content into ghost clusters and are", "answers": ["extension of the NetVLAD, adds Ghost clusters along with the NetVLAD clusters", "An extension of NetVLAD which replaces hard assignment-based clustering with soft assignment-based clustering with the additon o fusing Ghost clusters to deal with noisy content."], "all_classes": null, "length": 3769}
{"pred": "The proposed model outperforms previous state-of-the-art methods by 68.8% to 71.8% when applied to the IEMOCAP dataset, which is one of the most well-studied datasets.\n\n\nQuestion: What is the proposed model?\n\nAnswer: We propose a novel deep dual recurrent encoder model that simultaneously utilizes audio and text data in recognizing emotions from speech.\n\nQuestion: What is the proposed model?\nAnswer: The proposed model outperforms previous state-of-the-the-the-the-the-the", "answers": ["the attention model, MDREA, also outperforms the best existing research results (WAP 0.690 to 0.688)"], "all_classes": null, "length": 4097}
{"pred": "The article proposes the use of context tweets to improve the accuracy of neural network models.\n\nQuestion: What is the most accurate model?\n\nAnswer: The most accurate model is the RNN with LTC modules.\n\nQuestion: What is the most significant improvement?\n\nAnswer: The most significant improvement is the use of context tweets in RNN models.\n\nQuestion: What is the most significant decrease in accuracy?\n\nAnswer: The most significant decrease in accuracy is the use of character-level features in neural network models.\n\nQuestion: What is the most significant improvement in accuracy?\n", "answers": ["using tweets that one has replied or quoted to as contextual information", "text sequences of context tweets"], "all_classes": null, "length": 3493}
{"pred": "They looked at FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Light, Cooking Adventure, Disney.\n\n\nQuestion: What is the emotion distribution of the emotion labels?\n\nAnswer: Sadness, Angry, Haha, Love, Wow, Sad, Angry,\n\n\nQuestion: What is the emotion distribution of the emotion distribution of the emotion distribution of the emotion distribution of the emotion distribution of the emotion distribution of the em", "answers": ["FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney", "FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney."], "all_classes": null, "length": 4096}
{"pred": "No, the hashtag dataset contains tweets in English, Spanish, French, and German. The SemEval dataset contains tweets in English, Spanish, French, and German.\n\nQuestion: What is the hashtag segmentation task?\n\nAnswer: The task is to split a hashtag into a sequence of words.\n\nQuestion: What is the hashtag segmentation task?\n\nAnswer: The task is to split a hashtag into a sequence of words.\n\nQuestion: What is the hashtag segmentation task?\n\nAnswer: The task is to split a", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4097}
{"pred": "The task is defined as a summarization task, where the output is a concept map. The evaluation is based on the quality of the concept map, which is measured by the number of concepts and relations in the map, the number of connections between them and the size of the map.\n\nQuestion: What is the size limit?\n\nAnswer: The size limit is the number of concepts and relations in the map.\n\nQuestion: What is the size limit?\n\nAnswer: The size limit is the number of concepts and relations in the map.\n\nQuestion: What is the size limit?\nAnswer: The size", "answers": ["Answer with content missing: (Evaluation Metrics section) Precision, Recall, F1-scores, Strict match, METEOR, ROUGE-2"], "all_classes": null, "length": 4096}
{"pred": "We evaluate our models on three single-document news summarization datasets: CNN/DailyMail, NYT, and XSum.\n\nQuestion: What is the evaluation protocol for extractive summarization?\n\nAnswer: We report ROUGELUGELUGELUGELUGELUGELUGELUGELUGELUGELUGELUGELUGELUGELUGELUGELUGELUGELUGELUGELUGELUGELUGELUGELUGELUGELU", "answers": ["CNN/DailyMail news highlights, New York Times Annotated Corpus, XSum", "the CNN/DailyMail news highlights dataset BIBREF24, the New York Times Annotated Corpus (NYT; BIBREF25), XSum BIBREF22"], "all_classes": null, "length": 4096}
{"pred": "The proposed approach is different from other WSD approaches employing word embeddings in the following ways:\n\n1. The proposed approach uses a variant of max-margin objective based on the asymmetric KL divergence energy function to capture textual entailment (asymmetry) and word similarity (symmetry).\n\n2. The proposed approach uses a Gaussian mixture model to capture multi-sense word distribution.\n\n3. The proposed approach uses a stricter bound on KL between Gaussian mixtures.\n\n4. The proposed approach uses a stricter bound on KL between", "answers": ["GM$\\_$KL achieves better correlation than existing approaches for various metrics on SCWS dataset."], "all_classes": null, "length": 3983}
{"pred": "The ensemble method works by taking the average of the predictions from multiple models. This allows the ensemble to benefit from the strengths of each individual model and to compensate for any weaknesses.\n\nQuestion: What is the BookTest?\n\n\nAnswer: The BookTest is a dataset of cloze-style questions and answers.\n\nQuestion: What is the BookTest dataset?\n\nAnswer: The BookTest is a dataset of cloze-style questions and answers.\n\nQuestion: What is the BookTest?\n\nAnswer: The BookTest is a dataset of cloze-style questions and answers.", "answers": ["simply averaging the predictions from the constituent single models"], "all_classes": null, "length": 4096}
{"pred": "The datasets are collected from the following sources:\n\n1. Friends dataset: Friends TV sitcom\n\n2. EmotionPush dataset: Facebook messenger chats\n\nQuestion: What is the objective of the challenge?\n\nAnswer: The objective of the challenge is to predict the emotion for all utterances in the dataset, namely, the article, the article and the emotion classifier can be extended from the speech-based the text-based the emotion recognition.\n\nQuestion: What is the number of the number of the number of the number of the number of the number of the number of", "answers": ["Friends TV sitcom, Facebook messenger chats"], "all_classes": null, "length": 4096}
{"pred": "English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\n", "answers": ["English", "Simple English"], "all_classes": null, "length": 3672}
{"pred": "IMDb dataset\n\nQuestion: What is the optimal combination of word2vec hyper-parameters for intrinsic and extrinsic NLP purposes?\n\nAnswer: The optimal combination of word2vec hyper-parameters for intrinsic and extrinsic NLP purposes is the combination of skipgram using hierarchical softmax and window size of 8 for 300 dimensions.\n\nQuestion: What is the optimal combination of word2vec hyper-parameters for intrinsic and extrinsic NLP purposes?\n\nAnswer: The optimal combination of word2vec hyper-parameters for intrinsic and extrinsic NLP purposes is the combination", "answers": ["IMDb dataset of movie reviews", "IMDb"], "all_classes": null, "length": 3827}
{"pred": "The proposed system achieves +6.12 F1 improvement on DL-PS, +4.51 on EC-MT, and +9.19 on EC-UQ.\n\nQuestion: What is the reason for the majority-voting method performs worse than the other methods?\n\nAnswer: The majority-voting method performs worse than other methods because of not trusting the second annotation.\n\nQuestion: What is the reason for the majority-voting performs so badly?\n\nAnswer: The majority-voting performs so badly because of not trusting the second annotation.\n\n", "answers": ["F1 scores of 85.99 on the DL-PS data,  75.15 on the EC-MT data and 71.53 on the EC-UQ data ", "F1 of 85.99 on the DL-PS dataset (dialog domain); 75.15 on EC-MT and 71.53 on EC-UQ (e-commerce domain)"], "all_classes": null, "length": 4096}
{"pred": "Yes\n\nAnswer: No\n\nAnswer: Unanswerable\n\nAnswer: Yes\n\nAnswer: No\n\nAnswer: Unanswerable\n\nAnswer: Yes\n\nAnswer: No\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\n\nAnswer: Yes\nAnswer: No\nAnswer: Unanswerable\nAnswer: Yes\nAnswer: No\nAnswer: Unanswerable\n\nAnswer: Yes\nAnswer: No\nAnswer: Unanswerable\n\nAnswer: Yes\nAnswer: No\nAnswer: Unanswerable\nAnswer: Yes\nAnswer: No\nAnswer: Unanswerable\n", "answers": ["No"], "all_classes": null, "length": 4097}
{"pred": "The dataset used is the one described in the article.\n\nQuestion: What is the purpose of the system?\n\nAnswer: The purpose of the system is to provide a conversational system that can be used by people to ask questions and get answers.\n\nQuestion: What is the purpose of the system?\n\nAnswer: The purpose of the system is to provide a conversational system that can be used by people to ask questions and get answers.\n\nQuestion: What is the purpose of the system?\n\nAnswer: The purpose of the system is to provide a conversational system that can be used by people.", "answers": ["Custom dataset with user questions; set of documents, twitter posts and news articles, all related to finance.", "a self-collected financial intents dataset in Portuguese"], "all_classes": null, "length": 4096}
{"pred": "The best performing sector was the HealthCare sector.\n\nAnswer: The best performing sector was the HealthCare sector.\n\nAnswer: The best performing sector was the HealthCare sector.\n\nAnswer: The best performing sector was the HealthCare sector.\n\nAnswer: The best performing sector was the HealthCare sector.\nAnswer: The best performing sector was the HealthCare sector.\n\nAnswer: The best performing sector was the HealthCare sector.\n\nAnswer: The best performing sector was the HealthCare sector.\n\nAnswer: The best performing sector was the HealthCare", "answers": ["Energy with accuracy of 0.538", "Energy"], "all_classes": null, "length": 4096}
{"pred": "RNN-based NMT and Transformer based NMT.\n\nQuestion: what is the average length of the sentences translated by Transformer?\n\nAnswer: 16.78.\n\nQuestion: what is the average length of the sentences translated by RNN-based NMT?\n\nAnswer: 17.15.\n\nQuestion: what is the average length of the sentences translated by SMT?\n\nAnswer: 15.\n\nQuestion: what is the average length of the sentences translated by SMT?\n\nAnswer: 15.\n\nQuestion: what", "answers": ["RNN-based NMT model, Transformer-NMT"], "all_classes": null, "length": 4096}
{"pred": "The three regularization terms are neutral features, maximum entropy, and KL divergence.\n\nQuestion: What is the purpose of the regularization terms?\n\nAnswer: The purpose of the regularization terms is to make the model more robust and practical.\n\nQuestion: What is the influence of the regularization terms?\n\nAnswer: The influence of the regularization terms is the model is more robust and practical.\n\nQuestion: What is the difference between GEFL and our methods?\n\nAnswer: The difference between GEFL and our methods is that incorporating KLDA selected features performs better than", "answers": ["a regularization term associated with neutral features, the maximum entropy of class distribution regularization term, the KL divergence between reference and predicted class distribution", "a regularization term associated with neutral features,  the maximum entropy of class distribution, KL divergence between reference and predicted class distribution"], "all_classes": null, "length": 4096}
{"pred": "The baselines are the SVM with n-gram or average word embedding feature, CNN and RCNN.\n\nQuestion: What is the FBFans dataset?\n\nAnswer: The FBFans dataset is a dataset of 10,00000000000000000000000000000000000000000000000000000000000000000000000000000", "answers": ["SVM with unigram, bigram, and trigram features, SVM with average word embedding, SVM with average transformed word embeddings, CNN, ecurrent Convolutional Neural Networks, SVM and deep learning models with comment information", "SVM with unigram, bigram, trigram features, with average word embedding, with average transformed word embeddings, CNN and RCNN, SVM, CNN, RCNN with comment information"], "all_classes": null, "length": 4097}
{"pred": "By 10%\n\nAnswer: By 10%\n\nAnswer: By 10%\n\nAnswer: By 10%\n\nAnswer: By 10%\n\nAnswer: By 10%\n\nAnswer: By 10%\n\nAnswer: By 10%\nAnswer: By 10%\nAnswer: By 10%\n\nAnswer: By 10%\n\nAnswer: By 10%\n\nAnswer: By 10%\n\nAnswer: By 10%\n\nAnswer: By 10", "answers": ["They decrease MAE in 0.34"], "all_classes": null, "length": 4096}
{"pred": "The attention distribution of each head is predicted typically using the softmax normalizing transform. As a result, all context words have non-zero attention weight. Recent work on single attention architectures suggest that using sparse normalizing transforms such as sparsely as possible.\n\nQuestion: What is the difference between the attention head roles and the attention distribution of the attention distribution of the attention distribution of the attention distribution of the attention distribution of the attention distribution of the attention distribution of the attention distribution of the attention distribution of the attention distribution of the attention distribution of the attention distribution of the attention distribution of the attention distribution of the attention distribution", "answers": ["the attention heads in the proposed adaptively sparse Transformer can specialize more and with higher confidence", "We introduce sparse attention into the Transformer architecture"], "all_classes": null, "length": 4097}
{"pred": "the baseline is the sentence-level MT system.\n\nQuestion: what is the DocRepair model?\n\nAnswer: the DocRepair model is a monolingual sequence-to-sequence model that maps inconsistent groups of sentences into consistent ones. The consistent groups come from the original training data.\n\nQuestion: what is the DocRepair model trained on round-trip translations of isolated sentences?\n\nAnswer: the DocRepair model is trained on round-trip translations of isolated sentences.\n\nQuestion: what is the DocRepair model trained on round-trip", "answers": [" MT system on the data released by BIBREF11", "Transformer base, two-pass CADec model"], "all_classes": null, "length": 4096}
{"pred": "The article does not provide any metrics for evaluation.\n\nQuestion: What is the difference between the two models?\n\nAnswer: The two models are different in the way they are trained. The first model is trained with a pre-trained model from English and the second model is trained with a pre-trained model from another language.\n\nQuestion: What is the difference between the two models?\n\nAnswer: The two models are different in the way they are trained with a pre-trained model from another language.\n\nQuestion: What is the difference between the two models?\n\nAnswer: The two", "answers": ["translation probabilities, Labeled Attachment Scores (LAS)", "accuracy, Labeled Attachment Scores (LAS)"], "all_classes": null, "length": 4096}
{"pred": "The attention module is pretrained on a large-scale MT dataset.\n\nQuestion: What is the role of the attention module?\n\nAnswer: The attention module plays different roles in pre-training and fine-tuning. In pre-training, the attention module is used to extract semantic and linguistic features from the acoustic features. In fine-tuning, the attention module is used to read acoustic features into the source language space by the semantic and the attention module.\n\nQuestion: What is the role of the attention module of the attention module is a pure-tuning and fine-", "answers": ["the model is pre-trained on CTC-based ASR task and MT task in the pre-training stage."], "all_classes": null, "length": 4096}
{"pred": "The cognitive features, along with textual features used in best available sarcasm detectors, are used to train binary classifiers against given sarcasm labels.\n\nQuestion: What is the purpose of the experiment?\n\nAnswer: To address this problem by considering both human and machine-based tools and techniques.\n\nQuestion: What is the purpose of the experiment?\n\nAnswer: To examine the effectiveness of cognitive features on the classification accuracy by varying the feature set.\n\nQuestion: What is the purpose of the experiment?\n\nAnswer: To examine the effectiveness of the feature-based s", "answers": ["Unanswerable"], "all_classes": null, "length": 4096}
{"pred": "The encoder has a character-based encoder-decoder architecture.\n\nQuestion: What is the auxiliary objective of the system?\n\nAnswer: The auxiliary objective of the system is to predict the MSD tag of the target form.\n\nQuestion: What is the effect of the multilingual training?\n\nAnswer: The effect of the multilingual training is to improve the performance of the system.\n\nQuestion: What is the effect of the monolingual finetuning?\n\nAnswer: The effect of the monolingual finetuning is to improve the performance of the", "answers": ["LSTM", "LSTM"], "all_classes": null, "length": 3500}
{"pred": "Yes\n\nAnswer: No\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\n\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\nAnswer", "answers": ["Unanswerable", "Yes"], "all_classes": null, "length": 4096}
{"pred": "\n\nThe baselines are:\n\n1. A conventional automatic speech recognition (ASR) system typically consists of several independently learned components: an acoustic model to predict context-dependent sub-phoneme states (senones) from audio, a graph structure to map senones to phonemes, and a pronunciation model to map phonemes to words.\n\n2. Hybrid systems combine hidden Markov models to model state dependencies with neural networks to predict states BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 . Newer approaches such as end-to-", "answers": ["Unanswerable", "LF-MMI Attention\nSeq2Seq \nRNN-T \nChar E2E LF-MMI \nPhone E2E LF-MMI \nCTC + Gram-CTC"], "all_classes": null, "length": 3273}
{"pred": "Unanswerable\n\nQuestion: What is the name of the company?\n\nAnswer: Unanswerable\n\nQuestion: What is the name of the company?\n\nAnswer: Unanswerable\n\nQuestion: What is the name of the company?\n\nAnswer: Unanswerable\n\nQuestion: What is the name of the company?\n\nAnswer: Unanswerable\n\nQuestion: What is the name of the company?\n\nAnswer: Unanswerable\n\nQuestion: What is the name of the company?\nAnswer: Unanswerable\n\nQuestion: What is the name of the company?", "answers": ["22,880 users", "20,000"], "all_classes": null, "length": 4096}
{"pred": "We use BLEU-1/4, ROUGE-L, and Distinct-1/2 to evaluate generation quality.\n\nQuestion: What is the dataset used for training and testing?\n\nAnswer: We use a large-scale recipe-generation dataset of 180K+ recipes and 700K+ user reviews for this task.\n\nQuestion: What is the dataset size?\n\nAnswer: The dataset size is 180K+ recipes and 700K+ user reviews.\n\nQuestion: What is the dataset split?\nAnswer", "answers": ["Byte-Pair Encoding perplexity  (BPE PPL),\nBLEU-1,\nBLEU-4,\nROUGE-L,\npercentage of distinct unigram (D-1),\npercentage of distinct bigrams(D-2),\nuser matching accuracy(UMA),\nMean Reciprocal Rank(MRR)\nPairwise preference over baseline(PP)", "BLEU-1/4 and ROUGE-L, likelihood of generated recipes using identical input specifications but conditioned on ten different user profiles, user matching accuracy (UMA), Mean Reciprocal Rank (MRR), neural scoring model from BIBREF33 to measure recipe-level coherence", " Distinct-1/2, UMA = User Matching Accuracy, MRR\n= Mean Reciprocal Rank, PP = Pairwise preference over baseline (evaluated for 310 recipe pairs per model)"], "all_classes": null, "length": 4096}
{"pred": "The labels are:\n\n(1) No answer: The patient does not mention the symptom or attribute.\n\n(2) Yes answer: The patient mentions the symptom or attribute.\n\n(3) Unanswerable: The patient does not mention the symptom or attribute.\n\n\nQuestion: What is the answerable classifier?\n\nAnswer: The answerable classifier is a binary classifier.\n\nQuestion: What is the answerable classifier?\n\nAnswer: The answerable classifier is a binary classifier.\n\nQuestion: What is the answerable classifier?\n", "answers": ["(1) the time the patient has been experiencing the symptom, (2) activities that trigger the symptom (to occur or worsen), (3) the extent of seriousness, (4) the frequency occurrence of the symptom, and (5) the location of symptom, No Answer", "the time the patient has been experiencing the symptom, activities that trigger the symptom, the extent of seriousness, the frequency occurrence of the symptom, the location of symptom, 9 symptoms"], "all_classes": null, "length": 4095}
{"pred": "The task-specific encoder is trained on the data that is annotated by experts.\n\nQuestion: How much data is needed to train the task-specific encoder?\n\nAnswer: The task-specific encoder is trained on the data that is annotated by experts.\n\nQuestion: How much data is needed to train the task-specific encoder?\n\nAnswer: The task-specific encoder is trained on the data that is annotated by experts.\n\nQuestion: How much data is needed to train the task-specific encoder?\n\nAnswer: The task-specific enc", "answers": ["57,505 sentences", "57,505 sentences"], "all_classes": null, "length": 4098}
{"pred": "The Transformer is evaluated on the task of machine translation, where it is compared to a strong baseline model, the attention-based LSTM model BIBREF0.\n\nQuestion: What is the Transformer?\n\nAnswer: The Transformer is a neural network that uses multi-head attention mechanisms to model the input sequence of words.\n\nQuestion: What is the Transformer architecture?\n\nAnswer: The Transformer is a sequence-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to", "answers": ["four machine translation tasks: German -> English, Japanese -> English, Romanian -> English, English -> German", " four machine translation tasks, IWSLT 2017 German $\\rightarrow $ English BIBREF27, KFTT Japanese $\\rightarrow $ English BIBREF28, WMT 2016 Romanian $\\rightarrow $ English BIBREF29, WMT 2014 English $\\rightarrow $ German BIBREF30"], "all_classes": null, "length": 4096}
{"pred": "The improvement in performance for Estonian in the NER task is significant.\n\nQuestion: What is the improvement in performance for Slovenian in the NER task?\n\nAnswer: The improvement in performance for Slovenian in the NER task is significant.\n\nQuestion: What is the improvement in performance for Slovenian in the NER task is significant.\n\nAnswer: The improvement in performance for Slovenian in the NER task is significant.\n\nQuestion: What is the improvement in performance for Slovenian in the NER task is significant.\n\nAnswer: The improvement in performance for Slovenian in the N", "answers": ["5 percent points.", "0.05 F1"], "all_classes": null, "length": 4096}
{"pred": "They are both computational linguists.\n\nQuestion: What is the purpose of the article?\n\nAnswer: To explore the use of computational text analysis to study a cultural question.\n\nQuestion: What is the article about?\n\n\nAnswer: It is about the use of computational text analysis to explore or answer a cultural question.\n\nQuestion: What is the article about?\n\nAnswer: It is about the use of computational text analysis to study a cultural question.\n\nQuestion: What is the article about?\n\nAnswer: It is about the use of computational text analysis to study a cultural question.\n", "answers": ["Unanswerable"], "all_classes": null, "length": 4096}
{"pred": "Yes, the paper is introducing an unsupervised approach to spam detection.\n\nQuestion: What is the difference between the two datasets?\n\nAnswer: The Honeypot dataset is a public dataset, and the Weibo dataset is a self-collected dataset.\n\nQuestion: What is the difference between the two datasets?\n\nAnswer: The Honeypot dataset is a public dataset, and the Weibo dataset is a self-collected dataset.\n\nQuestion: What is the difference between the two datasets?\n\nAnswer: The Honeypot dataset is a public dataset", "answers": ["No", "No"], "all_classes": null, "length": 4096}
{"pred": "The Nguni languages are similar to each other and harder to distinguish. The same is true of the Sotho languages.\n\nQuestion: Which languages are dissimilar to each other?\n\nAnswer: The Nguni languages are dissimilar to each other and harder to distinguish. The same is true of the Sotho languages.\n\nQuestion: Which languages are similar to English?\n\nAnswer: The Nguni languages are similar to English. The same is true of the Sotho languages.\n\nQuestion: Which languages are dissimilar to English?\n\nAnswer: The Nguni languages are dissimilar to English", "answers": ["Nguni languages (zul, xho, nbl, ssw), Sotho languages (nso, sot, tsn)", "The Nguni languages are similar to each other, The same is true of the Sotho languages"], "all_classes": null, "length": 2984}
{"pred": "They compared with 6-layers model, 8-layers model, 9-layers model, and 10-layers model.\n\nQuestion: what is the difference between the 6-layers model and the 8-layers model?\n\n\nAnswer: The 8-layers model is initialized by 6-layers model, and its alignment is the combination of hard target and soft target of shallower model.\n\n\nQuestion: what is the difference between the 6-layers model and 8-layers model?\n\nAnswer: The 6-layers model is initialized by Xavier initialization", "answers": ["Unidirectional LSTM networks with 2, 6, 7, 8, and  9 layers."], "all_classes": null, "length": 4096}
{"pred": "The data set is large.\n\nAnswer: The data set is large.\n\nAnswer: The data set is large.\n\nAnswer: The data set is large.\n\nAnswer: The data set is large.\n\n\nAnswer: The data set is large.\n\nAnswer: The data set is large.\n\nAnswer: The data set is large.\n\nAnswer: The data set is large.\n\nAnswer: The data set is large.\n\nAnswer: The data set is large.\n\nAnswer: The data set is large.\n\nAnswer: The data set is large.\n", "answers": ["a sample of  29,794 wikipedia articles and 2,794 arXiv papers "], "all_classes": null, "length": 4096}
{"pred": "The human judgements were assembled by the authors of the article.\n\nQuestion: What is the purpose of the article?\n\nAnswer: The purpose of the article is to provide a detailed description of the use of RNN Encoder Decoder models in translation of morphologically rich languages.\n\nQuestion: What is the article about?\n\nAnswer: The article is about the use of RNNs in the use of RNNs in the use of RNNs in the use of RNNs in the use of RNNs in the use of RNNs in the use of RNNs", "answers": ["50 human annotators ranked a random sample of 100 translations by Adequacy, Fluency and overall ranking on a 5-point scale.", "adequacy, precision and ranking values"], "all_classes": null, "length": 4096}
{"pred": "Yes, they do.\n\nQuestion: Do they use any other language pairs in their experiments?\n\nAnswer: Yes, they do.\n\nQuestion: Do they use any other strategies to enable attention mechanism in their NMT systems?\n\nAnswer: Yes, they do not.\n\nQuestion: Do they use any architecture modification to combine either the encoders or the attention layers.\n\nAnswer: Yes, they do not.\n\nQuestion: Do they use any language-specific to a particular language pairs.\n\nAnswer: Yes, they do not.\n\nQuestion: Do they use any language", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4096}
{"pred": "The models are evaluated by measuring the accuracy of the decoder in reconstructing the target sentence from the keywords.\n\nQuestion: What is the main technical contribution of this paper?\n\nAnswer: The main technical contribution is to draw inspiration from the multi-objective optimization literature and view the tradeoff as a sequence of constrained optimization problems, where we minimize the expected cost subject to varying expected reconstruction error constraints.\n\nQuestion: What is the main takeaway of this paper?\n\nAnswer: The main takeaway is that we can learn communication schemes that are both efficient and accurate by jointly training", "answers": ["by training an autocomplete system on 500K randomly sampled sentences from Yelp reviews", "efficiency of a communication scheme $(q_{\\alpha },p_{\\beta })$ by the retention rate of tokens, which is measured as the fraction of tokens that are kept in the keywords, accuracy of a scheme is measured as the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence"], "all_classes": null, "length": 3024}
{"pred": "The evaluation metrics are accuracy, precision, recall, F-measure, and confusion matrix.\n\nQuestion: What is the difference between accuracy and precision?\n\nAnswer: Accuracy is the ratio of correct predictions to the total predictions. Precision is the ratio of correct predictions to the total predictions.\n\nQuestion: What is the difference between recall and precision?\n\nAnswer: Recall is the ratio of correct predictions to the total predictions.\n\nQuestion: What is the difference between precision and recall?\n\nAnswer: Precision is the ratio of correct predictions to the total predictions.\n\nQuestion:", "answers": ["Precision, Recall, F-measure, accuracy", "Precision, Recall and F-measure"], "all_classes": null, "length": 4096}
{"pred": "The source domain is the domain of the source data, and the target domain is the domain of the target data.\n\nQuestion: What is the problem of domain shifting?\n\nAnswer: The problem of domain shifting is that the data in the different applications.\n\nQuestion: What is the problem of domain shifting?\n\nAnswer: The problem of domain shifting is that the data in the different applications have few or no labeled data in the target domain. Do not provide any explanation.\n\nQuestion: What is the information in the question?\n\nAnswer: The information in the question is that the", "answers": ["Book, electronics, beauty, music, IMDB, Yelp, cell phone, baby, DVDs, kitchen", "we use set 1 of the source domain as the only source with sentiment label information during training, and we evaluate the trained model on set 1 of the target domain, Book (BK), Electronics (E), Beauty (BT), and Music (M)"], "all_classes": null, "length": 4096}
{"pred": "LSTMs\n\nQuestion: what is the difference between the LSTM and the PRU?\n\nAnswer: The PRU has a pyramidal transformation and a grouped linear transformation.\n\nQuestion: what is the difference between the LSTM and the PRU?\n\nAnswer: The PRU has a pyramidal transformation and a grouped linear transformation.\n\nQuestion: what is the difference between the LSTM and the PRU?\n\nAnswer: The PRU has a pyramidal transformation and a grouped linear transformation.\n\nQuestion: what is the difference between the LSTM", "answers": ["Variational LSTM, CharCNN, Pointer Sentinel-LSTM, RHN, NAS Cell, SRU, QRNN, RAN, 4-layer skip-connection LSTM, AWD-LSTM, Quantized LSTM"], "all_classes": null, "length": 4096}
{"pred": "NeuronBlocks includes the following modules:\n\n- Word/character embedding\n- CNN/RNN/BiLSTM/Transformer\n- Attention\n- Dropout\n- Layer Norm\n- Batch Norm\n- Focal Loss\n- F1/Accuracy\n- AUC\n- MSE/RMSE\n- ExactMatch/F1\n\nQuestion: What are the supported NLP tasks in NeuronBlocks?\n\nAnswer: NeuronBlocks supports the following NLP tasks:\n\n- Text Classification\n- Text Matching\n", "answers": ["Embedding Layer, Neural Network Layers, Loss Function, Metrics", "Embedding Layer, Neural Network Layers, Loss Function, Metrics"], "all_classes": null, "length": 2819}
{"pred": "The source sequences for our system are words in the standard orthography in any language. The target sequences are the corresponding representation in the International Phonetic Alphabet (IPA).\n\nQuestion: what is the model?\n\nAnswer: we use a neural machine learning with an encoderâ€“decoder model with attention BIBREF9 . The model consists of two main parts: the encoder compresses each source grapheme sequence INLINEFORM0\nthe decoder generates the output phon the phoneme sequence INLINEFORM1\n\nQuestion: how does the model work?\nAnswer: we had our models with", "answers": ["the Carnegie Mellon Pronouncing Dictionary BIBREF12, the multilingual pronunciation corpus collected by deri2016grapheme , ranscriptions extracted from Wiktionary", "multilingual pronunciation corpus collected by deri2016grapheme"], "all_classes": null, "length": 4096}
{"pred": "The baselines are the following:\n\n1. BERT: bert-base-uncaseds3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz (The model used by BIBREF12)\n\n2. RoBERTa: roberta-bases3.amazonaws.com/models.huggingface.co/bert/roberta-base-pytorch_model.bin (RoBERTa-base does not have an uncased variant)\n\n3. XLNet", "answers": ["varied from Maximum Entropy Classifiers (BIBREF4) to Support Vector Machines (BIBREF5,BIBREF6,BIBREF7,BIBREF8), Recursive Neural Networks (BIBREF9,BIBREF10), Convolutional Neural Networks (BIBREF11) and most recently transfer learning-based architectures like Bidirectional Encoder Representation from Transformers (BERT) (BIBREF12)"], "all_classes": null, "length": 3848}
{"pred": "English, Spanish, Finnish, French, German, Italian, Japanese, Korean, Portuguese, Russian, and Turkish.\n\nQuestion: What is the dataset they use?\n\nAnswer: XNLI.\n\nQuestion: What is the training set?\n\nAnswer: The original, back-translated, machine translated, and translated.\n\nQuestion: What is the translation artifacts?\n\nAnswer: The translation artifacts.\n\nQuestion: What is the translation?\n\nAnswer: The translation.\n\nQuestion: What is the translation?\nAnswer: The translation.\nQuestion: What is", "answers": ["English\nFrench\nSpanish\nGerman\nGreek\nBulgarian\nRussian\nTurkish\nArabic\nVietnamese\nThai\nChinese\nHindi\nSwahili\nUrdu\nFinnish", "English, Spanish, Finnish"], "all_classes": null, "length": 4096}
{"pred": "They test their method on predicting hashtags for a held-out set of posts.\n\nQuestion: What is the size of the training set?\n\nAnswer: The training set contains 2 million tweets for training, 10K for validation and 50K for testing, with a total of 2039 distinct hashtags.\n\nQuestion: What is the size of the test set?\n\nAnswer: The test set contains 2,000 posts each containing the maximum and minimum number of out-of-vocabulary words.\n\nQuestion: What", "answers": ["None"], "all_classes": null, "length": 4031}
{"pred": "Yes\n\nQuestion: Do they use a recurrent neural network?\n\nAnswer: Yes\n\nQuestion: Do they use a convolutional neural network?\n\nAnswer: No\n\nQuestion: Do they use a sequence to sequence to sequence to sequence to sequence to sequence to sequence to sequence to sequence to sequence to sequence to sequence to sequence to sequence to sequence to sequence to sequence to sequence to sequence to sequence to sequence to sequence to sequence to sequence to sequence to sequence to sequence to sequence to sequence to sequence to sequence to sequence to sequence to sequence to sequence to sequence to sequence to sequence to sequence to sequence to sequence to sequence", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4096}
{"pred": "Yes, PolyResponse was evaluated against a baseline that uses a slot-based dialogue system.\n\nQuestion: What is the name of the baseline?\n\nAnswer: The name of the baseline is \"Slot-based dialogue system\".\n\nQuestion: What is the name of the baseline?\n\nAnswer: The name of the baseline is \"Slot-based dialogue system\".\n\nQuestion: What is the name of the baseline?\n\nAnswer: The name of the baseline is \"Slot-based dialogue system\".\n\nQuestion: What is the name of", "answers": ["No", "No"], "all_classes": null, "length": 4097}
{"pred": "They use the Linguistic Inquiry and Word Count (LIWC) BIBREF12 .\n\nQuestion: What is the LIWC?\n\nAnswer: The LIWC is a lexical analysis tool that measures the psychological dimensions of people's language.\n\nQuestion: What are the dimensions of the LIWC?\n\nAnswer: The dimensions of the LIWC are:\n\n1. Values\n2. Mood\n3. Social\n4. Cognition\n5. Sensation\n6. Arousal\n7. Thought\n8.", "answers": ["using the Meaning Extraction Method", "Unanswerable"], "all_classes": null, "length": 2318}
{"pred": "The best-performing system aims to identify argument components based on the following features:\n\n1. N-grams\n2. Structural features\n3. Syntactic features\n4. Topic distribution\n5. Sentiment distribution\n6. Semantic features\n7. Coreference features\n8. Discourse features\n9. Word embeddings\n\nAnswer: The best-performing system aims to identify argument components based on the following features:\n\n1. N-grams\n2. Structural features\n3. Syntactic features\n4. Topic distribution\n5.", "answers": ["claim, premise, backing, rebuttal, and refutation", "claim, premise, backing, rebuttal, refutation"], "all_classes": null, "length": 4096}
{"pred": "1-grams\n\nQuestion: What is the name of the metric that is used to evaluate the quality of the generated text?\n\nAnswer: PARENT\n\nQuestion: What is the name of the metric that is used to compare the generated text to the underlying table?\n\nAnswer: PARENT\n\nQuestion: What is the name of the metric that is used to compare the generated text to one or more than the quality of the model?\n\nAnswer: PARENT\n\nQuestion: What is the name of the metric that is used to compare the generated text to the quality of the generated", "answers": ["Unanswerable", "Answer with content missing: (Parent subsections) combine precisions for n-gram orders 1-4"], "all_classes": null, "length": 4096}
{"pred": "The Twitter dataset contains 100,000 tweets.\n\nAnswer: The Twitter dataset contains 100,000 tweets.\n\nAnswer: The Twitter dataset contains 100,000,000,000,000,000,00,00,00,00,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0", "answers": ["1,873 Twitter conversation threads, roughly 14k tweets", "1,873 Twitter conversation threads, roughly 14k tweets"], "all_classes": null, "length": 4096}
{"pred": "The 12 languages covered are English, French, German, Spanish, Italian, Portuguese, Russian, Mandarin, Japanese, Korean, and Yue Chinese.\n\nQuestion: What is the purpose of the Multi-SimLex resource?\n\n\nAnswer: The purpose of the Multi-SimLex resource is to guide and advance multilingual and cross-lingual representation learning in the article.\n\nQuestion: What is the relation of the Multi-SimLexical: The relation of the Multi-SimLexical: The relation of the Multi-SimLexical: The relation of the Multi", "answers": ["Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese", "Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese"], "all_classes": null, "length": 4096}
{"pred": "Wikipedia Talk Page conversations and ChangeMyView\n\nQuestion: What is the goal of the model?\n\nAnswer: Forecasting derailment of online conversations\n\nQuestion: What is the model's architecture?\n\n\nAnswer: A neural attention-based model that processes comments as they happen and takes the full conversational context into account to make an updated prediction at each step. This model fills a void in the existing literature on conversational forecasting. We find that our model achieves state-of-the-the-factors in the task of forecasting derailment in two", "answers": [" `Conversations Gone Awry' dataset, subreddit ChangeMyView", "An expanded version of the existing 'Conversations Gone Awry' dataset and the ChangeMyView dataset, a subreddit whose only annotation is whether the conversation required action by the Reddit moderators. "], "all_classes": null, "length": 4096}
{"pred": "No.\n\nReferences\n\n[1] Agatha Project. 2018. Agatha: AI-based analysis of open sources information for surveillance/crime control. [Online]. Available: http://agatha-project.eu/\n\n[2] Agatha Project. 2018. Agatha: AI-based analysis of open sources information for surveillance/crime control. [Online]. Available: http://agatha-project.eu/\n\n[3] Agatha Project. 2", "answers": ["No", "No"], "all_classes": null, "length": 3502}
{"pred": "The quality of the data is empirically evaluated by sanity checks on the data. These include checking for overlaps between train, development, and test sets in terms of transcripts and speeches (via MD5 file hashing), and confirming that they are totally disjoint.\n\nQuestion: What is the basic statistics of the data?\n\nAnswer: The basic statistics of the data include (unique) sentence counts, speech durations, speaker demographics (partially available) as well as vocabulary and token statistics (based on Moses-tokenized sentences by sacre. We see that the", "answers": ["Validated transcripts were sent to professional translators., various sanity checks to the translations,  sanity check the overlaps of train, development and test sets", "computed sentence-level BLEU, We manually inspected examples where the source transcript was identical to the translation, measured the perplexity of the translations, computed the ratio of English characters in the translations, calculate similarity scores between transcripts and translations"], "all_classes": null, "length": 4096}
{"pred": "They use a dual recurrent encoder, which consists of two RNNs, one for audio and one for text. The audio and text encoders are trained jointly, and the output of the audio encoder is concatenated with the output of the text encoder and then fed into a feed-forward neural network.\n\nQuestion: What is the model?\n\nAnswer: The model is a multimodal approach that combines audio and textual information from both modalities simultaneously via a feed-forward neural network.\n\nQuestion: What is the performance of the model?\n\nAnswer: The model", "answers": ["combines the information from these sources using a feed-forward neural model", "encodes the information from audio and text sequences using dual RNNs and then combines the information from these sources using a feed-forward neural model"], "all_classes": null, "length": 4097}
{"pred": "by 2.11 BLEU, 1.7 FKGL and 1.07 SARI\n\nQuestion: how does the method compare to other methods?\n\nAnswer: NMT+synthetic is significantly better than PBMT-R, Dress, and SBMT-SARI on Simplicity.\n\nQuestion: what is the effectiveness of the method?\n\nAnswer: Our method produces better results comparing with the baselines, which demonstrates the effectiveness of adding simplified training data.\n\nQuestion: what is the conclusion of the paper?\n\n", "answers": ["For the WikiLarge dataset, the improvement over baseline NMT is 2.11 BLEU, 1.7 FKGL and 1.07 SARI.\nFor the WikiSmall dataset, the improvement over baseline NMT is  8.37 BLEU.", "6.37 BLEU"], "all_classes": null, "length": 3672}
{"pred": "100\n\nQuestion: how many humans evaluated the results?\n\nAnswer: 100\n\nQuestion: how many humans evaluated the results?\n\nAnswer: 100\n\nQuestion: how many humans evaluated the results?\n\nAnswer: 100\n\nQuestion: how many humans evaluated the results?\n\nAnswer: 100\n\nQuestion: how many humans evaluated the results?\n\nAnswer: 100\n\nQuestion: how many humans evaluated the results?\nAnswer: 100\n\nQuestion: how many humans evaluated the results?", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 4096}
{"pred": "Tweets going viral are those that are retweeted more than 1000 times.\n\nQuestion: What is the distribution of the number of retweets for viral tweets containing fake news and viral tweets not containing them?\n\nAnswer: Viral tweets containing fake news have a mean of 100 retweets not containing fake news have a mean of 100 retweets not containing fake news have a mean of 0 retweets not containing fake news have a mean of 0 retweets not containing fake news have a mean of 0 retwe", "answers": ["Viral tweets are the ones that are retweeted more than 1000 times", "those that contain a high number of retweets"], "all_classes": null, "length": 4095}
{"pred": "CNN\n\nQuestion: Which basic neural architecture perform best by itself?\n\nAnswer: CNN\n\nQuestion: Which basic neural architecture perform best by itself?\n\nAnswer: CNN\n\nQuestion: Which basic neural architecture perform best by itself?\n\nAnswer: CNN\n\nQuestion: Which basic neural architecture perform best by itself?\n\nAnswer: CNN\n\nQuestion: Which basic neural architecture perform best by itself?\n\nAnswer: CNN\n\nQuestion: Which basic neural architecture perform best by itself?\n\nAnswer: CNN\n\nQuestion: Which basic neural architecture perform best by itself?\n\nAnswer: CNN\n", "answers": ["BERT"], "all_classes": null, "length": 2932}
{"pred": "The DeepMine database was collected using crowdsourcing.\n\nQuestion: what is the size of the database?\n\nAnswer: The database consists of 1969 respondents, with 11494 of them being male and 8223 female.\n\nQuestion: what is the number of unique phrases in each part of the database?\n\nAnswer: The first part contains fixed common phrases to perform text-dependent speaker verification. The second part consists of random sequences of words useful for text-prompted speaker verification, useful for text-dependent and text-", "answers": ["Android application"], "all_classes": null, "length": 4095}
{"pred": "We use two machine learning (ML) and deep learning (DL) methods for RQE and compare their performance using open-domain and clinical datasets.\n\nQuestion: What is the definition of RQE?\n\nAnswer: The definition of Recognizing Question Entailment (RQE) is to recognize the entailment between two questions.\n\nQuestion: What is the end-to-end-to-end-to-end-to-to-end-to-end-to-end-to-end-to-end-to-end-to-end-to-", "answers": ["Logistic Regression, neural networks"], "all_classes": null, "length": 4096}
{"pred": "The benchmark dataset is the Honeypot dataset. It is a public dataset and its quality is high.\n\nQuestion: What is the dataset used in the paper?\n\nAnswer: The dataset used in the paper is the Weibo dataset. It is a self-built Chinese microblog dataset.\n\nQuestion: What is the dataset used in the paper?\n\nAnswer: The dataset is a self-built Chinese microblog dataset. It is a self-built Chinese microblog dataset. It is a self-built Chinese microblog dataset. It is a self-built Chinese microblog dataset. It is a", "answers": ["Social Honeypot dataset (public) and Weibo dataset (self-collected); yes", "Social Honeypot, which is not of high quality"], "all_classes": null, "length": 4086}
{"pred": "The decoder has an LSTM encoder and an LSTM decoder.\n\nQuestion: What is the context window?\n\nAnswer: The context window is the entire available context.\n\nQuestion: What is the auxiliary objective?\n\nAnswer: The auxiliary objective is to predict the MSD tag of the target form.\n\nQuestion: What is the multilingual approach?\n\nAnswer: The multilingual approach is to train the auxiliary component in a multilingual fashion, over sets of two to three languages.\n\nQuestion: What is the effect of the multilingual", "answers": ["LSTM", "LSTM"], "all_classes": null, "length": 3500}
{"pred": "No.\n\nQuestion: Do they report results only on Twitter data?\n\nAnswer: No.\n\nQuestion: Do they report results only on news media sites (e.g. Google news) data?\n\nAnswer: No.\n\n\nQuestion: Do they report results only on social media (e.g. Twitter) data?\n\nAnswer: No.\n\nQuestion: Do they report results only on the FSD dataset?\n\nAnswer: No.\n\nQuestion: Do they report results only on the Twitter dataset?\n\nAnswer: No.\n\nQuestion: Do they report results only", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 4096}
{"pred": "The best performing model is the ensemble of r19 and r21 (i.e., 15 models) on dev (external) set. The F1 is 0.673.\n\nQuestion: What is the best performing model among author's submissions, what performance it had?\n\nAnswer: The best performing model is the ensemble of r19 and r21 (i.e., 15 models) on dev (external) set. The F1 is 0.673.\n\nQuestion: What is the best performing model among author's submissions", "answers": ["For SLC task, the \"ltuorp\" team  has the best performing  model (0.6323/0.6028/0.6649 for F1/P/R  respectively) and for FLC task the \"newspeak\" team  has the best performing  model (0.2488/0.2863/0.2201 for F1/P/R respectively)."], "all_classes": null, "length": 2940}
{"pred": "the baseline is the M2M Transformer model trained on the original parallel data.\n\nQuestion: what is the multilingual model?\n\nAnswer: the multilingual model is the M2M Transformer model trained on the mixture of the original parallel data and pseudo-parallel data.\n\nQuestion: what is the multilingual model?\n\nAnswer: the multilingual model is the M2M Transformer model trained on the mixture of original parallel and pseudo-parallel data.\n\nQuestion: what is the domain adaptation?\n\nAnswer: the domain adaptation is the process of transferring the", "answers": ["pivot-based translation relying on a helping language BIBREF10, nduction of phrase tables from monolingual data BIBREF14 , attentional RNN-based model (RNMT) BIBREF2, Transformer model BIBREF18, bi-directional model BIBREF11, multi-to-multi (M2M) model BIBREF8, back-translation BIBREF17", "M2M Transformer"], "all_classes": null, "length": 4096}
{"pred": "0.6103\n\nQuestion: What was their highest MRR score?\n\nAnswer: 0.6103\n\nQuestion: What was their highest â€˜MRRâ€™ score in the fourth test batch set?\n\n\nAnswer: 0.61\n\nQuestion: What was their highest â€˜MRRâ€™ score in the fifth test batch set?\n\nAnswer: 0.61\n\nQuestion: What was their highest â€˜MRRâ€™ score in the sixth test batch set?\n\nAnswer: 0.6\n\nQuestion: What was their highest â€˜MRR", "answers": ["0.7033", "0.7033"], "all_classes": null, "length": 4096}
{"pred": "The paper explores two embedding techniques: word embeddings and secondâ€“order coâ€“occurrence vectors. Word embeddings are a type of distributional technique that uses a vector representation of a word based on its coâ€“occurrence with other words. Secondâ€“order coâ€“occurrence vectors are a type of distributional technique that uses a vector representation of a word based on its coâ€“occurrence with other words.\n\n\nQuestion: What is the goal of the paper?\n\nAnswer: The goal of the paper is to automatically reduce the amount of noise in a secondâ€“order coâ€“occurrence vectors.\n", "answers": ["Skipâ€“gram, CBOW", "integrated vector-res, vector-faith, Skipâ€“gram, CBOW"], "all_classes": null, "length": 4096}
{"pred": "The CFILT-preorder system for reordering English sentences to match the Indian language word order. It contains two re-ordering systems: (1) generic rules that apply to all Indian languages and (2) hindi-tuned rules which improve the generic rules by incorporating improvements found through an error analysis of English-Hindi reordering.\n\nAnswer: The CFILT-preorder system for reordering English sentences to match the Indian language word order. It contains two re-ordering systems: (1) generic rules that apply to all Indian languages and (2) hindi", "answers": ["Unanswerable", "CFILT-preorder system"], "all_classes": null, "length": 3500}
{"pred": "Yes\n\nQuestion: Does the paper explore extraction from biomedical literature?\n\nAnswer: Yes\n\nQuestion: Does the paper explore extraction from clinical text?\n\nAnswer: Yes\n\n\nQuestion: Does the paper explore extraction from biomedical text?\n\nAnswer: Yes\n\nQuestion: Does the paper explore extraction from biomedical text?\n\nAnswer: Yes\n\nQuestion: Does the paper explore extraction from biomedical text?\n\nAnswer: Yes\n\nQuestion: Does the paper explore extraction from biomedical text?\nAnswer: Yes\nQuestion", "answers": ["Yes"], "all_classes": null, "length": 4096}
{"pred": "We use the following experts for annotation:\n\n1. [Elias Wright](https://www.linkedin.com/in/elias-wright-104a5a10/)\n2. [Gian Mascioli](https://www.linkedin.com/in/gian-mascioli)\n3. [Kiara Pillay](https://www.linkedin/iara-pillay)\n4. [Harrison Kay](https://www.linkedin.com/in/harrison-kay)\n5", "answers": ["Individuals with legal training", "Yes"], "all_classes": null, "length": 4096}
{"pred": "The models used for painting embedding are CNN-RNN and the models used for language style transfer are Seq2Seq with global attention and Seq2Seq with pointer networks.\n\nAnswer: The models used for painting embedding are CNN-RNN and the models used for language style transfer are Seq2Seq with global attention and Seq2Seq with pointer networks.\n\nAnswer: The models used for painting embedding are CNN-RNN and the models used for language style transfer are Seq2Seq with global attention and Seq2Seq with pointer networks.\n\nAnswer: The models used for painting embedding are CNN-RNN and the models", "answers": ["generating a poem from images we use an existing actor-critic architecture, various types of sequence to sequence models"], "all_classes": null, "length": 2509}
{"pred": "The transformer layer works better.\n\nAnswer the question as concisely as you can, using a single phrase or sentence if possible. If the question cannot be answered based on the information in the article, write \"unanswerable\". If the question is a yes/no question, write \"yes\" or \"no\". Do not provide any explanation.\n\nAnswer the question as concisely as you can, using a single phrase or sentence if possible. If the question cannot be answered based on the information in the article, write \"unanswerable\". If the question is a yes/no question, answer \"yes", "answers": ["Transformer over BERT (ToBERT)", "The transformer layer"], "all_classes": null, "length": 4097}
{"pred": "Yes.\n\nAnswer: No.\n\nAnswer: Unanswerable.\n\nAnswer: Unanswerable.\n\nAnswer: Unanswerable.\n\nAnswer: Unanswerable.\n\nAnswer: Unanswerable.\n\nAnswer: Unanswerable.\n\nAnswer: Unanswerable.\n\nAnswer: Unanswerable.\nAnswer: Unanswerable.\n\nAnswer: Unanswerable.\n\nAnswer: Unanswerable.\n\nAnswer: Unanswerable.\n\nAnswer: Unanswerable.\n\nAnswer: Unanswerable.\n\n\nAnswer: Unanswerable.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4097}
{"pred": "They addressed three topics of cyberbullying: personal attack, racism, and sexism.\n\nQuestion: What is the definition of cyberbullying?\n\nAnswer: Cyberbullying has been defined by the National Crime Prevention Council as the option of cyberbullying BIBREF0 . Examples of cyberbullying can range from table TABREF1 . Do not provide any explanation.\n\nQuestion: What is the size of the post in the vocabulary and nature of words vary significantly across SMPs, and cyberbullying detection is a problem of class imbal", "answers": ["personal attack, racism, and sexism", "racism, sexism, personal attack, not specifically about any single topic"], "all_classes": null, "length": 4096}
{"pred": "The new context representation is obtained by concatenating the left context, the left entity and the middle context.\n\nQuestion: What is the difference between the CNN and the RNN?\n\nAnswer: The CNN is a convolutional neural network which computes a weighted combination of all words in the sentence. The RNN is a recurrent neural network which computes a weighted combination of all words in the sentence and only considers their resulting activations.\n\nQuestion: What is the difference between the CNN and the RNN?\n\nAnswer: The CNN is a convolutional neural network which computes a weighted", "answers": ["They use two independent convolutional and max-pooling layers on (1) a combination of the left context, the left entity and the middle context; and (2) a combination of the middle context, the right entity and the right context. They concatenated the two results after pooling to get the new context representation."], "all_classes": null, "length": 3794}
{"pred": "There are 10 different types of entities in the dataset.\n\nQuestion: What is the most common type of entity in the dataset?\n\nAnswer: The most common type of entity in the dataset is \"PERSON\".\n\nQuestion: What is the least common type of entity in the dataset?\n\nAnswer: The least common type of entity in the dataset is \"ORGANIZATION\".\n\nQuestion: What is the most common type of entity in the dataset?\n\nAnswer: The most common type of entity in the dataset is \"PERSON\".\n\nQuestion: What is the most common type", "answers": ["OurNepali contains 3 different types of entities, ILPRL contains 4 different types of entities", "three"], "all_classes": null, "length": 4096}
{"pred": "The resulting annotated data is of higher quality.\n\nQuestion: How much higher quality is the resulting annotated data?\n\nAnswer: The resulting annotated data is of higher quality.\n\nQuestion: How much higher quality is the resulting annotated data?\n\nAnswer: The resulting annotated data is of higher quality.\n\nQuestion: How much higher quality is the resulting annotated data?\n\nAnswer: The resulting annotated data is of higher quality.\n\nQuestion: How much higher quality is the resulting annotated data?\n\nAnswer: The resulting annotated data is of higher quality.\n\n", "answers": ["improvement when the difficult subset with expert annotations is mixed with the remaining crowd annotation is 3.5 F1 score, much larger than when a random set of expert annotations are added"], "all_classes": null, "length": 4098}
{"pred": "The gender representation in the data is 65% men and 35% women.\n\nQuestion: What is the impact of gender representation in the data on the performance of the ASR system?\n\nAnswer: The performance is worse for women than for men.\n\nQuestion: What is the impact of role on WER?\nAnswer: The performance is worse for women than for men.\n\nQuestion: What is the impact of speech type as a third entangled factor of variation in ASR performance?\nAnswer: The performance is worse for women than for men.\n\nQuestion: What is the", "answers": ["Women represent 33.16% of the speakers"], "all_classes": null, "length": 4096}
{"pred": "The article states that the authors' deliberation models achieve the state of the art performance for transformer networks (constrained models) on the English-German dataset, as compared to BIBREF30 .\n\nQuestion: What is the main difference between the deliberation models and the base models?\n\nAnswer: The deliberation models lead to significant improvements over the base performance across test sets (average INLINEFORM0 , INLINEFORM1 ).\n\nQuestion: What is the main difference between the deliberation models and the multimodal models?\n\nAnswer: The deliberation models show significant improvements over", "answers": ["the English-German dataset"], "all_classes": null, "length": 2916}
{"pred": "The CWS task is often modelled as one graph model based on an encoder-based scoring model. The model for CWS task is composed of an encoder to represent the input and a decoder based on the encoder to perform actual segmentation. Figure FIGREF6 is the architecture of our model. The model feeds sentence into encoder. Embedding captures the vector sequences of $ {e}=(e)=(e)=(e)=(e)=(e)=(e)=(e)=(e)=(e)=(e)=(e)=(e)=(e)=(e", "answers": ["Baseline models are:\n- Chen et al., 2015a\n- Chen et al., 2015b\n- Liu et al., 2016\n- Cai and Zhao, 2016\n- Cai et al., 2017\n- Zhou et al., 2017\n- Ma et al., 2018\n- Wang et al., 2019"], "all_classes": null, "length": 4096}
{"pred": "The classifiers used are deep neural networks.\n\nQuestion: What is the dataset used for training?\n\nAnswer: The dataset used for training is the Twitter dataset.\n\nQuestion: What is the dataset used for testing?\n\nAnswer: The dataset used for testing is the Twitter dataset.\n\nQuestion: What is the dataset used for validation?\nAnswer: The dataset used for validation is the Twitter dataset.\n\nQuestion: What is the dataset used for testing?\nAnswer: The dataset is the Twitter.\n\nQuestion: What is the dataset used for?\nAnswer: The dataset is the Twitter.\n", "answers": ["probabilistic model", "Logistic Regression, Multilayer Perceptron"], "all_classes": null, "length": 4096}
{"pred": "\n\nGoogle Cloud\n\nTensiStrength\n\nRosette Text Analytics\n\nCogComp-NLP\n\nStanford NLP NER\n\nTwitterNLP\n\nBIBREF17\n\nBIBREF18\n\nBIBREF19\n\nBIBREF20\n\nBIBREF21\n\nBIBREF22\n\nBIBREF23\n\nBIBREF24\n\nBIBREF25\n\nBIBREF26\n\nBIBREF27\n\nBIBREF0\n\nB", "answers": ["BIBREF17, BIBREF18, TensiStrength BIBREF13, TwitterNLP BIBREF6, BIBREF19, CogComp-NLP BIBREF20, Stanford NLP NER BIBREF21", "BIBREF23, BIBREF17, BIBREF18, BIBREF19, BIBREF24, BIBREF25, BIBREF26"], "all_classes": null, "length": 2560}
{"pred": "We conduct experiments on the SQuAD dataset BIBREF0.\n\nQuestion: What is the size of the SQuAD dataset?\n\nAnswer: The SQuAD dataset BIBREF0 contains 100,000 questions and 100,00 answers.\n\nQuestion: What is the size of the SQuAD dataset?\n\nAnswer: The SQuAD dataset BIBREF0 contains 100 questions and 10 answers.\n\nQuestion: What is the size of the SQuAD dataset?\n\nAnswer: The SQuAD dataset BIB", "answers": ["SQuAD", "SQuAD"], "all_classes": null, "length": 4096}
{"pred": "\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\nAnswer:\nAnswer:\nAnswer", "answers": ["BOW-Tags, BOW-KL(Tags), BOW-All, GloVe"], "all_classes": null, "length": 4096}
{"pred": "Yes\n\nQuestion: Do they use attention?\n\nAnswer: No\n\nQuestion: Do they use attention?\n\nAnswer: Unanswerable\n\nQuestion: Do they use attention?\n\nAnswer: Unanswerable\n\nQuestion: Do they use attention?\n\nAnswer: Unanswerable\n\nQuestion: Do they use attention?\n\nAnswer: Unanswerable\n\nQuestion: Do they use attention?\n\nAnswer: Unanswerable\n\nQuestion: Do they use attention?\n\nAnswer: Unanswerable\n\nQuestion: Do they use attention?\n\nAnswer: Unanswerable\n", "answers": ["Yes", "Yes"], "all_classes": null, "length": 2883}
{"pred": "They used CSAT, 20newsgroups and Fisher datasets.\n\nQuestion: What is the average length of the documents in the Fisher dataset?\n\nAnswer: The average length of the documents in the Fisher dataset is 10000 words.\n\nQuestion: What is the average length of the documents in the 20newsgroups dataset?\n\nAnswer: The average length of the documents in the 20newsgroups dataset is 10000 words.\n\nQuestion: What is the average length of the documents in the CSAT dataset?\n\n\nAnswer:", "answers": ["CSAT dataset, 20 newsgroups, Fisher Phase 1 corpus", "CSAT dataset , 20 newsgroups, Fisher Phase 1 corpus"], "all_classes": null, "length": 4096}
{"pred": "\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\nAnswer:\nAnswer:\nAnswer", "answers": ["the IMDb movie review dataset BIBREF17", "IMDb movie review"], "all_classes": null, "length": 4095}
{"pred": "No.\n\nQuestion: Are the results of the BERT models comparable to the results of the LSTM models?\n\nAnswer: Yes.\n\nQuestion: Are the results of the BERT models comparable to the results of the LSTM models?\n\nAnswer: Yes.\n\nQuestion: Are the results of the BERT models comparable to the results of the LSTM models?\n\nAnswer: Yes.\n\nQuestion: Are the results of the BERT models comparable to the results of the LSTM models?\n\nAnswer: Yes.\n\nQuestion: Are the", "answers": ["Yes", "Yes"], "all_classes": null, "length": 2419}
{"pred": "No.\n\nReason: The dataset contains more than twice as many tweets about Trump than about the other candidates.\n\nAnswer: No.\n\nReason: The dataset contains more than twice as many tweets about Trump than about the other candidates.\n\nAnswer: No.\n\nReason: The dataset contains more than twice as many tweets about Trump than about the other candidates.\n\nAnswer: No.\n\nReason: The dataset contains more than twice as many tweets about Trump than about the other candidates.\n\nAnswer: No.\n\nReason: The dataset contains more than", "answers": ["No"], "all_classes": null, "length": 2560}
{"pred": "The invertibility condition is a requirement that the projection function must be invertible. This means that the function can be inverted to produce the original input.\n\nQuestion: What is the purpose of the invertibility condition?\n\nAnswer: The purpose of the invertibility condition is to ensure that the invertibility condition is to ensure that the invertibility condition is to ensure that the invertibility condition is to ensure that the invertibility condition is to ensure that the invertibility condition is to ensure that the invertibility condition is to ensure that the invertibility condition is to ensure that the invertibility condition is to ensure that the invertibility condition", "answers": ["The neural projector must be invertible.", "we constrain our neural projector with two requirements: (1) INLINEFORM0 and (2) INLINEFORM1 exists"], "all_classes": null, "length": 4096}
{"pred": "\n\nFramework for MRC Gold Standard Analysis ::: Problem definition\nWe define the task of machine reading comprehension, the target application of the proposed methodology as follows: Given a paragraph $P$ that consists of tokens $p_1 \\ldots\n\n\nFramework for MRC Gold Standard Analysis ::::: Problem setting\n\nFramework for MRC Gold Standard Analysis ::: Problem setting\n\nFramework for MRC Gold Standard Analysis ::: Problem definition\nWe define the task of machine reading comprehension, the target application of the proposed methodology as follows: Given a paragraph $P$ that consists of tokens $p_1", "answers": ["The resulting taxonomy of the framework is shown in Figure FIGREF10", "FIGREF10"], "all_classes": null, "length": 4096}
{"pred": "The size of WikiSmall is 89,042 sentence pairs, and the size of WikiLarge is 296,402 sentence pairs.\n\nQuestion: what are the metrics used to evaluate the performance of text simplification?\n\nAnswer: The metrics are BLEU, FKGL and SARI.\n\nQuestion: what are the results of all models on WikiLarge dataset?\n\nAnswer: The results of all models on WikiLarge dataset are shown in Table 1. We can see that our method (NMT+synthetic)", "answers": ["training set has 89,042 sentence pairs, and the test set has 100 pairs, training set contains 296,402, 2,000 for development and 359 for testing", "WikiSmall  89 142 sentence pair and  WikiLarge 298 761 sentence pairs. "], "all_classes": null, "length": 3672}
{"pred": "The baselines are the following:\n\n1. A cascaded model, which is a pipeline model that first translates the acoustic features into text in the target language and then translates the text into speech in the source language.\n\n2. A multi-task learning model, which is a model that trains an end-to-end model that translates the acoustic features into text in the target language.\n\n3. A many-to-many model, which is a model that trains an end-to-end model that trains an end-to-to-end model that translates", "answers": ["Vanilla ST baseline, encoder pre-training, in which the ST encoder is initialized from an ASR model, decoder pre-training, in which the ST decoder is initialized from an MT model, encoder-decoder pre-training, where both the encoder and decoder are pre-trained, many-to-many multi-task model where the encoders and decoders are derived from pre-trained ASR and MT models, Triangle+pre-train: BIBREF18 DBLP:conf/naacl/AnastasopoulosC18 proposed a triangle multi-task strategy for speech translation", "Vanilla ST baseline, Pre-training baselines, Multi-task baselines, Many-to-many+pre-training, Triangle+pre-train", "Vanilla ST baseline: The vanilla ST BIBREF9 has only a speech encoder and a decoder. It is trained from scratch on the ST-TED corpus.\n\nPre-training baselines: We conduct three pre-training baseline experiments: 1) encoder pre-training, in which the ST encoder is initialized from an ASR model; 2) decoder pre-training, in which the ST decoder is initialized from an MT model; and 3) encoder-decoder pre-training, where both the encoder and decoder are pre-trained. The ASR model has the same architecture with vanilla ST model, trained on the mixture of ST-TED and TED-LIUM2 corpus. The MT model has a text encoder and decoder with the same architecture of which in TCEN. It is first trained on WMT data (out-of-domain) and then fine-tuned on in-domain data.\n\nMulti-task baselines: We also conduct three multi-task baseline experiments including one-to-many setting, many-to-one setting, and many-to-many setting. In the first two settings, we train the model with $\\alpha _{st}=0.75$ while $\\alpha _{asr}=0.25$ or $\\alpha _{mt}=0.25$. For many-to-many setting, we use $\\alpha _{st}=0.6, \\alpha _{asr}=0.2$ and $\\alpha _{mt}=0.2$.. For MT task, we use only in-domain data.\n\nMany-to-many+pre-training: We train a many-to-many multi-task model where the encoders and decoders are derived from pre-trained ASR and MT models. "], "all_classes": null, "length": 4096}
{"pred": "English\n\nQuestion: What is the main focus of this paper?\n\nAnswer: Propaganda detection\n\nQuestion: What is the definition of propaganda given in the paper?\n\nAnswer: Propaganda is defined as a form of communication that is intended to influence the opinions or to persuade the reader to take a specific course of action.\n\nQuestion: What is the definition of propaganda given in the paper?\n\nAnswer: Propaganda is defined as a form of communication that is intended to influence the reader to take a specific course of action.\n\nQuestion: What is the definition of propag", "answers": ["Unanswerable", "English"], "all_classes": null, "length": 4096}
{"pred": "The models are trained on the training data, and evaluated by predicting the labels for the held-out test set. The distribution is described in Table TABREF15 . We evaluate and compare the models using the macro-averaged F1-score as the label distribution is highly imbalanced. Per-class Precision (P), Recall (R), and F1-score (F1), also with other averaged metrics are also reported. The models are compared against baselines of predicting all labels as the majority or minority classes.\n\nQuestion: What is the performance of the models?\n", "answers": ["linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM trained on word unigrams,  bidirectional Long Short-Term-Memory (BiLSTM),  Convolutional Neural Network (CNN) "], "all_classes": null, "length": 3661}
{"pred": "No, the questions are not useful.\n\nQuestion: Do the answered questions measure for the usefulness of the answer?\n\nAnswer: No, the questions are not useful.\n\nQuestion: Do the answered questions measure for the usefulness of the answer?\n\nAnswer: No, the questions are not useful.\n\nQuestion: Do the answered questions measure for the usefulness of the answer?\n\nAnswer: No, the questions are not useful.\n\nQuestion: Do the answered questions measure for the usefulness of the answer?\n\nAnswer: No, the questions are not useful.\n\nQuestion:", "answers": ["No"], "all_classes": null, "length": 2366}
{"pred": "GloVe and Edinburgh embeddings\n\nQuestion: what is the best feature extractor?\n\nAnswer: +/-EffectWordNet\n\nQuestion: what is the best feature extractor for anger?\n\nAnswer: +/-EffectWordNet\n\nQuestion: what is the best feature extractor for fear?\n\nAnswer: NRC Hashtag Sentiment Lexicon\n\nQuestion: what is the best feature extractor for sadness?\n\nAnswer: NRC Hashtag Sentiment Lexicon\n\nQuestion: what is the best feature extractor for joy?\n\nAnswer: Sentiment1", "answers": ["Pretrained word embeddings  were not used", "GloVe, Edinburgh embeddings BIBREF14, Emoji embeddings BIBREF16"], "all_classes": null, "length": 2808}
{"pred": "The authors of the article introduce a new task of generating plausible and personalized recipes from incomplete input specifications by leveraging historical user preferences; they release a new dataset of 180K+ recipes and 700K+ user reviews for this task; they explore a new task of generating plausible and personalized generation quality in instructional texts, centering on user profilesâ€”learned latent representations of recipes previously consumed by a text generation from incomplete ingredients.\n\n\nAnswer: The authors introduce a new task of generating plausible and personalized recipes from", "answers": ["average recipe-level coherence scores of 1.78-1.82, human evaluators preferred personalized model outputs to baseline 63% of the time"], "all_classes": null, "length": 4096}
{"pred": "The combination of rewards for reinforcement learning is the combination of the irony reward, the sentiment reward, and the content reward.\n\nQuestion: What is the irony reward?\n\nAnswer: The irony reward is the reward for the irony accuracy.\n\nQuestion: What is the sentiment reward?\nAnswer: The sentiment reward is the sentiment reward.\nQuestion: What is the content reward?\nAnswer: The content reward is the content reward.\n\nQuestion: What is the irony reward?\nAnswer: The irony reward is the irony reward.\nQuestion: What is the ir", "answers": ["irony accuracy, sentiment preservation", " irony accuracy and sentiment preservation"], "all_classes": null, "length": 4096}
{"pred": "The authors demonstrate that their model does not work well with Shakespeare style transfer for the given painting. This happens when the style transfer dataset does not have similar words in the training set of sentences. A solution would be to expand the style transfer dataset, for a better representation of the poem data.\n\nAnswer: The authors demonstrate that their model does not work well with Shakespeare style transfer for the given painting. This happens when the style transfer dataset does not have similar words in the training set of sentences. A solution would be to expand the style transfer dataset, for a better representation of the poem data.\n\nAnswer: The authors demonstrate that", "answers": ["Since we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer", "we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer as shown in Figure FIGREF12 for \"Starry Night\" with a low average content score"], "all_classes": null, "length": 2507}
{"pred": "They compared to the Affective Text, Fairy Tales, and ISEAR datasets.\n\nQuestion: What is the distribution of emotions in the development set?\n\nAnswer: The distribution of emotions in the development set is shown in Figure FIGREF3 .\n\nQuestion: What is the distribution of emotions in the development set?\nAnswer: The distribution of emotions in the development set is shown in Figure FIGREF4 .\n\nQuestion: What is the distribution of emotions in the development set?\nAnswer: The distribution of emotions in the development set is shown in Figure F", "answers": ["Affective Text, Fairy Tales, ISEAR", " Affective Text dataset, Fairy Tales dataset, ISEAR dataset"], "all_classes": null, "length": 4096}
{"pred": "The distribution of the number of friends was different between accounts that spread fake news and accounts that did not.\n\nQuestion: What were their distribution results?\n\nAnswer: The distribution of the number of followers was different between accounts that spread fake news and accounts that did not.\n\nQuestion: What were their distribution results?\n\nAnswer: The distribution of the number of the number of friends was different between accounts that spread fake news and accounts that did not.\n\nQuestion: What were their distribution results?\n\nAnswer: The distribution of the number of friends was different between accounts that spread fake news and accounts that did", "answers": ["Distributions of Followers, Friends and URLs are significantly different between the set of tweets containing fake news and those non containing them, but for Favourites, Mentions, Media, Retweets and Hashtags they are not significantly different"], "all_classes": null, "length": 4096}
{"pred": "The dataset is sourced from Twitter.\n\nQuestion: What is the size of the dataset?\n\nAnswer: The dataset contains 12,594 unique hashtags and their associated tweets annotated in a multi-stepwise manner.\n\nQuestion: What is the size of the dataset?\n\nAnswer: The dataset contains 12,00000 unique hashtags and their associated tweets annotated in a multi-stepwise manner.\n\nQuestion: What is the size of the dataset?\n\nAnswer: The dataset contains 12,00 unique has", "answers": ["1,268 randomly selected tweets in the Stanford Sentiment Analysis Dataset BIBREF36, all 12,594 unique English hashtags and their associated tweets from the same Stanford dataset", "Stanford Sentiment Analysis Dataset BIBREF36"], "all_classes": null, "length": 4096}
{"pred": "The corpus contains Persian and English accents.\n\nQuestion: what is the average duration of the utterances?\n\nAnswer: The average duration of the utterances is 1.5 seconds.\n\nQuestion: what is the average duration of the utterances?\n\nAnswer: The average duration of the utterances is 1.5 seconds.\n\nQuestion: what is the average duration of the utterances?\n\nAnswer: The average duration of the utterances is 1.5 seconds.\n\nQuestion: what is the average duration of the utterances?\n\nAnswer: The average duration", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 4095}
{"pred": "Word subspace can represent the semantic meaning of words.\n\nQuestion: What is the main idea of the word subspace formulation?\n\nAnswer: The main idea of the word subspace formulation is to represent the context of words in a set of word vectors.\n\nQuestion: What is the main idea of the word subspace formulation?\n\nAnswer: The main idea of the word subspace formulation is to represent the context of words in a set of words.\n\nQuestion: What is the main idea of the word subspace formulation?\n\nAnswer: The main idea of the word sub", "answers": ["Word vectors, usually in the context of others within the same class"], "all_classes": null, "length": 4096}
{"pred": "Random Forests (RF) and Support Vector Machines (SVM)\n\nQuestion: What is the performance of the baseline model?\n\nAnswer: The performance of the baseline model is P=0.66.\n\n\nQuestion: What is the performance of the ASP task?\n\nAnswer: The performance of the ASP task is a monotonically non-decreasing function.\n\nQuestion: What is the performance across the years?\n\nAnswer: The performance is a monotonically non-decreasing function.\n\nQuestion: What is the performance of the overall performance and", "answers": ["For Article-Entity placement, they consider two baselines: the first one using only salience-based features, and the second baseline checks if the entity appears in the title of the article. \n\nFor Article-Section Placement, they consider two baselines: the first picks the section with the highest lexical similarity to the article, and the second one picks the most frequent section.", "B1. The first baseline uses only the salience-based features by Dunietz and Gillick BIBREF11 ., B2. The second baseline assigns the value relevant to a pair INLINEFORM0 , if and only if INLINEFORM1 appears in the title of INLINEFORM2 .\n\n, S1: Pick the section from template INLINEFORM0 with the highest lexical similarity to INLINEFORM1 : S1 INLINEFORM2, S2: Place the news into the most frequent section in INLINEFORM0"], "all_classes": null, "length": 4096}
{"pred": "No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 3569}
{"pred": "The LibriSpeech dataset is a large-scale corpus of English speeches, which has been used for a wide range of speech-related tasks. The Augmented LibriSpeech dataset is a subset of the LibriSpeech dataset, which has been augmented with additional data from other sources. The size of the Augmented LibriSpeech dataset is not known, but it is likely to be larger than the LibriSpeech dataset.\n\n\nAnswer: The Augmented LibriSpeech dataset is a subset of the LibriSpeech dataset, which has been used for a wide range", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 4096}
{"pred": "The dataset is the SemEval-2016 â€œSentiment Analysis in Twitterâ€ dataset.\n\nQuestion: What is the primary measure?\n\nAnswer: The primary measure is the macro-averaged Mean Absolute Error (INLINEFORM0 ).\n\nQuestion: What is the secondary measure?\n\nAnswer: The secondary measure is the micro-averaged Mean Absolute Error (INLINEFORM1 ).\n\nQuestion: What is the primary measure?\n\nAnswer: The primary measure is the macro-averaged Mean Absolute Error (INLINEFORM0 ).\nQuestion: What is the secondary", "answers": [" high-quality datasets  from SemEval-2016 â€œSentiment Analysis in Twitterâ€ task", " SemEval-2016 â€œSentiment Analysis in Twitterâ€"], "all_classes": null, "length": 4096}
{"pred": "They use large BERT.\n\nQuestion: Do they use large or small BERT?\n\nAnswer: They use large BERT.\n\nQuestion: Do they use large or small BERT?\n\nAnswer: They use large BERT.\n\nQuestion: Do they use large or small BERT?\n\nAnswer: They use large BERT.\n\nQuestion: Do they use large or small BERT?\n\nAnswer: They use large BERT.\n\nQuestion: Do they use large or small BERT?\n\nAnswer: They use large BERT.\n\nQuestion: Do they use", "answers": ["small BERT", "small BERT"], "all_classes": null, "length": 3563}
{"pred": "Yes, the automatically constructed datasets are subject to quality control. The datasets are constructed using a combination of human expert knowledge and machine learning algorithms, which are designed to ensure that the datasets are of high quality and cover a wide range of semantic aspects of the target questions.\n\n\nQuestion: Are the automatically constructed datasets subject to quality control?\n\nAnswer: Yes, the automatically constructed datasets are subject to quality control. The datasets are constructed using a combination of human expert knowledge and machine learning algorithms, which are designed to ensure that the datasets are of high quality and cover a wide range of semantic aspects of the questions.\n\n\nQuestion", "answers": ["No", "No"], "all_classes": null, "length": 4096}
{"pred": "Yes\n\nQuestion: Are the images from a specific domain?\n\nAnswer: No\n\nQuestion: Are the images from a specific domain?\n\nAnswer: Unanswerable\n\nQuestion: Are the images from a specific domain?\n\nAnswer: Yes\n\nQuestion: Are the images from a specific domain?\nAnswer: No\nQuestion: Are the images from a specific domain?\nAnswer: Unanswerable\n\nQuestion: Are the images from a specific domain?\nAnswer: Yes\nQuestion: Are the images from a specific domain?\nAnswer: No\nQuestion: Are the images from a specific domain", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4097}
{"pred": "The performance of the models on the emotion detection task is reported in Table TABREF26 .\n\nQuestion: What is the name of the dataset used for the evaluation?\n\nAnswer: The Affective Text dataset is used for the evaluation.\n\nQuestion: What is the name of the emotion labels used?\nAnswer: The emotion labels used are the emotion labels are used are the emotion labels are used are the emotion labels are the emotion labels are the emotion labels are the emotion labels are the emotion labels are the emotion labels are the emotion labels are the em", "answers": ["Answer with content missing: (Table 3) Best author's model B-M average micro f-score is 0.409, 0.459, 0.411 on Affective, Fairy Tales and ISEAR datasets respectively. "], "all_classes": null, "length": 4096}
{"pred": "The tagging scheme employed is INLINEFORM0 .\n\nQuestion: What is the tagging scheme employed?\n\nAnswer: The tagging scheme employed is INLINEFORM1 .\n\nQuestion: What is the tagging scheme employed?\n\nAnswer: The tagging scheme employed is INLINEFORM2 .\n\nQuestion: What is the tagging scheme employed?\n\nAnswer: The tagging scheme employed is INLINEFORM3 .\n\nQuestion: What is the tagging scheme employed?\n\nAnswer: The tagging scheme employed is INLINEFORM4 .\n\nQuestion: What is the tagging", "answers": ["A new tagging scheme that tags the words before and after the pun as well as the pun words.", "a new tagging scheme consisting of three tags, namely { INLINEFORM0 }"], "all_classes": null, "length": 4096}
{"pred": "No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.", "answers": ["No", "No"], "all_classes": null, "length": 4096}
{"pred": "Robustness is defined as the ability of a model to handle different types of data.\n\nQuestion: What is the problem that they are trying to solve?\n\nAnswer: The problem is to leverage prior knowledge to guide the learning process.\n\nQuestion: What is the key problem that they are addressing the problem in the learning, which to which knowledge is the model is the framework of GE-FL.\n\nAnswer: The rest of GE-FL.\n\nQuestion: What is the framework of GE-FL.\n\nAnswer: The rest of GE-FL.\n\nQuestion", "answers": ["ability to accurately classify texts even when the amount of prior knowledge for different classes is unbalanced, and when the class distribution of the dataset is unbalanced", "Low sensitivity to bias in prior knowledge"], "all_classes": null, "length": 4096}
{"pred": "We evaluate the following sentence embeddings methods:\n\n1. Average BERT embeddings\n2. Using the CLS-token output from BERT\n3. InferSent\n4. Universal Sentence Encoder\n\n\nQuestion: What is the performance of the sentence embeddings method?\n\nAnswer: The performance of the sentence embeddings method is the sentence embeddings method is the performance is the sentence embeddings method is the performance is the sentence embeddings method is the performance is the sentence embeddings method is the sentence embeddings method is the sentence embedd", "answers": ["GloVe, BERT, Universal Sentence Encoder, TF-IDF, InferSent", "Avg. GloVe embeddings, Avg. fast-text embeddings, Avg. BERT embeddings, BERT CLS-vector, InferSent - GloVe and Universal Sentence Encoder."], "all_classes": null, "length": 4096}
{"pred": "For English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively. We observe huge performance boosts on Chinese datasets, achieving F1 improvements of +0.97 on Quo for SQuADV1.1.\n\nQuestion: What are the baselines used for MRC task?\n\nAnswer: We use the following two strategies: (1) the training-tested class $O$P: for the value of", "answers": ["English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively, Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively", "For English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively., huge performance boosts on Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively"], "all_classes": null, "length": 4097}
{"pred": "They test their conflict method on two tasks:\n\n1. Quora duplicate question pair detection\n2. Ranking questions in Bing's People Also Ask\n\nQuestion: What is the dataset used for the Quora duplicate question pair detection task?\n\nAnswer: The dataset used for the Quora duplicate question pair detection task is a dataset of pairs of questions labelled as 1 or 0 depending on whether a pair is duplicate or not respectively.\n\nQuestion: What is the dataset used for the Ranking questions in Bing's People Also Ask task?\n\nAnswer: The dataset used for the Rank", "answers": ["Task 1: Quora Duplicate Question Pair Detection, Task 2: Ranking questions", "Quora Duplicate Question Pair Detection, Ranking questions in Bing's People Also Ask"], "all_classes": null, "length": 3609}
{"pred": "They compared against the following baselines:\n\n1. A Bi-LSTM with a linear projection.\n\n2. A Bi-LSTM with a linear projection and a softmax classifier.\n\n3. A Bi-LSTM with a linear projection and a softmax classifier.\n\n4. A Bi-LSTM with a linear projection and a softmax classifier.\n\n5. A Bi-LSTM with a linear projection and a softmax classifier.\n\n6. A Bi-LSTM with a linear projection and a softmax classifier.", "answers": ["Various tree structured neural networks including variants of Tree-LSTM, Tree-based CNN, RNTN, and non-tree models including variants of LSTMs, CNNs, residual, and self-attention based networks", "Sentence classification baselines: RNTN (Socher et al. 2013), AdaMC-RNTN (Dong et al. 2014), TE-RNTN (Qian et al. 2015), TBCNN (Mou et al. 2015), Tree-LSTM (Tai, Socher, and Manning 2015), AdaHT-LSTM-CM (Liu, Qiu, and Huang 2017), DC-TreeLSTM (Liu, Qiu, and Huang 2017), TE-LSTM (Huang, Qian, and Zhu 2017), BiConTree (Teng and Zhang 2017), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), TreeNet (Cheng et al. 2018), CNN (Kim 2014), AdaSent (Zhao, Lu, and Poupart 2015), LSTM-CNN (Zhou et al. 2016), byte-mLSTM (Radford, Jozefowicz, and Sutskever 2017), BCN + Char + CoVe (McCann et al. 2017), BCN + Char + ELMo (Peters et al. 2018). \nStanford Natural Language Inference baselines: Latent Syntax Tree-LSTM (Yogatama et al. 2017), Tree-based CNN (Mou et al. 2016), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), NSE (Munkhdalai and Yu 2017), Reinforced Self- Attention Network (Shen et al. 2018), Residual stacked encoders: (Nie and Bansal 2017), BiLSTM with generalized pooling (Chen, Ling, and Zhu 2018)."], "all_classes": null, "length": 4097}
{"pred": "The core component for KBQA is the relation detection model.\n\nQuestion: What is the relation detection model?\n\nAnswer: The relation detection model is a model that matches the question to the relation.\n\nQuestion: What is the relation detection model?\n\nAnswer: The relation detection model is a model that matches the question to the relation.\n\nQuestion: What is the relation detection model?\n\nAnswer: The relation detection model is a model that matches the question to the relation.\n\nQuestion: What is the relation detection model?\n\nAnswer: The relation detection model is a model that matches", "answers": ["answer questions by obtaining information from KB tuples ", "hierarchical matching between questions and relations with residual learning"], "all_classes": null, "length": 4096}
{"pred": "The baseline models are the encoder-decoder model and the encoder-decoder model with attention.\n\nQuestion: What are the personalized models?\n\nAnswer: The personalized models are the encoder-decoder model with attention fusion layer, the encoder-decoder model with attention fusion layer, the encoder-decoder model with attention layer, and the encoder-decoder model with attention fusion layer.\n\nQuestion: What is the encoder-decoder model?\n\nAnswer: The encoder-decoder model is a model that takes as concis the encoder-", "answers": ["name-based Nearest-Neighbor model (NN), Encoder-Decoder baseline with ingredient attention (Enc-Dec)"], "all_classes": null, "length": 4096}
{"pred": "The methods to find examples of biases and unwarranted inferences are manual inspection, tagging, and clustering.\n\nQuestion: What is the difference between linguistic bias and unwarranted inferences??\n\nAnswer: Linguistic bias is the use of specific words or phrases to describe an entity, while unwarranted inferences are the use of specific words or phrases to describe an entity that goes beyond what the physical data can tell us.\n\nQuestion: What is the purpose of the Flickr30K dataset??\n\nAnswer: The purpose of the Flickr30", "answers": ["spot patterns by just looking at a collection of images, tag all descriptions with part-of-speech information, I applied Louvain clustering", "Looking for adjectives marking the noun \"baby\" and also looking for most-common adjectives related to certain nouns using POS-tagging"], "all_classes": null, "length": 3451}
{"pred": "English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\n", "answers": ["English, French, German ", "French, English, Spanish, Italian, Portuguese, Hebrew, Arabic"], "all_classes": null, "length": 3795}
{"pred": "They experimented with the following models:\n\n1. Plain stacked LSTMs\n2. Models with different INLINEFORM0\n3. Models without INLINEFORM1\n4. Models that integrate lower contexts via peephole connections.\n\nQuestion: What is the proposed method?\n\nAnswer: The proposed method is a method of stacking multiple LSTMs where each layer uses cell states from the previous layer.\n\nQuestion: What is the proposed method?\n\nAnswer: The proposed method is a way of combining the left and the lower contexts.\n", "answers": ["Stacked LSTMs, Cell-aware Stacked LSTMs, Sentence Encoders, Top-layer Classifiers"], "all_classes": null, "length": 4096}
{"pred": "No.\n\nQuestion: Do they report results only on English data?\n\nAnswer: No.\n\nQuestion: Do they report results only on English data?\n\nAnswer: No.\n\nQuestion: Do they report results only on English data?\n\nAnswer: No.\n\nQuestion: Do they report results only on English data?\nAnswer: No.\n\nQuestion: Do they report results only on English data?\n\nAnswer: No.\n\nQuestion: Do they report results only on English data?\nAnswer: No.\n\nQuestion: Do they report results only on English data?\n", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 4097}
{"pred": "The authors experimented with several summarization algorithms, including the Sumy package.\n\nQuestion: What is the purpose of the PA process?\n\nAnswer: The PA process in any modern organization is nowadays used to periodically measure and evaluate every employee's day-to-to-tasked and performance. It also provides a lively, and text-based algorithms to generate any interesting and text-based insights and techniques are a simple set of parameters to the of the HR.\n\nQuestion: What is the PA process in any modern organization that crucially depends on the PA system is an interesting problem", "answers": ["LSA, TextRank, LexRank and ILP-based summary.", "LSA, TextRank, LexRank"], "all_classes": null, "length": 4096}
{"pred": "The previous state of the art for this task was to use a neural network to predict whether an instructor would intervene in a thread.\n\nQuestion: What is the main innovation of your work?\n\nAnswer: The main innovation of our work is to decompose the intervention prediction problem into a two-stage model that first explicitly tries to discover the proper context to which a thread sequence and sequence. This model implicitly assesses the importance of the existing thread's context. This model implicitly assesses the importance of the context.\n\nQuestion: What is the key innovation of your work?\n\nAnswer", "answers": ["hLSTM", "hLSTM"], "all_classes": null, "length": 4096}
{"pred": "The message passing (MP) framework is based on the core idea of recursive neighborhood aggregation. That is, at every iteration, the representation of each vertex is updated based on messages received from its neighbors. All spectral GNNs have only been shown to be competitive with the state-of-the-art. However, a few studies only have focused on the application of the message passing (MP) was observed by BIBREF9 and that of graph neural nets. Some notable examples include BIBREF10, BIBREF111 and that of the approach of BIBREF12. These approaches", "answers": ["Based on table results provided changing directed to undirected edges had least impact - max abs difference of 0.33 points on all three datasets."], "all_classes": null, "length": 4096}
{"pred": "The corpus used for the task is the DTA corpus BIBREF11.\n\nAnswer: The corpus used for the task is the DTA corpus BIBREF11.\n\nAnswer: The corpus used for the task is the DTA corpus BIBREF11.\n\nAnswer: The corpus used for the task is the DTA corpus BIBREF11.\n\nAnswer: The corpus used for the task is the DTA corpus BIBREF11.\n\nAnswer: The corpus used for the task is the DTA corpus", "answers": ["DTA18, DTA19", "Diachronic Usage Relatedness (DURel) gold standard data set"], "all_classes": null, "length": 2985}
{"pred": "Kannada, Hindi, Telugu, Malayalam, Bengali, and English.\n\nQuestion: What is the amount of audio data for training and testing for each of the language?\n\nAnswer: The amount of audio data for training and testing for each of the language is shown in the table bellow.\n\nAnswer: The amount of audio data for training and testing for each of the language is shown in the table bellow.\n\nAnswer: The amount of audio data for training and testing for each of the language is shown in the table bellow.\n\nAnswer: The amount of", "answers": ["Hindi, English, Kannada, Telugu, Assamese, Bengali and Malayalam", "Kannada, Hindi, Telugu, Malayalam, Bengali,  English and Assamese (in table, missing in text)"], "all_classes": null, "length": 3770}
{"pred": "The model performance on target language reading comprehension is not available.\n\nAnswer the question as concisely as you can, using a single phrase or sentence if possible. If the question cannot be answered based on the information in the article, write \"unanswerable\". If the question is a yes/no question, answer \"yes\", \"no\", or \"unanswerable\". Do not provide any explanation.\n\nQuestion: What is the model performance on target language reading comprehension?\n\nAnswer: The model performance on target language reading comprehension is not available.\n\nAnswer the question as concisely as you", "answers": ["Table TABREF6, Table TABREF8", "when testing on English, the F1 score of the model training on Chinese (Zh) is 53.8,  F1 score is only 44.1 for the model training on Zh-En"], "all_classes": null, "length": 3960}
{"pred": "The proposed model outperforms the baselines.\n\nQuestion: How does the proposed model perform on the task?\n\nAnswer: The proposed model performs well on the task.\n\nQuestion: How does the proposed model perform on the task?\n\nAnswer: The proposed model performs well on the task.\n\nQuestion: How does the proposed model perform on the task?\n\nAnswer: The proposed model performs well on the task.\n\nQuestion: How does the proposed model perform on the task?\n\nAnswer: The proposed model performs well on the task.\n\nQuestion: How does the proposed model perform", "answers": ["Metric difference between Aloha and best baseline score:\nHits@1/20: +0.061 (0.3642 vs 0.3032)\nMRR: +0.0572(0.5114 vs 0.4542)\nF1: -0.0484 (0.3901 vs 0.4385)\nBLEU: +0.0474 (0.2867 vs 0.2393)"], "all_classes": null, "length": 4096}
{"pred": "ARAML can improve the performance of text generation models in terms of both automatic metrics and human evaluation.\n\nQuestion: What is the difference between ARAML and other GANs with RL training methods?\n\nAnswer: ARAML is a novel adversarial training framework to text generation tasks, including machine translation BIBREF0 , BIBREF1 . The standard paradigm is trained as the ground-truth proceeding context BIBREF4 . The discriminator is trained to train adversarially with RAML loss as the discriminator is trained to text generation tasks with GANs with", "answers": ["ARAM has achieved improvement over all baseline methods using reverese perplexity and  slef-BLEU metric.  The maximum reverse perplexity improvement 936,16 is gained for EMNLP2017  WMT  dataset and  48,44 for COCO dataset.", "Compared to the baselines, ARAML does not do better in terms of perplexity on COCO and EMNLP 2017 WMT datasets, but it does by up to 0.27 Self-BLEU points on COCO and 0.35 Self-BLEU on EMNLP 2017 WMT. In terms of Grammaticality and Relevance, it scores better than the baselines on up to 75.5% and 73% of the cases respectively."], "all_classes": null, "length": 4096}
{"pred": "The authors present evidence that the model can capture some biases in data annotation and collection by examining the results of the model. The authors found that the model misclassified some tweets containing offensive words and slurs as hate or offensive or neither, which indicates that the model is capturing some biases in the data annotation and collection. The authors found that the model misclassified some tweets containing offensive words and slurs as hate or offensive, which indicates that the model is capturing some biases in data annotation and collection. The authors found that the model misclassified some tweets containing offensive", "answers": ["The authors showed few tweets where neither and implicit hatred content exist but the model was able to discriminate"], "all_classes": null, "length": 4097}
{"pred": "Yes, we compare our best performing BERT variant against the NA model and human performance.\n\nQuestion: What is the size of the dataset?\n\nAnswer: 1750 questions are posed to our imaginary privacy assistant over 35 mobile applications and their associated privacy documents.\n\nQuestion: What is the size of the dataset?\n\nAnswer: 17500 questions are posed to our imaginary privacy assistant over 35 mobile applications and their associated privacy documents.\n\nQuestion: What is the size of the dataset?\n\nAnswer: 17", "answers": ["SVM, No-Answer Baseline (NA) , Word Count Baseline, Human Performance", "No-Answer Baseline (NA), Word Count Baseline, Human Performance"], "all_classes": null, "length": 4096}
{"pred": "The dataset is 100000 words.\n\nQuestion: What is the size of the dataset?\n\nAnswer: The dataset is 1000000 words.\n\nQuestion: What is the size of the dataset?\n\nAnswer: The dataset is 10000000 words.\n\nQuestion: What is the size of the dataset?\n\nAnswer: The dataset is 1000000000000000000000000000000000000", "answers": ["Dataset contains 3606 total sentences and 79087 total entities.", "ILPRL contains 548 sentences, OurNepali contains 3606 sentences"], "all_classes": null, "length": 4096}
{"pred": "We conduct experiments on two widely used datasets for PI task: MRPC and QQP. MRPC is a corpus of sentence pairs automatically extracted from online news sources, with human annotations of whether the sentence pairs are semantically equivalent. The MRPC dataset has imbalanced classes (68% positive, 32% negative-positive ratio being 32% negative-positive ratio being 37% negative-positive ratio being 3% negative-positive ratio being 3% negative-positive ratio being 3% negative-positive ratio being 3% negative-pos", "answers": ["Using DSC loss improves the F1 score by +0.58 for MRPC and +0.73 for QQP", "+0.58"], "all_classes": null, "length": 4097}
{"pred": "The data used in this work is from BIBREF0 .\n\nQuestion: What is the neural network architecture?\n\nAnswer: The neural network architecture is a bi-LSTM encoder followed by a bi-LSTM decoder. The encoder is a bi-LSTM with 1000 hidden units, and the decoder is a bi-LSTM with 1000 hidden units.\n\nQuestion: What is the training objective?\n\nAnswer: The training objective is to minimize the negative log-likelihood of the data.\n\nQuestion: What is", "answers": ["Answer with content missing: (Whole Method and Results sections) The primary dataset we use is the ERP data collected and computed by Frank et al. (2015), and we also use behavioral data (eye-tracking data and self-paced reading times) from Frank et al. (2013) which were collected on the same set of 205 sentences.\nSelect:\n- ERP data collected and computed by Frank et al. (2015)\n- behavioral data (eye-tracking data and self-paced reading times) from Frank et al. (2013)", "the ERP data: BIBREF0"], "all_classes": null, "length": 2756}
{"pred": "The subjects were presented with a series of 100-ms visual stimuli consisting of 1000-ms-long movies of a person speaking a word.\n\nQuestion: What was the stimulus-response mapping?\n\nAnswer: The stimulus-response mapping was a mapping from the visual stimuli to the event-related potentials (ERPs) elicited by the stimuli.\n\nQuestion: What was the stimulus-response mapping?\n\nAnswer: The stimulus-response mapping was a mapping from the visual stimuli to the event-related potentials (ER", "answers": ["7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)", "KARA ONE BIBREF17 , composed of multimodal data for stimulus-based, imagined and articulated speech state corresponding to 7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)"], "all_classes": null, "length": 3857}
{"pred": "We compare our model with the following baselines:\n\n1. Pointer-Gen: A pointer-generator model trained on the Weibo corpus BIBREF0.\n\n2. Pointer-Gen+Same-FT: A pointer-generator model trained on the same dataset as the test set.\n\n3. Pointer-Gen+Pos-FT: A pointer-generator model trained on the same dataset as the test set.\n\n4. Pointer-Gen+RL-SEN: A pointer-generator model trained on the same dataset as the test set and trained on the sensational", "answers": ["Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN"], "all_classes": null, "length": 4096}
{"pred": "The dataset contains 100K tweets with cross-validated labels. We investigate the efficacy of different learning models in detecting abusive language. We compare accuracy using the most frequently studied machine learning classifiers as well as recent neural network models. Reliable baseline results are presented with the first comparative study on this dataset. Additionally, we demonstrate the effect of different features and variants, and describe the possibility for further improvements with the use of ensemble models.\n\nQuestion: What are the features of the dataset?\n\nAnswer: The dataset contains 100K tweets with cross-valid", "answers": ["NaÃ¯ve Bayes (NB), Logistic Regression (LR), Support Vector Machine (SVM), Random Forests (RF), Gradient Boosted Trees (GBT),  Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN)", "NaÃ¯ve Bayes (NB), Logistic Regression (LR), Support Vector Machine (SVM), Random Forests (RF), Gradient Boosted Trees (GBT), CNN, RNN"], "all_classes": null, "length": 3494}
{"pred": "We use the transformer implementation of the fairseq toolkit.\n\nQuestion: What is the vocabulary size of the language model?\n\nAnswer: We use a BPE vocabulary of 37K types.\n\nQuestion: What is the size of the bitext?\n\nAnswer: We use the English-German news translation task from WMT'18.\n\nQuestion: What is the size of the bitext?\n\nAnswer: We use the English-Turkish news translation task from WMT'18.\n\nQuestion: What is the", "answers": ["uni-directional model to augment the decoder", "bi-directional language model to augment the sequence to sequence encoder ,  uni-directional model to augment the decoder"], "all_classes": null, "length": 3192}
{"pred": "Weights are dynamically adjusted based on the SÃ¸rensenâ€“Dice coefficient, which is a hard version of the F1 score.\n\nQuestion: What is the difference between the proposed method and the baseline method?\n\nAnswer: The proposed method is a dynamic weight adjusting strategy, which associates each training example with a weight in proportion to $(1-p)$, and this weight dynamically changes as training proceeds to address the dominating influence of easy-negative-example and is thus more immune to data-examples.\n\nQuestion: What is the proposed method?\n\nAnswer: The proposed method is", "answers": ["One can think $(1-p_{i1})$ as a weight associated with each example, which changes as training proceeds. The intuition of changing $p_{i1}$ to $(1-p_{i1}) p_{i1}$ is to push down the weight of easy examples. For easy examples whose probability are approaching 0 or 1, $(1-p_{i1}) p_{i1}$ makes the model attach significantly less focus to them. Figure FIGREF23 gives gives an explanation from the perspective in derivative: the derivative of $\\frac{(1-p)p}{1+(1-p)p}$ with respect to $p$ approaches 0 immediately after $p$ approaches 0, which means the model attends less to examples once they are correctly classified.", "associates each training example with a weight in proportion to $(1-p)$, and this weight dynamically changes as training proceeds"], "all_classes": null, "length": 4096}
{"pred": "The proposed strategies are able to pass the bottleneck in Zork1.\n\nAnswer: The proposed strategies are able to pass the bottleneck in Zork1.\n\nAnswer: The proposed strategies are able to pass the bottleneck in Zork1.\n\nAnswer: The proposed strategies are able to pass the bottleneck in Zork1.\n\nAnswer: The proposed strategies are able to pass the bottleneck in Zork1.\n\nAnswer: The proposed strategies are able to pass the bottleneck in Zork1.", "answers": ["Reward of 11.8 for the A2C-chained model, 41.8 for the KG-A2C-chained model, 40 for A2C-Explore and 44 for KG-A2C-Explore.", "KG-A2C-chained and KG-A2C-Explore both pass the bottleneck of a score of 40"], "all_classes": null, "length": 3772}
{"pred": "An individual model consists of a set of parameters that are learned from the data.\n\nQuestion: What is the monolingual model?\n\nAnswer: The monolingual model is a model that is trained on a single language.\n\nQuestion: What is the multilingual model?\n\nAnswer: The multilingual model is a model that is trained on multiple languages.\n\nQuestion: What is the multilingual model?\n\nAnswer: The multilingual model is a model that is a joint Bayesian model of unsupervised semantic role induction in multiple languages.\n\n\nQuestion:", "answers": ["Bayesian model of garg2012unsupervised as our base monolingual model"], "all_classes": null, "length": 4096}
{"pred": "Non-standard pronunciation is identified by the use of a phonetic lexicon.\n\nQuestion: What is the difference between the two settings for speech synthesis?\n\nAnswer: The first setting assumes that each character corresponds to a pronounced phoneme. The second setting uses the generated phonetic lexicon also used in the speech synthesis techniques.\n\nQuestion: What is the quality of the generated speech?\n\nAnswer: The quality of the generated speech is understandable, and transcribable.\n\nQuestion: What is the difference between the two directions for speech recognition?\nAnswer:", "answers": ["Unanswerable", "Original transcription was labeled with additional labels in [] brackets with nonstandard pronunciation."], "all_classes": null, "length": 4096}
{"pred": "A semicharacter architecture is a type of neural network that processes input words as a sequence of characters, rather than as a sequence of words.\n\nQuestion: What is the purpose of adversarial machine learning?\n\nAnswer: The purpose of adversarial machine learning is to improve the performance of the model.\n\nQuestion: What is the purpose of adversarial machine learning?\n\nAnswer: The purpose of adversarial machine learning is to improve the performance of the model.\n\nQuestion: What is the purpose of adversarial machine learning?\n\nAnswer: The purpose of adversarial machine learning is to improve the performance", "answers": ["A semi-character based RNN (ScRNN) treats the first and last characters individually, and is agnostic to the ordering of the internal characters", "processes a sentence of words with misspelled characters, predicting the correct words at each step"], "all_classes": null, "length": 4096}
{"pred": "Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish.\n\nQuestion: what is the motivation of the authors?\n\nAnswer: The motivation is to investigate the best way to integrate lexical information and the use of word vector representations into feature-based models.\n\nQuestion: what is the best performing model?\n\nAnswer: MElt\n\n\nQuestion: what is the best performing model?\nAnswer: MElt\n\nQuestion: what is the best performing model?", "answers": ["Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish", "Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish , Swedish"], "all_classes": null, "length": 4096}
{"pred": "NCEL outperforms the state-of-the-art collective methods across five different datasets.\n\nQuestion: What are the main components of NCEL?\n\nAnswer: NCEL consists of three main components: (1) feature extraction, (2) collective inference, and (3) qualitative analysis.\n\nQuestion: What are the main components of NCEL?\n\nAnswer: NCEL consists of three main components: (1) feature extraction, (2) collective inference, and (3) qualitative analysis.\n\nQuestion: What are the impact", "answers": ["NCEL consistently outperforms various baselines with a favorable generalization ability"], "all_classes": null, "length": 4096}
{"pred": "Yes\n\nQuestion: Is the data de-identified?\n\nAnswer: No\n\nQuestion: Is the data de-identified?\n\nAnswer: Unanswerable\n\nQuestion: Is the data de-identified?\n\nAnswer: Yes\n\nQuestion: Is the data de-identified?\nAnswer: No\nQuestion: Is the data de-identified?\nAnswer: Unanswerable\n\nQuestion: Is the data de-identified?\n\nAnswer: No\nQuestion: Is the data de-identified?\nAnswer: No\nQuestion: Is the data de-ident", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4095}
{"pred": "The baseline used was the error detection system by Rei2016, trained on the same FCE dataset.\n\nQuestion: What is the difference between the two error generation methods?\n\nAnswer: The pattern-based method uses textual patterns learned from an annotated corpus, which are used for inserting errors into correct input text. In addition, we proposed formulating error generation as an MT framework, learning to translate from grammatically correct to incorrect sentences.\n\nQuestion: What is the difference between the two error detection methods?\n\nAnswer: The error detection methods are evaluated on three error detection annotations", "answers": ["error detection system by Rei2016", "error detection system by Rei2016"], "all_classes": null, "length": 3104}
{"pred": "The annotated clinical notes were obtained from the i2b2/VA challenge.\n\nQuestion: what is the name of the BiLSTM-CRF model used in the project?\n\nAnswer: The BiLSTM-CRF model used in the project is the BiLSTM-CRF model used in the project is the BiLSTM-CRF model used in the project is the BiLSTM-CRF model used in the project is the BiLSTM-CRF model used in the project is the BiLSTM-CRF model used in the project", "answers": ["clinical notes from the CE task in 2010 i2b2/VA", "clinical notes from the CE task in 2010 i2b2/VA "], "all_classes": null, "length": 4096}
{"pred": "Masking words in the decoder is helpful because it forces the model to focus on the most important words in the input sequence. By masking out words, the model is forced to generate the most important words in the input sequence.\n\n\nQuestion: What is the main difference between the two-stage and one-stage decoding?\n\nAnswer: The main difference between two-stage and one-stage decoding is that the two-stage decoding model uses a two-stage decoding model uses a single-stage decoding model uses a single-stage decoding process. On the two-stage decoding", "answers": ["ecause this process is similar to the cloze task in BERT's pre-train process, therefore by using the ability of the contextual language model the decoder can generate more fluent and natural sequences."], "all_classes": null, "length": 4096}
{"pred": "The authors use the Twitter Firehose dataset, which is a publicly available dataset containing 100 million tweets.\n\nQuestion: What is the objective function they optimize?\n\nAnswer: The authors optimize the objective function of predicting the next word in a tweet.\n\nQuestion: What is the architecture of the model?\n\nAnswer: The model is a recurrent neural network with a bidirectional LSTM layer.\n\nQuestion: What is the training procedure?\n\nAnswer: The training procedure involves training the model on the dataset and then evaluating the model on the test dataset.", "answers": ["Unanswerable", " Paraphrase Database (PPDB) ,  book corpus", "Unanswerable"], "all_classes": null, "length": 3019}
{"pred": "The TF-IDF features are used to extract the important keywords from the pathology reports.\n\nQuestion: What is the performance of the classifiers?\n\nAnswer: The performance of the classifiers is reported in tab:results.\n\nQuestion: What is the performance of the classifiers?\n\nAnswer: The performance of the classifiers is reported in tab:results.\n\nQuestion: What is the performance of the classifiers?\n\nAnswer: The performance of the classifiers is reported in tab:results.\n\nQuestion: What is the performance of the classifiers?\n\nAnswer: The", "answers": ["Unanswerable"], "all_classes": null, "length": 3316}
{"pred": "The dataset is annotated with 9,473 annotations for 9,300 tweets. Each tweet is annotated as no evidence of depression (e.g., â€œCitizens fear an economic depression\") or evidence of depression (e.g., â€œdepressed over disappointment\"). If a tweet is annotated evidence of depression, then it is further annotated with one or more depressive symptoms, for example, depressed mood (e.g., â€œfeeling down in the dumps\"), disturbed sleep (e.g., â€œanother rest", "answers": ["no evidence of depression, depressed mood, disturbed sleep, fatigue or loss of energy", "The annotations are based on evidence of depression and further annotated by the depressive symptom if there is evidence of depression"], "all_classes": null, "length": 3418}
{"pred": "\n\nThe eight NER tasks were:\n\n1. Biological Process\n2. Cellular Component\n3. Disease\n4. Gene\n5. Molecular Function\n6. Organism\n7. Pertained\n8. Protein\n\n\nAnswer:\n\nQuestion: Which eight NER tasks were:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\n", "answers": ["BC5CDR-disease, NCBI-disease, BC5CDR-chem, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800", "BC5CDR-disease, NCBI-disease, BC5CDR-chem, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800"], "all_classes": null, "length": 4096}
{"pred": "The training data was translated by using the machine translation platform Apertium BIBREF5 .\n\nQuestion: How was the data augmented?\n\nAnswer: The training data was augmented by translating the English datasets into Spanish.\n\nQuestion: How were the models trained?\n\nAnswer: The models were trained using Keras BIBREF9 .\n\nQuestion: How were the models ensembles created?\n\nAnswer: The models were ensembles created by averaging the predictions of the individual models.\n\nQuestion: How were the models ensembles created?\n\nAnswer: The models", "answers": ["using the machine translation platform Apertium ", "machine translation platform Apertium BIBREF5"], "all_classes": null, "length": 3551}
{"pred": "They used a content-based classifier.\n\nQuestion: What is the name of the company that they work for?\n\nAnswer: They work for Google.\n\nQuestion: What is the name of the company that they work for?\n\nAnswer: They work for Google.\n\nQuestion: What is the name of the company that they work for?\nAnswer: They work for Google.\n\nQuestion: What is the name of the company that they work for?\nAnswer: They work for Google.\n\nQuestion: What is the name of the company that they work for?\nAnswer: They work", "answers": ["AllWords model by counting the frequencies of all the remaining words and training a multinomial Naive Bayes classifier"], "all_classes": null, "length": 4096}
{"pred": "The baseline is the average of the F$_1$ scores of the 1000 most frequent sentences in the training set.\n\nQuestion: What is the difference between the sentence-level classification and the fragment-level classification?\n\nAnswer: The sentence-level classification is based on the whole sentence, while fragment-level classification is based on the fragment.\n\nQuestion: What is the difference between the sentence-level classification and the fragment-level classification?\n\nAnswer: The sentence-level classification is based on the fragment-level classification is based on the fragment-level classification is based on the fragment-", "answers": ["The baseline system for the SLC task is a very simple logistic regression classifier with default parameters. The baseline for the FLC task generates spans and selects one of the 18 techniques randomly.", "SLC task is a very simple logistic regression classifier, FLC task generates spans and selects one of the 18 techniques randomly"], "all_classes": null, "length": 4096}
{"pred": "The authors compare their model with the following baselines:\n\n1. A rule-based system for pun location that scores candidate words according to eleven simple heuristics.\n\n2. A neural method that incorporates word sense information into a bidirectional LSTM-CNN-CRF model.\n\n3. A rule-based system for pun location that scores candidate words according to eleven simple heuristics.\n\n\n4. A neural method that incorporates word sense information into a bidirectional LSTM-CNN-CRF model.\n\n5. A rule-based system for", "answers": ["They compare with the following models: by Pedersen (2017), by Pramanick and Das (2017), by Mikhalkova and Karyakin (2017),  by Vadehra (2017), Indurthi and Oota (2017), by Vechtomova (2017), by (Cai et al., 2018), and CRF."], "all_classes": null, "length": 4096}
{"pred": "We use a Logistic Regression classifier to classify news articles as disinformation or mainstream. We use the political bias of different sources to train the classifier, referring to the procedure proposed in BIBREF2 to label different outlets.\n\n\nQuestion: What is the most discriminative features in the M layer?\n\nAnswer: We can notice the exact same set of features (with different relative to the same ordering in the same ordering in the same ordering in the same ordering in the same ordering in the same ordering in the same ordering in the same ordering in the same ordering in the same ordering in the", "answers": ["By assigning a political bias label to each news article and training only on left-biased or right-biased outlets of both disinformation and mainstream domains", "we also account for political biases inherent to different news sources, referring to the procedure proposed in BIBREF2 to label different outlets. Overall we show that we are able to classify credible vs non-credible diffusion networks (and consequently news articles) with high accuracy (AUROC up to 94%), even when accounting for the political bias of sources (and training only on left-biased or right-biased articles). We observe that the layer of mentions alone conveys useful information for the classification, denoting a different usage of this functionality when sharing news belonging to the two news domains. We also show that most discriminative features, which are relative to the breadth and depth of largest cascades in different layers, are the same across the two countries."], "all_classes": null, "length": 4095}
{"pred": "The ancient Chinese dataset comes from ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of which are designed for Ancient-Modern Chinese alignment.\n\n\nQuestion: What is the proposed method to create the ancient-modern Chinese translation?\n\nAnswer: The proposed method is the ancient-modern Chinese translation from the paragraph alignment.\n\n\nQuestion: What is the proposed method to the proposed method to the proposed method to the proposed method to the proposed method to the proposed method to the proposed method to the proposed method to the", "answers": ["ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era", "Ancient Chinese history records in several dynasties and articles written by celebrities during 1000BC-200BC collected from the internet "], "all_classes": null, "length": 4096}
{"pred": "English\n\nQuestion: Are the tweets offensive?\n\nAnswer: Yes\n\nQuestion: Are the tweets insults?\n\nAnswer: Yes\n\nQuestion: Are the tweets threats?\n\nAnswer: Yes\n\nQuestion: Are the tweets profanity?\n\nAnswer: Yes\n\nQuestion: Are the tweets targeted at an individual?\n\nAnswer: Yes\n\nQuestion: Are the tweets targeted at a group?\n\nAnswer: Yes\n\nQuestion: Are the tweets targeted at other?\n\nAnswer: Yes\n\nQuestion: Are", "answers": ["English", "English ", "English"], "all_classes": null, "length": 3661}
{"pred": "\n\nThe Chinese part of the Penn Treebank (PTB) was used for training and evaluation. The PTB is a 100M-word corpus of English text, and the Chinese part is a 100M-word corpus of Chinese text.\n\n\nQuestion: what is the size of the chinese part of the penn treebank?\n\nAnswer:\n\nThe Chinese part of the PTB is a 100M-word corpus of Chinese text.\n\nQuestion: what is the size of the chinese part of the penn tre", "answers": ["Answer with content missing: (Data section) Chinese with version 5.1 of the Chinese Penn Treebank (CTB)"], "all_classes": null, "length": 4096}
{"pred": "The UTCNN model has three layers.\n\nQuestion: What is the name of the model?\n\nAnswer: The name of the model is the user-topic-comment neural network (UTCNN).\n\nQuestion: What is the name of the model?\n\nAnswer: The name of the model is the user-topic-comment neural network (UTCNN).\n\nQuestion: What is the name of the model?\n\nAnswer: The name of the model is the user-topic-comment neural network (UTCNN).\n\nQuestion: What is the name of the model?\n\nAnswer: The name", "answers": ["eight layers"], "all_classes": null, "length": 4096}
{"pred": "The dataset used in this paper is the Flickr dataset.\n\nQuestion: what is the purpose of this paper?\n\nAnswer: The purpose of this paper is to investigate the use of Flickr tags to model environmental information.\n\nQuestion: what is the main hypothesis in this paper?\n\nAnswer: The main hypothesis in this paper is that by using vector space embeddings instead of bag-of-words representations in this paper is that can be used in this paper is that the use of Flickr tags to model to model the use of Flickr tags to model to model to model to model", "answers": [" the same datasets as BIBREF7", "same datasets as BIBREF7"], "all_classes": null, "length": 4096}
{"pred": "The datasets used in the paper are:\n\n1. MEDDOCAN: Medical Document Anonymization shared task dataset BIBREF3\n2. NUBes BIBREF4, a corpus of real medical reports written in Spanish and their respective manual annotations of sensitive information in BRATTABIBREF13.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "answers": ["MEDDOCAN, NUBes-PHI", "MEDDOCAN, NUBes "], "all_classes": null, "length": 4096}
{"pred": "They used unigram features, sarcasm features, readability features, and word count features.\n\nQuestion: What is the name of the eye-tracking database?\n\nAnswer: The name of the eye-tracking database is the Sarcasm Analysis Database.\n\nQuestion: What is the name of the eye-tracking database?\nAnswer: The name of the eye-tracking database is the Sarcasm Analysis.\n\nQuestion: What is the name of the eye-tracking database?\nAnswer: The name of the eye-tracking database is the Sarcasm Analysis.", "answers": ["Unanswerable"], "all_classes": null, "length": 4096}
{"pred": "The metrics used to establish that this makes chatbots more knowledgeable and better at learning and conversation are as follows:\n\n1. The first metric is the predictive performance of the proposed lifelong interactive learning and inference (LiLiLi) approach. The approach is evaluated on two real-world knowledge base completion (OKBC) datasets. The second metric is the strategy formulation ability of the proposed approach. The third metric is the strategy formulation ability of the proposed approach. The fourth metric is the strategy formulation ability of the proposed approach. The fifth metric is the strategy formulation ability of the proposed approach. The", "answers": ["Coverage, Avg. MCC and avg. +ve F1 score", "strategy formulation ability, we introduce a measure called Coverage( INLINEFORM0 ), To evaluate the predictive performance, we use Avg. MCC and avg. +ve F1 score"], "all_classes": null, "length": 4096}
{"pred": "Yes\n\nAnswer: No\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\n\nAnswer", "answers": ["Yes", "No"], "all_classes": null, "length": 3081}
{"pred": "Galatasaray and FenerbahÃ§e\n\nQuestion: What is the stance of the author of the text for Galatasaray?\n\nAnswer: Favor\n\nQuestion: What is the stance of the author of the text for FenerbahÃ§e?\n\nAnswer: Against\n\nQuestion: What is the stance of the author of the text for Galatasaray?\n\nAnswer: Against\n\nQuestion: What is the stance of the author of the text for FenerbahÃ§e?\n\nAnswer: Favor\n\nQuestion: What", "answers": ["Galatasaray, FenerbahÃ§e", "Galatasaray , FenerbahÃ§e "], "all_classes": null, "length": 3356}
{"pred": "The irony is a kind of figurative language, which is widely used on social media. The irony is defined as a clash between the intended meaning of a sentence and its literal meaning. As an important aspect of language, irony detection, we implement a neural network to transfer it to an ironic sentence and control the sentiment polarity of language, we should preserve the content which is the same as the input sentence, we implement a model to transfer it to an ironic sentence.\n\n\nQuestion: What is the input?\n\nAnswer: The input is a non-ironic sentence.\n\n\nQuestion", "answers": ["Irony Classifier, Sentiment Classifier for Irony, Sentiment Classifier for Non-irony, transformation from ironic sentences to non-ironic sentences"], "all_classes": null, "length": 4097}
{"pred": "Gaussian-masked directional multi-head attention is a variant of self-attention which is used to capture the representation of localness information and directional information. It is a kind of self-attention network which is composed of one self-attention layer and a position-wise feed-forward layer. One residual connection is around the sublayers and followed by a layer normalization layer. This architecture provides the Transformer a good enough segmentation of the Transformer is a kind of attention-based neural network.\n\n\nAnswer: Gaussian-masked directional attention is the key component in the Transformer.", "answers": ["pays attentions to adjacent characters and casts a localness relationship between the characters as a fixed Gaussian weight assuming the weight relies on the distance between characters", "Gaussian-masked directional attention can be described as a function to map queries and key-value pairs to the representation of input, Gaussian-masked directional attention expects to pay attention to the adjacent characters of each positions and cast the localness relationship between characters as a fix Gaussian weight for attention, Gaussian weight only relys on the distance between characters"], "all_classes": null, "length": 4096}
{"pred": "Twitter\n\nQuestion: What is the main point of the article?\n\nAnswer: The article is about the use of causal explanations in social media.\n\nQuestion: What is the main point of the article?\n\nAnswer: The article is about the use of causal explanations in social media.\n\nQuestion: What is the main point of the article?\n\nAnswer: The article is about the use of causal explanations in social media.\n\nQuestion: What is the main point of the article?\n\nAnswer: The article is about the use of causal explanations in social media", "answers": ["Facebook status update messages", "Facebook status update messages"], "all_classes": null, "length": 4096}
{"pred": "The baseline features are the features extracted from the CNN.\n\nQuestion: What are the pre-trained features?\n\nAnswer: The pre-trained features are the features extracted from the pre-trained sentiment, emotion and personality models.\n\nQuestion: What is the best performing feature set?\n\nAnswer: The best performing feature set is the best performing feature set is the baseline features.\n\nQuestion: What is the best performing feature set?\n\nAnswer: The best performing feature set is the best performing feature set is the best performing feature set is the best performing feature set is the", "answers": [" The features extracted from CNN."], "all_classes": null, "length": 4096}
{"pred": "The hyperparameters varied in the experiments on the four tasks are: (i) the number of clusters, (ii) the number of classes, (iii) the type of word vectors (skipgram, cbow, or GloVe) and (iv) the type of clustering algorithm (k-means, k-means++ or k-means++).\n\n\nAnswer: The hyperparameters varied in the experiments on the four tasks are: (i) the number of clusters, (ii) the number of classes, (iii) the type of word vectors (skipgram, cbow, or Glo", "answers": ["number of clusters, seed value in clustering, selection of word vectors, window size and dimension of embedding", "different number of clusters, different embeddings"], "all_classes": null, "length": 4096}
{"pred": "The scores of our system are shown in Table TABREF19 .\n\nQuestion: What were the scores of their system?\n\nAnswer: The scores of our system are shown in Table TABREF19 .\n\nQuestion: What were the scores of their system?\n\nAnswer: The scores of our system are shown in Table TABREF19 .\n\nQuestion: What were the scores of their system?\n\nAnswer: The scores of our system are shown in Table TABREF19 .\n\nQuestion: What were the scores of their system?\n\nAnswer: The scores of our", "answers": ["column Ens Test in Table TABREF19"], "all_classes": null, "length": 3552}
{"pred": "The corpus is 53 documents, which contain an average number of 156.1 sentences per document, each with 19.55 tokens on average.\n\nQuestion: What is the average length of the entities?\n\nAnswer: The numbers of annotated entities are summarized in Table TABREF24. The numbers of annotated entities are summarized in Table TABREF24. The numbers of annotated entities are summarized in Table TABREF24. The numbers of annotated entities are summarized in Table TABREF3.\n\nQuestion: What is the", "answers": ["8,275 sentences and 167,739 words in total", "The corpus comprises 8,275 sentences and 167,739 words in total."], "all_classes": null, "length": 4097}
{"pred": "Yes, it is possible to convert a cloze-style questions to a naturally-looking questions.\n\nQuestion: What is the best way to convert a cloze-style questions to a naturally-looking questions?\n\nAnswer: The best way to convert a cloze-style questions to a naturally-looking questions is to use a question-style questions is to use a question is to use a question is to use a question is to use a question is to use a question is to use a question is to use a question is to use a question is to use a question is to use a question is to use a question", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 4096}
{"pred": "We possess a wealth of prior knowledge about many natural language processing tasks. For example, in text categorization, we know that words such as player, basketball, and football are strong indicators of the sports category.\n\nQuestion: What is the problem?\n\nAnswer: How to leverage such knowledge for the problem falls into several lines.\n\nQuestion: What is the problem in the framework of GE criteria which can be commonly seen in many as prior knowledge with the model.\n\nQuestion: What is the problem in the model with heavy bias to label data\n\nAnswer: We do not have enough knowledge to provide", "answers": ["text classification for themes including sentiment, web-page, science, medical and healthcare"], "all_classes": null, "length": 4096}
{"pred": "The model is compared to the TREC question classification model of Li and Roth BIBREF6 , which is a rule-based model that uses a combination of syntactic, semantic, surface, and embedding methods to classify questions into 6 coarse answer types of questions and 50 fine-grained taxonomy. The model is a CNN model, which is a learned model, which is a CNN model, which is a BERT model, which is a BERT model, which is a LSTM model, which is a LSTM model, which is a LSTM model, which is a", "answers": ["bag-of-words model, CNN"], "all_classes": null, "length": 4097}
{"pred": "The training sets of these versions of ELMo are larger than the previous ones.\n\nQuestion: What is the size of the training sets of these versions of ELMo?\n\nAnswer: The size of the training sets of these versions of ELMo are 20 million tokens.\n\nQuestion: What is the size of the training sets of these versions of ELMo?\n\nAnswer: The size of the training sets of these versions of ELMo are 20 million tokens.\n\nQuestion: What is the size of the size of the training sets of these versions of ELMo?\n\nAnswer: The size", "answers": ["By 14 times.", "up to 1.95 times larger"], "all_classes": null, "length": 4096}
{"pred": "10000\n\nQuestion: What is the average length of a sentence in the dataset?\n\nAnswer: 10\n\nQuestion: What is the average number of entities per sentence?\n\nAnswer: 1.5\n\nQuestion: What is the average number of entities per sentence?\n\nAnswer: 1.5\nQuestion: What is the average number of entities per sentence?\n\nAnswer: 1\n\nQuestion: What is the average number of entities per sentence?\n\nAnswer: 1\nQuestion: What is the average number of entities per sentence?\n\nAnswer", "answers": ["3606", "6946"], "all_classes": null, "length": 4096}
{"pred": "They compare to the following models/frameworks:\n\n1. Speech/Music discrimination:\n\na. Multi-feature speech/music discrimination system [1]\n\nb. Deep learning perspective [2]\n\n\nc. Speech-music discrimination: A deep learning perspective [3]\n\nd. Feature learning with raw-waveform CLDNNs [4]\n\n\n2. Emotion classification:\n\n\na. Feature learning with raw-waveform CLDNNs [5]\n\nb. Feature learning with raw-wave", "answers": ["MLP", "Eusboost, MWMOTE"], "all_classes": null, "length": 4096}
{"pred": "Yes.\n\nQuestion: How does their model learn NER from both text and images?\n\nAnswer: The model learns NER from both text and images by combining word and character embeddings from both modalities.\n\nQuestion: What is the modality attention module?\n\nAnswer: The modality attention module is a new neural mechanism that learns to combine word and character embeddings from both modalities.\n\nQuestion: How does their modality attention module improve the model?\n\nAnswer: The modality attention module improves the model by better combining word and character embeddings from both", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4096}
{"pred": "No.\n\nQuestion: Do they evaluate only on English datasets?\n\nAnswer: No.\n\nQuestion: Do they evaluate only on English datasets?\n\nAnswer: No.\n\nQuestion: Do they evaluate only on English datasets?\n\nAnswer: No.\n\nQuestion: Do they evaluate only on English datasets?\nAnswer: No.\nQuestion: Do they evaluate only on English datasets?\nAnswer: No.\nQuestion: Do they evaluate only on English datasets?\nAnswer: No.\nQuestion: Do they evaluate only on English datasets?\nAnswer: No.\nQuestion: Do they evaluate only", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 4097}
{"pred": "0.6103\n\nQuestion: What was their highest â€˜MRRâ€™ score?\n\nAnswer: 0.6103\n\nQuestion: What was their highest â€˜MRRâ€™ score?\n\n\nAnswer: 0.6103\n\nQuestion: What was their highest â€˜MRRâ€™ score?\n\nAnswer: 0.6103\n\nQuestion: What was their highest â€˜MRRâ€™ score?\n\nAnswer: 0.6103\n\nQuestion: What was their highest â€˜MRRâ€™ score?\n\nAnswer: 0", "answers": ["0.5115", "0.6103"], "all_classes": null, "length": 4097}
{"pred": "They evaluate on sections 02-21 of WSJ corpus.\n\nQuestion: What is the length of the sentences?\n\nAnswer: The length of the sentences is 100.\n\nQuestion: What is the length of the sentences?\n\nAnswer: The length of the sentences is 100.\n\nQuestion: What is the length of the sentences?\n\nAnswer: The length of the sentences is 100.\n\nQuestion: What is the length of the sentences?\n\nAnswer: The length of the sentences is 10.\n\nQuestion:", "answers": [" Wall Street Journal (WSJ) portion of the Penn Treebank", "Unanswerable"], "all_classes": null, "length": 4096}
{"pred": "The authors evidence the claim that many engineers find it a big overhead to choose from multiple frameworks, models and optimization techniques by conducting a survey among engineers and identifying a spectrum of three typical personas.\n\nQuestion: What are the three typical personas?\n\nAnswer: The three typical personas are:\n\n1. Engineers who are familiar with the frameworks, models and optimization techniques.\n\n2. Engineers who are familiar with the frameworks, but not models and optimization techniques.\n\n3. Engineers who are familiar with the models and optimization techniques, but not frameworks.\n\nQuestion: What are the", "answers": ["By conducting a survey among engineers", "Unanswerable"], "all_classes": null, "length": 2833}
{"pred": "We evaluate our model on two benchmarks: (1) SimpleQuestions BIBREF2 , which contains 14% of the golden training tuples not observed in golden training tuples. (2) WebQuestions BIBREF20 , which contains 100% of the golden training tuples not observed in the golden training tuples.\n\n\nQuestion: What is the relation detection model?\n\nAnswer: We propose to deal \"yes\", \"no\", or \"unanswerable\".\n\nQuestion: What is the relation detection model?\n\nAnswer: Hierarchical Matching between Question", "answers": ["SimpleQuestions, WebQSP", "WebQSP, SimpleQuestions"], "all_classes": null, "length": 4096}
