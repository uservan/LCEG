{"pred": "The ground truth for fake news is established by an expert.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "answers": ["Ground truth is not established in the paper"], "all_classes": null, "length": 3141}
{"pred": "GhostVLAD is an extension of the NetVLAD approach, which we discussed in the previous section. The GhostVLAD model was proposed for face recognition by Y. Zhong [10]. GhostVLAD works exactly similar to NetVLAD except it adds Ghost clusters along with the NetVLAD clusters. So, now we will have a K+G number of clusters instead of K clusters. Where G is the number of ghost clusters, we want to add (typically 2-4). The Ghost clusters are added to map any noisy or irrelevant content into ghost clusters and are", "answers": ["extension of the NetVLAD, adds Ghost clusters along with the NetVLAD clusters", "An extension of NetVLAD which replaces hard assignment-based clustering with soft assignment-based clustering with the additon o fusing Ghost clusters to deal with noisy content."], "all_classes": null, "length": 2454}
{"pred": "The proposed model outperforms the state of the art results by 68.8% to 71.8%.\n\nQuestion: What is the name of the dataset used in the experiments?\n\nAnswer: The dataset used in the experiments is the Interactive Emotional Dyadic Motion Capture (IEMOCAP) dataset.\n\nQuestion: What is the name of the toolkit used to extract the MFCC features?\n\nAnswer: The toolkit used to extract the MFCC features is the openSMILE toolkit.\n\nQuestion: What is the name of the", "answers": ["the attention model, MDREA, also outperforms the best existing research results (WAP 0.690 to 0.688)"], "all_classes": null, "length": 3207}
{"pred": "We use the context tweets of the tweets as additional features.\n\nQuestion: What is the size of the dataset?\n\nAnswer: The dataset contains 100K tweets.\n\nQuestion: What is the distribution of labels?\n\nAnswer: The dataset contains 4 labels: \"normal\", \"spam\", \"hateful\", and \"abusive\".\n\nQuestion: What is the size of the training set?\n\nAnswer: The training set contains 70,904 tweets.\n\nQuestion: What is the size of the test set?\n\nAnswer:", "answers": ["using tweets that one has replied or quoted to as contextual information", "text sequences of context tweets"], "all_classes": null, "length": 2060}
{"pred": "They looked at FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney.\n\nQuestion: What emotions did they use?\n\nAnswer: They used anger, joy, sadness, surprise.\n\nQuestion: What was the best model?\n\nAnswer: The best model was the one that used Time, The Guardian and Disney.\n\nQuestion: What was the best feature?\n\nAnswer", "answers": ["FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney", "FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney."], "all_classes": null, "length": 3411}
{"pred": "Yes, the hashtag dataset contains only English data. The SemEval dataset contains both English and non-English data.\n\nQuestion: What is the size of the hashtag dataset?\n\nAnswer: The hashtag dataset contains 12,594 unique hashtags and their associated tweets.\n\nQuestion: What is the size of the SemEval dataset?\n\nAnswer: The SemEval dataset contains 12,594 unique hashtags and their associated tweets.\n\nQuestion: What is the size of the STAN dataset?\n\nAnswer:", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3735}
{"pred": "We propose a new evaluation protocol that is based on the manual evaluation of the reference concept maps.\n\nQuestion: What is the size of the corpus?\n\nAnswer: The corpus consists of 30 topics, each with around 40 documents and a summarizing concept map.\n\nQuestion: What is the size of the reference concept maps?\n\nAnswer: The maps have on average 25 concepts and 24 to 28 relations.\n\nQuestion: What is the size of the documents?\n\nAnswer: The documents have on average 2413 tokens, leading to an", "answers": ["Answer with content missing: (Evaluation Metrics section) Precision, Recall, F1-scores, Strict match, METEOR, ROUGE-2"], "all_classes": null, "length": 4263}
{"pred": "We evaluated our model on three benchmark datasets, namely the CNN/DailyMail news highlights dataset BIBREF24, the New York Times Annotated Corpus (NYT; BIBREF25), and XSum BIBREF22. These datasets represent different summary styles ranging from highlights to very brief one sentence summaries. The summaries also vary with respect to the type of rewriting operations they exemplify (e.g., some showcase more cut and paste operations while others are genuinely abstractive). Table TABREF12 presents statistics on these datasets (test set);", "answers": ["CNN/DailyMail news highlights, New York Times Annotated Corpus, XSum", "the CNN/DailyMail news highlights dataset BIBREF24, the New York Times Annotated Corpus (NYT; BIBREF25), XSum BIBREF22"], "all_classes": null, "length": 4369}
{"pred": "The proposed approach is a variant of max-margin objective based on the asymmetric KL divergence energy function to capture textual entailment. The proposed approach captures polysemy and also captures entailment relations among the words. The proposed approach is compared with the previous approaches w2g BIBREF9 ( single Gaussian model) and w2gm BIBREF10 (mixture of Gaussian model with expected likelihood kernel). The proposed approach GM$\\_$KL achieves better correlation than existing approaches for various metrics on SCWS dataset.\n\n\n\n\n\n\n\n\n\n", "answers": ["GM$\\_$KL achieves better correlation than existing approaches for various metrics on SCWS dataset."], "all_classes": null, "length": 2189}
{"pred": "The ensemble method is a technique that combines the predictions of multiple models to improve the overall accuracy of the system. In the case of the BookTest dataset, the ensemble method combines the predictions of multiple models trained on the BookTest dataset to improve the overall accuracy of the system.\nQuestion: How does the BookTest dataset differ from the CBT dataset?\n\nAnswer: The BookTest dataset is a dataset of textual passages and questions that is similar to the Children's Book Test dataset, but it is much larger and more diverse. The BookTest dataset was created by automatically generating questions from the text of books", "answers": ["simply averaging the predictions from the constituent single models"], "all_classes": null, "length": 4212}
{"pred": "The source of the Friends dataset is the scripts of the Friends TV sitcom. The EmotionPush dataset is made up of Facebook messenger chats.\n\nQuestion: What are the properties of the datasets?\n\nAnswer: The Friends dataset is a speech-based dataset which is annotated dialogues from the TV sitcom. The EmotionPush dataset does not have this trait due to the anonymous mechanism. In addition, features such as typo, hyperlink, and emoji that only appear in chat-based data will need some domain-specific techniques to process.\n\nQuestion: What are the properties", "answers": ["Friends TV sitcom, Facebook messenger chats"], "all_classes": null, "length": 3185}
{"pred": "The paper focuses on English.\n\nQuestion: what is the main contribution of this paper?\n\nAnswer: The main contribution of this paper is to show that we can effectively adapt neural translation models for text simplification with simplified corpora.\n\nQuestion: what is the main limitation of this paper?\n\nAnswer: The main limitation of this paper is that we only use the simplified corpora for training the NMT model, and we do not use the simplified corpora for testing.\n\nQuestion: what is the main conclusion of this paper?\n\nAnswer: The main conclusion of this paper is that we can", "answers": ["English", "Simple English"], "all_classes": null, "length": 2243}
{"pred": "IMDb dataset of movie reviews by BIBREF11\n\nQuestion: What is the name of the NER dataset used?\n\nAnswer: Groningen Meaning Bank (GMB) by BIBREF12\n\nQuestion: What is the name of the SA dataset used?\n\nAnswer: IMDb dataset of movie reviews by BIBREF11\n\nQuestion: What is the name of the NER dataset used?\n\nAnswer: Groningen Meaning Bank (GMB) by BIBREF12\n\nQuestion: What is the name of the SA dataset used?\n\nAnswer:", "answers": ["IMDb dataset of movie reviews", "IMDb"], "all_classes": null, "length": 2327}
{"pred": "The proposed system achieves 89.2% F1 on the DL-PS dataset, 88.2% F1 on the EC-MT dataset, and 89.1% F1 on the EC-UQ dataset.\n\nQuestion: What is the difference between the proposed system and the baseline system?\n\nAnswer: The proposed system is based on the adversarial learning, which is able to learn worker independent features from the crowd-annotated data.\n\nQuestion: What is the difference between the proposed system and the baseline system?\n\nAnswer: The proposed system is", "answers": ["F1 scores of 85.99 on the DL-PS data,  75.15 on the EC-MT data and 71.53 on the EC-UQ data ", "F1 of 85.99 on the DL-PS dataset (dialog domain); 75.15 on EC-MT and 71.53 on EC-UQ (e-commerce domain)"], "all_classes": null, "length": 5310}
{"pred": "Yes, they did.\n\nQuestion: What did they do?\n\nAnswer: They compared the eye-tracking and EEG data of normal reading and task-specific reading.\n\nQuestion: What did they find?\n\nAnswer: They found that the eye-tracking and EEG data of normal reading and task-specific reading are similar.\n\nQuestion: What did they conclude?\n\nAnswer: They concluded that the eye-tracking and EEG data of normal reading and task-specific reading are similar.\n\nQuestion: What did they conclude about the eye-tracking and EEG data", "answers": ["No"], "all_classes": null, "length": 3445}
{"answers": "", "length": 13401}
{"answers": "", "length": 10349}
{"pred": "RNN-based NMT and Transformer-NMT.\n\nQuestion: what is the size of the dataset?\n\nAnswer: 1.24M bilingual sentence pairs.\n\nQuestion: what is the size of the training set?\n\nAnswer: 48K bilingual sentence pairs.\n\nQuestion: what is the size of the test set?\n\nAnswer: 48K bilingual sentence pairs.\n\nQuestion: what is the size of the dev set?\n\nAnswer: 2K bilingual sentence pairs.\n\nQuestion: what is the", "answers": ["RNN-based NMT model, Transformer-NMT"], "all_classes": null, "length": 3708}
{"pred": "The three regularization terms are: (1) a regularization term associated with neutral features; (2) the maximum entropy of class distribution regularization term; and (3) the KL divergence between reference and predicted class distribution.\n\nQuestion: What is the difference between the maximum entropy of class distribution regularization term and the KL divergence between reference and predicted class distribution?\n\nAnswer: The maximum entropy of class distribution regularization term is to constrain the model to predict the class distribution that is uniform distributed. The KL divergence between reference and predicted class distribution is to constrain the model to predict the", "answers": ["a regularization term associated with neutral features, the maximum entropy of class distribution regularization term, the KL divergence between reference and predicted class distribution", "a regularization term associated with neutral features,  the maximum entropy of class distribution, KL divergence between reference and predicted class distribution"], "all_classes": null, "length": 3604}
{"pred": "The baselines are the following: 1) SVM with unigram, bigram, and trigram features, which is a standard yet rather strong classifier for text features; 2) SVM with average word embedding, where a document is represented as a continuous representation by averaging the embeddings of the composite words; 3) SVM with average transformed word embeddings (the INLINEFORM0 in equation EQREF6 ), where a document is represented as a continuous representation by averaging the transformed embeddings of the composite words; 4) two mature deep learning models on text classification", "answers": ["SVM with unigram, bigram, and trigram features, SVM with average word embedding, SVM with average transformed word embeddings, CNN, ecurrent Convolutional Neural Networks, SVM and deep learning models with comment information", "SVM with unigram, bigram, trigram features, with average word embedding, with average transformed word embeddings, CNN and RCNN, SVM, CNN, RCNN with comment information"], "all_classes": null, "length": 4512}
{"pred": "They improved by 10%.\n\nQuestion: How much did they improve?\n\nAnswer: They improved by 10%.\n\nQuestion: How much did they improve?\n\nAnswer: They improved by 10%.\n\nQuestion: How much did they improve?\n\nAnswer: They improved by 10%.\n\nQuestion: How much did they improve?\n\nAnswer: They improved by 10%.\n\nQuestion: How much did they improve?\n\nAnswer: They improved by 10%.\n\nQuestion: How much did they improve?\n\nAnswer: They improved", "answers": ["They decrease MAE in 0.34"], "all_classes": null, "length": 2735}
{"pred": "The authors of the article propose a novel Transformer architecture wherein they simply replace softmax with $\\alpha$-entmax in the attention heads. Concretely, they replace the row normalization $\\pi$ in Equation DISPLAY_FORM7 by\nThis change leads to sparse attention weights, as long as $\\alpha >1$; in particular, $\\alpha =1.5$ is a sensible starting point.\nThe authors further propose an adaptive version of sparse attention, where the shape of each attention head is learnable and can vary continuously and dynamically between the dense limit case of softmax and the sparse, piece", "answers": ["the attention heads in the proposed adaptively sparse Transformer can specialize more and with higher confidence", "We introduce sparse attention into the Transformer architecture"], "all_classes": null, "length": 4902}
{"pred": "the baseline is the sentence-level MT system.\n\nQuestion: what is the DocRepair model?\n\nAnswer: the DocRepair model is a monolingual sequence-to-sequence model that maps inconsistent groups of sentences into consistent ones.\n\nQuestion: what is the training process of the DocRepair model?\n\nAnswer: the training process of the DocRepair model involves the following steps:\n\n*\n\n*sample several groups of sentences from the monolingual data;\n\n*for each sentence in a group, (i) translate it using a target-", "answers": [" MT system on the data released by BIBREF11", "Transformer base, two-pass CADec model"], "all_classes": null, "length": 3716}
{"pred": "We use the following metrics for evaluation:\n\n*\n\n*Accuracy for XNLI\n\n*LAS for dependency parsing\n\n*\n\n*Accuracy for XNLI: Accuracy is the percentage of correct predictions.\n\n*LAS for dependency parsing: LAS is the average of the scores of all the arcs in the dependency tree.\n\nQuestion: What is the difference between the two models?\n\nAnswer: The difference between the two models is the number of parameters. RAMEN$_{\\textsc {base}}$ has 110M parameters and RAMEN$", "answers": ["translation probabilities, Labeled Attachment Scores (LAS)", "accuracy, Labeled Attachment Scores (LAS)"], "all_classes": null, "length": 3402}
{"pred": "The attention module is pretrained on MT task.\n\nQuestion: What is the length of the speech encoder output?\n\nAnswer: The length of the speech encoder output is proportional to the length of the input frame.\n\nQuestion: What is the length of the text encoder output?\n\nAnswer: The length of the text encoder output is proportional to the length of the input sentence.\n\nQuestion: What is the length of the decoder output?\n\nAnswer: The length of the decoder output is proportional to the length of the input sentence.\n\nQuestion: What is the", "answers": ["the model is pre-trained on CTC-based ASR task and MT task in the pre-training stage."], "all_classes": null, "length": 4656}
{"pred": "The features obtained are: (a) Unigrams and Pragmatic features (b) Stylistic patterns (c) patterns related to situational disparity (d) Hastag interpretations\n\nQuestion: What is the accuracy of polarity annotation?\n\nAnswer: The accuracy of polarity annotation varies between 72%-91% for sarcastic texts and 75%-91% for non-sarcastic text, showing the inherent difficulty of sentiment annotation, when sarcasm is present in the text under consideration.\n\nQuestion: What is the accuracy", "answers": ["Unanswerable"], "all_classes": null, "length": 3543}
{"pred": "The encoder is an LSTM.\n\nQuestion: What is the size of the LSTM layer?\n\nAnswer: The LSTM layer is 100.\n\nQuestion: What is the size of the embedding layer?\n\nAnswer: The embedding layer is 100.\n\nQuestion: What is the size of the attention layer?\n\nAnswer: The attention layer is 100.\n\nQuestion: What is the size of the dropout layer?\n\nAnswer: The dropout layer is 0.3.\n\nQuestion: What is the learning rate", "answers": ["LSTM", "LSTM"], "all_classes": null, "length": 2289}
{"pred": "Yes, WordNet is useful for taxonomic reasoning for this task.\n\nQuestion: Is WordNet useful for taxonomic reasoning for this task?\n\nAnswer: No, WordNet is not useful for taxonomic reasoning for this task.\n\nQuestion: Is WordNet useful for taxonomic reasoning for this task?\n\nAnswer: Unanswerable.\n\nQuestion: Is WordNet useful for taxonomic reasoning for this task?\n\nAnswer: Unanswerable.\n\nQuestion: Is WordNet useful for taxonomic reasoning for this task?\n\nAnswer: Unanswerable.\n", "answers": ["Unanswerable", "Yes"], "all_classes": null, "length": 6391}
{"pred": "We compare to other models trained using the same data and report Hub5'00 results in Table TABREF31 .\n\nQuestion: what is the difference between the 10x3 and 10x5 models?\n\nAnswer: We trained a smaller Jasper 10x3 model with SGD with momentum optimizer for 400 epochs on a combined WSJ dataset (80 hours): LDC93S6A (WSJ0) and LDC94S13A (WSJ1). The results are provided in Table TABREF2", "answers": ["Unanswerable", "LF-MMI Attention\nSeq2Seq \nRNN-T \nChar E2E LF-MMI \nPhone E2E LF-MMI \nCTC + Gram-CTC"], "all_classes": null, "length": 1856}
{"pred": "Unanswerable\n\nQuestion: How many users do they look at?\n\nAnswer: Unanswerable\n\nQuestion: How many users do they look at?\n\nAnswer: Unanswerable\n\nQuestion: How many users do they look at?\n\nAnswer: Unanswerable\n\nQuestion: How many users do they look at?\n\nAnswer: Unanswerable\n\nQuestion: How many users do they look at?\n\nAnswer: Unanswerable\n\nQuestion: How many users do they look at?\n\nAnswer: Unanswerable\n\nQuestion: How many users do they look at", "answers": ["22,880 users", "20,000"], "all_classes": null, "length": 4160}
{"pred": "We use BLEU-1/4, ROUGE-L, Distinct-1/2, and Recipe-level Coherence.\n\nQuestion: What is the size of the dataset?\n\nAnswer: We have 180K+ recipes and 700K+ user reviews.\n\nQuestion: What is the size of the vocabulary?\n\nAnswer: We use BPE tokenization with 15K tokens.\n\nQuestion: What is the size of the user profiles?\n\nAnswer: We have 180K+ users.", "answers": ["Byte-Pair Encoding perplexity  (BPE PPL),\nBLEU-1,\nBLEU-4,\nROUGE-L,\npercentage of distinct unigram (D-1),\npercentage of distinct bigrams(D-2),\nuser matching accuracy(UMA),\nMean Reciprocal Rank(MRR)\nPairwise preference over baseline(PP)", "BLEU-1/4 and ROUGE-L, likelihood of generated recipes using identical input specifications but conditioned on ten different user profiles, user matching accuracy (UMA), Mean Reciprocal Rank (MRR), neural scoring model from BIBREF33 to measure recipe-level coherence", " Distinct-1/2, UMA = User Matching Accuracy, MRR\n= Mean Reciprocal Rank, PP = Pairwise preference over baseline (evaluated for 310 recipe pairs per model)"], "all_classes": null, "length": 2673}
{"pred": "The labels are the same as the ones in the article.\n\nQuestion: What is the dataset?\n\nAnswer: The dataset is the same as the one in the article.\n\nQuestion: What is the question?\n\nAnswer: The question is the same as the one in the article.\n\nQuestion: What is the answer?\n\nAnswer: The answer is the same as the one in the article.\n\nQuestion: What is the answer?\n\nAnswer: The answer is the same as the one in the article.\n\nQuestion: What is the answer?\n\nAnswer: The answer is", "answers": ["(1) the time the patient has been experiencing the symptom, (2) activities that trigger the symptom (to occur or worsen), (3) the extent of seriousness, (4) the frequency occurrence of the symptom, and (5) the location of symptom, No Answer", "the time the patient has been experiencing the symptom, activities that trigger the symptom, the extent of seriousness, the frequency occurrence of the symptom, the location of symptom, 9 symptoms"], "all_classes": null, "length": 3424}
{"pred": "The task-specific encoder is trained on the same data as the universal encoder.\n\nQuestion: How many sentences are in the training set?\n\nAnswer: The training set contains 57,505 sentences.\n\nQuestion: How many sentences are in the test set?\n\nAnswer: The test set contains 2,428 sentences.\n\nQuestion: How many sentences are in the training set that contain PICO spans?\n\nAnswer: 4,741 sentences contain PICO spans.\n\nQuestion: How many sentences are in the test set that", "answers": ["57,505 sentences", "57,505 sentences"], "all_classes": null, "length": 4371}
{"pred": "The tasks used for evaluation are:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "answers": ["four machine translation tasks: German -> English, Japanese -> English, Romanian -> English, English -> German", " four machine translation tasks, IWSLT 2017 German $\\rightarrow $ English BIBREF27, KFTT Japanese $\\rightarrow $ English BIBREF28, WMT 2016 Romanian $\\rightarrow $ English BIBREF29, WMT 2014 English $\\rightarrow $ German BIBREF30"], "all_classes": null, "length": 4898}
{"pred": "The improvement in performance for Estonian in the NER task is significant.\n\nQuestion: What is the improvement in performance for Latvian in the NER task?\n\nAnswer: The improvement in performance for Latvian in the NER task is significant.\n\nQuestion: What is the improvement in performance for Slovenian in the NER task?\n\nAnswer: The improvement in performance for Slovenian in the NER task is significant.\n\nQuestion: What is the improvement in performance for Croatian in the NER task?\n\nAnswer: The improvement in performance for Croatian in the N", "answers": ["5 percent points.", "0.05 F1"], "all_classes": null, "length": 3290}
{"pred": "They have a background in computational linguistics, social science, and the humanities.\n\nQuestion: What is the purpose of the article?\n\nAnswer: The purpose of the article is to describe the research process of computational text analysis.\n\nQuestion: What is the main point of the article?\n\nAnswer: The main point of the article is to describe the research process of computational text analysis.\n\nQuestion: What is the main idea of the article?\n\nAnswer: The main idea of the article is to describe the research process of computational text analysis.\n\nQuestion: What is the main conclusion of the", "answers": ["Unanswerable"], "all_classes": null, "length": 8506}
{"pred": "No, the paper is not introducing an unsupervised approach to spam detection. The paper is introducing a supervised approach to spam detection.\n\nQuestion: What is the difference between the two datasets?\n\nAnswer: The Honeypot dataset is a public dataset, and the Weibo dataset is a self-collected dataset.\n\nQuestion: What is the difference between the two features?\n\nAnswer: The two features are different in the way of calculating. The first one is the global outlier standard score, and the second one is the local outlier standard score.\n\nQuestion", "answers": ["No", "No"], "all_classes": null, "length": 2239}
{"pred": "The Nguni languages are similar to each other and harder to distinguish. The same is true of the Sotho languages.\n\nQuestion: Which languages are disjunctively written?\n\nAnswer: The Sotho languages are disjunctively written.\n\nQuestion: Which languages are conjunctively written?\n\nAnswer: The Nguni languages are conjunctively written.\n\nQuestion: Which languages are written in the Latin script?\n\nAnswer: The Nguni languages are written in the Latin script.\n\nQuestion: Which languages are written in the Cyrillic script?\n\nAnswer:", "answers": ["Nguni languages (zul, xho, nbl, ssw), Sotho languages (nso, sot, tsn)", "The Nguni languages are similar to each other, The same is true of the Sotho languages"], "all_classes": null, "length": 1877}
{"pred": "They compared with 2-layers LSTM, 3-layers LSTM, 4-layers LSTM, 5-layers LSTM, 6-layers LSTM, 7-layers LSTM, 8-layers LSTM, 9-layers LSTM, 10-layers LSTM, 11-layers LSTM, 12-layers LSTM, 13-layers LSTM, 14-layers LSTM, 15-layers LSTM, 16-layers LST", "answers": ["Unidirectional LSTM networks with 2, 6, 7, 8, and  9 layers."], "all_classes": null, "length": 4286}
{"pred": "The Wikipedia dataset consists of articles from English Wikipedia, with quality class labels assigned by the Wikipedia community. Wikipedia articles are labelled with one of six quality classes, in descending order of quality: Featured Article (“FA”), Good Article (“GA”), B-class Article (“B”), C-class Article (“C”), Start Article (“Start”), and Stub Article (“Stub”). A description of the criteria associated with the different classes can be found in the Wikipedia grading scheme page. The quality class of a Wikipedia article is assigned by Wikipedia reviewers or any registered user", "answers": ["a sample of  29,794 wikipedia articles and 2,794 arXiv papers "], "all_classes": null, "length": 4187}
{"pred": "The human judgements were assembled by a group of 50 native people who were well-versed in both English and Tamil languages.\n\nQuestion: What is the difference between the RNNMorph and RNNSearch models?\n\nAnswer: The RNNMorph model uses morphological segmentation to split the words into morphemes. The RNNSearch model uses word2vec embeddings to embed semantic understanding.\n\nQuestion: What is the difference between the RNNMorph and RNNSearch + Word2Vec models?\n\nAnswer: The RNNMorph model", "answers": ["50 human annotators ranked a random sample of 100 translations by Adequacy, Fluency and overall ranking on a 5-point scale.", "adequacy, precision and ranking values"], "all_classes": null, "length": 5344}
{"pred": "Yes, they do. They test their framework performance on commonly used language pairs, such as English-to-German.\n\nQuestion: What is the difference between the two approaches?\n\nAnswer: The difference between the two approaches is that the first approach uses a single encoder and decoder for all languages, while the second approach uses multiple encoders and decoders for each language.\n\nQuestion: What is the advantage of using a single encoder and decoder for all languages?\n\nAnswer: The advantage of using a single encoder and decoder for all languages is that it simplifies the training", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4472}
{"pred": "The models are evaluated by measuring the accuracy of the decoding strategy $p_{\\beta }(x\\mid z)$ and the efficiency of the encoding strategy $q_{\\alpha }(z\\mid x)$.\n\nQuestion: What is the difference between the two baselines Unif and Stopword?\n\nAnswer: Unif is a rule-based baseline which randomly keeps tokens to generate keywords with the probability $\\delta $. Stopword is a rule-based baseline which keeps all tokens but drops stop words (e.g. `the', `a', `or') all the time ($\\delta =0$)", "answers": ["by training an autocomplete system on 500K randomly sampled sentences from Yelp reviews", "efficiency of a communication scheme $(q_{\\alpha },p_{\\beta })$ by the retention rate of tokens, which is measured as the fraction of tokens that are kept in the keywords, accuracy of a scheme is measured as the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence"], "all_classes": null, "length": 1873}
{"pred": "We used precision, recall and F-measure for classification tasks.\n\nQuestion: What is the difference between the two datasets used in this paper?\n\nAnswer: Dataset D1 contains 2000 sentences randomly selected from the supervisor assessment corpus. Dataset D2 contains 2000 sentences randomly selected from the supervisor assessment corpus, where each sentence is tagged with attributes.\n\nQuestion: What is the difference between the two datasets used in this paper?\n\nAnswer: Dataset D1 contains 2000 sentences randomly selected from the supervisor assessment cor", "answers": ["Precision, Recall, F-measure, accuracy", "Precision, Recall and F-measure"], "all_classes": null, "length": 3044}
{"pred": "The source domain is the domain of the labeled data, and the target domain is the domain of the unlabeled data.\nQuestion: What is the difference between the source and target domains?\n\nAnswer: The difference between the source and target domains is that the vocabulary of the target domain is different from that of the source domain.\nQuestion: What is the purpose of the proposed method?\n\nAnswer: The purpose of the proposed method is to reduce the domain discrepancy between the source and target domains.\nQuestion: What is the difference between the proposed method and the baseline methods?\n", "answers": ["Book, electronics, beauty, music, IMDB, Yelp, cell phone, baby, DVDs, kitchen", "we use set 1 of the source domain as the only source with sentiment label information during training, and we evaluate the trained model on set 1 of the target domain, Book (BK), Electronics (E), Beauty (BT), and Music (M)"], "all_classes": null, "length": 5061}
{"pred": "We compare with the following models: RAN BIBREF7 , QRNN BIBREF33 , NAS BIBREF31 , and AWD-LSTM BIBREF0 .\n\nQuestion: what is the difference between the pyramidal transformation and the linear transformation?\n\nAnswer: The pyramidal transformation is a non-linear transformation that uses subsampling to effect multiple views of the input vector. The subsampled representations are combined in a pyramidal fusion structure, resulting in richer interactions between the individual dimensions of the input vector than is possible with a linear transformation.\n\n", "answers": ["Variational LSTM, CharCNN, Pointer Sentinel-LSTM, RHN, NAS Cell, SRU, QRNN, RAN, 4-layer skip-connection LSTM, AWD-LSTM, Quantized LSTM"], "all_classes": null, "length": 3319}
{"pred": "NeuronBlocks provides a gallery of neural network modules, including embedding, CNN, RNN, Transformer, attention, etc.\n\nQuestion: What are the supported NLP tasks in NeuronBlocks?\n\nAnswer: NeuronBlocks supports four types of NLP tasks, including text classification, sequence labeling, knowledge distillation, and machine reading comprehension.\n\nQuestion: How to build a model in NeuronBlocks?\n\nAnswer: Users can either instantiate an existing template from Model Zoo, or construct a new architecture based on the blocks from Block Zoo.", "answers": ["Embedding Layer, Neural Network Layers, Loss Function, Metrics", "Embedding Layer, Neural Network Layers, Loss Function, Metrics"], "all_classes": null, "length": 1678}
{"pred": "The data used for training and testing is the multilingual pronunciation corpus collected by deri2016grapheme. It is already partitioned into training and test sets. Corpus statistics are presented in Table TABREF10 .\n\nQuestion: what is the difference between the LangID and NoLangID versions of the model?\n\nAnswer: The LangID version of the model prepends each training, validation, and test sample with an artificial token identifying the language of the sample. NoLangID omits this token. LangID and NoLangID have identical structure otherwise.", "answers": ["the Carnegie Mellon Pronouncing Dictionary BIBREF12, the multilingual pronunciation corpus collected by deri2016grapheme , ranscriptions extracted from Wiktionary", "multilingual pronunciation corpus collected by deri2016grapheme"], "all_classes": null, "length": 3244}
{"pred": "The baselines used for the task of speculation detection and scope resolution were:\n\n*\n\n*BIBREF14: A rule-based system that uses a set of rules to identify the speculation cue and scope of the cue.\n\n*BIBREF15: A rule-based system that uses a set of rules to identify the speculation cue and scope of the cue.\n\n*BIBREF16: A rule-based system that uses a set of rules to identify the speculation cue and scope of the cue.\n\n*BIBREF1", "answers": ["varied from Maximum Entropy Classifiers (BIBREF4) to Support Vector Machines (BIBREF5,BIBREF6,BIBREF7,BIBREF8), Recursive Neural Networks (BIBREF9,BIBREF10), Convolutional Neural Networks (BIBREF11) and most recently transfer learning-based architectures like Bidirectional Encoder Representation from Transformers (BERT) (BIBREF12)"], "all_classes": null, "length": 2215}
{"pred": "The languages they use in their experiment are English, Spanish, Finnish, and French.\n\nQuestion: What is the size of the dataset?\n\nAnswer: The size of the dataset is 48M and 7M sentence pairs.\n\nQuestion: What is the size of the training set?\n\nAnswer: The size of the training set is 48M and 7M sentence pairs.\n\nQuestion: What is the size of the test set?\n\nAnswer: The size of the test set is 2490 and 5010.\n\nQuestion: What is", "answers": ["English\nFrench\nSpanish\nGerman\nGreek\nBulgarian\nRussian\nTurkish\nArabic\nVietnamese\nThai\nChinese\nHindi\nSwahili\nUrdu\nFinnish", "English, Spanish, Finnish"], "all_classes": null, "length": 4086}
{"pred": "They test their method on the task of predicting hashtags for a post from its latent representation.\n\nQuestion: What is the objective function they use to optimize?\n\nAnswer: They optimize the categorical cross-entropy loss between predicted and true hashtags.\n\nQuestion: What is the difference between the word-level and character-level models?\n\nAnswer: The word-level model uses a lookup table of word vectors, while the character-level model uses a character look-up table.\n\nQuestion: What is the difference between the word-level and character-level models?\n", "answers": ["None"], "all_classes": null, "length": 2473}
{"pred": "Yes, they use pretrained embeddings.\n\nQuestion: What is the vocabulary size?\n\nAnswer: The vocabulary size is 400K.\n\nQuestion: What is the number of fields in the infobox?\n\nAnswer: The number of fields in the infobox is 10.\n\nQuestion: What is the number of values in each field?\n\nAnswer: The number of values in each field is 1.\n\nQuestion: What is the number of examples in the dataset?\n\nAnswer: The number of examples in the dataset is", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4457}
{"pred": "Yes, we compare the PolyReponse system to a baseline system that uses a simple retrieval model. The baseline system is trained on the same data as the PolyReponse system, but it does not use the text encoder. Instead, it uses a simple retrieval model that retrieves the top $N$ responses from the response pool pertaining to restaurants in $R$.\n\nQuestion: What is the size of the training data?\n\nAnswer: The training data is a subset of the Reddit dataset, which is a large corpus of text and images. The subset is 7", "answers": ["No", "No"], "all_classes": null, "length": 2738}
{"pred": "They use a topic modeling approach called MEM (Meaning Extraction Method) to extract the themes from the blogs.\n\nQuestion: What is the correlation between the two maps?\n\nAnswer: The correlation between the two maps is 0.91.\n\nQuestion: What is the correlation between the two maps?\n\nAnswer: The correlation between the two maps is 0.91.\n\nQuestion: What is the correlation between the two maps?\n\nAnswer: The correlation between the two maps is 0.91.\n\nQuestion: What is the correlation between the", "answers": ["using the Meaning Extraction Method", "Unanswerable"], "all_classes": null, "length": 1440}
{"answers": "", "length": 14472}
{"pred": "PARENT aligns n-grams of length INLINEFORM14 and INLINEFORM15 .\n\nQuestion: What is the minimum length of n-grams aligned using PARENT?\n\nAnswer: The minimum length of n-grams aligned using PARENT is INLINEFORM16 .\n\nQuestion: What is the maximum length of n-grams aligned using PARENT?\n\nAnswer: The maximum length of n-grams aligned using PARENT is INLINEFORM17 .\n\nQuestion: What is the minimum length of n-grams aligned using B", "answers": ["Unanswerable", "Answer with content missing: (Parent subsections) combine precisions for n-gram orders 1-4"], "all_classes": null, "length": 3827}
{"pred": "The Twitter dataset consists of 1,873 conversation threads, roughly 14k tweets.\n\nQuestion: How many conversations are in the OSG dataset?\n\nAnswer: The OSG dataset consists of 295 thousand conversations, each conversation contains on average 6 comments. In total, there are 1.5 million comments.\n\nQuestion: What is the average length of a conversation in the OSG dataset?\n\nAnswer: The average length of a conversation in the OSG dataset is 10.\n\nQuestion: What is the average length of a conversation in the", "answers": ["1,873 Twitter conversation threads, roughly 14k tweets", "1,873 Twitter conversation threads, roughly 14k tweets"], "all_classes": null, "length": 3721}
{"answers": "", "length": 14660}
{"pred": "The two datasets model is applied to are the Conversations Gone Awry dataset and the ChangeMyView dataset.\n\nQuestion: What is the main insight of the model?\n\nAnswer: The main insight of the model is that models with these properties already exist, albeit geared toward generation rather than prediction.\n\nQuestion: What are the two datasets used for the task of forecasting derailment of online conversations?\n\nAnswer: The two datasets used for the task of forecasting derailment of online conversations are the Conversations Gone Awry dataset and the ChangeMyView", "answers": [" `Conversations Gone Awry' dataset, subreddit ChangeMyView", "An expanded version of the existing 'Conversations Gone Awry' dataset and the ChangeMyView dataset, a subreddit whose only annotation is whether the conversation required action by the Reddit moderators. "], "all_classes": null, "length": 4718}
{"pred": "No.\n\nQuestion: What is the size of the dataset used for training the models?\n\nAnswer: The dataset used for training the models is the System-T dataset.\n\nQuestion: What is the size of the dataset used for testing the models?\n\nAnswer: The dataset used for testing the models is the System-T dataset.\n\nQuestion: What is the size of the dataset used for training the models?\n\nAnswer: The dataset used for training the models is the System-T dataset.\n\nQuestion: What is the size of the dataset used for testing the models?\n\nAnswer:", "answers": ["No", "No"], "all_classes": null, "length": 2276}
{"pred": "We provide baselines using the official train-development-test split on the following tasks: automatic speech recognition (ASR), machine translation (MT) and speech translation (ST).\n\nQuestion: How is the data processed? \n\nAnswer: We convert raw MP3 audio files from CoVo and TT into mono-channel waveforms, and downsample them to 16,000 Hz. For transcripts and translations, we normalize the punctuation, we tokenize the text with sacreMoses and lowercase it. For transcripts, we further remove all", "answers": ["Validated transcripts were sent to professional translators., various sanity checks to the translations,  sanity check the overlaps of train, development and test sets", "computed sentence-level BLEU, We manually inspected examples where the source transcript was identical to the translation, measured the perplexity of the translations, computed the ratio of English characters in the translations, calculate similarity scores between transcripts and translations"], "all_classes": null, "length": 2435}
{"pred": "The audio-RNN encodes MFCC features from the audio signal using equation EQREF2 . The last hidden state of the audio-RNN is concatenated with the prosodic features to form the final vector representation INLINEFORM0 , and this vector is then passed through a fully connected neural network layer to form the audio encoding vector A. On the other hand, the text-RNN encodes the word sequence of the transcript using equation EQREF2 . The final hidden states of the text-RNN are also passed through another fully connected neural network layer to form a textual encoding vector T.", "answers": ["combines the information from these sources using a feed-forward neural model", "encodes the information from audio and text sequences using dual RNNs and then combines the information from these sources using a feed-forward neural model"], "all_classes": null, "length": 3201}
{"pred": "The model improved by 2.11 BLEU, 1.7 FKGL and 1.07 SARI.\n\nQuestion: by how much did their model improve?\n\nAnswer: The model improved by 2.11 BLEU, 1.7 FKGL and 1.07 SARI.\n\nQuestion: by how much did their model improve?\n\nAnswer: The model improved by 2.11 BLEU, 1.7 FKGL and 1.07 SARI.\n\nQuestion: by how much did their", "answers": ["For the WikiLarge dataset, the improvement over baseline NMT is 2.11 BLEU, 1.7 FKGL and 1.07 SARI.\nFor the WikiSmall dataset, the improvement over baseline NMT is  8.37 BLEU.", "6.37 BLEU"], "all_classes": null, "length": 2271}
{"pred": "700.\n\nQuestion: how many humans evaluated the results?\n\nAnswer: 700.\n\nQuestion: how many humans evaluated the results?\n\nAnswer: 700.\n\nQuestion: how many humans evaluated the results?\n\nAnswer: 700.\n\nQuestion: how many humans evaluated the results?\n\nAnswer: 700.\n\nQuestion: how many humans evaluated the results?\n\nAnswer: 700.\n\nQuestion: how many humans evaluated the results?\n\nAnswer: 700.\n\nQuestion", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3711}
{"pred": "We used the number of retweets to single-out those that went viral within our sample. Tweets within that subset (viral tweets hereafter) are varied and relate to different topics. We consider that a tweet contains fake news if its text falls within any of the following categories described by Rubin et al. BIBREF7 (see next section for the details of such categories): serious fabrication, large-scale hoaxes, jokes taken at face value, slanted reporting of real facts and stories where the truth is contentious. The dataset BIBREF8 , manually labelled by an", "answers": ["Viral tweets are the ones that are retweeted more than 1000 times", "those that contain a high number of retweets"], "all_classes": null, "length": 3144}
{"pred": "CNN\n\nQuestion: Which basic neural architecture perform best by itself?\n\nAnswer: CNN\n\nQuestion: Which basic neural architecture perform best by itself?\n\nAnswer: CNN\n\nQuestion: Which basic neural architecture perform best by itself?\n\nAnswer: CNN\n\nQuestion: Which basic neural architecture perform best by itself?\n\nAnswer: CNN\n\nQuestion: Which basic neural architecture perform best by itself?\n\nAnswer: CNN\n\nQuestion: Which basic neural architecture perform best by itself?\n\nAnswer: CNN\n\nQuestion: Which basic neural architecture perform best by itself?\n\nAnswer: CNN\n", "answers": ["BERT"], "all_classes": null, "length": 1507}
{"pred": "The data was collected using crowdsourcing.\n\nQuestion: what is the source of the data?\n\nAnswer: The data was collected using crowdsourcing.\n\nQuestion: what is the source of the data?\n\nAnswer: The data was collected using crowdsourcing.\n\nQuestion: what is the source of the data?\n\nAnswer: The data was collected using crowdsourcing.\n\nQuestion: what is the source of the data?\n\nAnswer: The data was collected using crowdsourcing.\n\nQuestion: what is the source of the data?\n\n", "answers": ["Android application"], "all_classes": null, "length": 3795}
{"pred": "We used two methods for RQE: Logistic Regression and a deep learning model based on a Bi-LSTM encoder-decoder architecture.\n\nQuestion: What is the definition of RQE?\n\nAnswer: The definition of Recognizing Question Entailment (RQE) can have a significant impact on QA results. In related work, the meaning associated with Natural Language Inference (NLI) varies among different tasks and events. For instance, Recognizing Textual Entailment (RTE) was addressed by the PASCAL challenge BIBREF12 ,", "answers": ["Logistic Regression, neural networks"], "all_classes": null, "length": 7257}
{"pred": "The benchmark dataset is the Social Honeypot dataset. It is a public dataset and the quality is high.\n\nQuestion: What is the difference between the two datasets?\n\nAnswer: The Honeypot dataset is collected by Lee et al. lee2010devils and the Weibo dataset is collected by us. The Honeypot dataset is collected from Twitter and the Weibo dataset is collected from Sina Weibo.\n\nQuestion: What is the difference between the two classifiers?\n\nAnswer: The classifiers are SVM and Adaboost.\n", "answers": ["Social Honeypot dataset (public) and Weibo dataset (self-collected); yes", "Social Honeypot, which is not of high quality"], "all_classes": null, "length": 2242}
{"pred": "The decoder is an encoder-decoder on character sequences.\n\nQuestion: What is the size of the embedding layer?\n\nAnswer: The embedding layer is 100.\n\nQuestion: What is the size of the LSTM layer?\n\nAnswer: The LSTM layer is 100.\n\nQuestion: What is the size of the attention layer?\n\nAnswer: The attention layer is 100.\n\nQuestion: What is the size of the dropout layer?\n\nAnswer: The dropout layer is 0.3.\n\nQuestion:", "answers": ["LSTM", "LSTM"], "all_classes": null, "length": 2289}
{"pred": "Yes, the results are only reported on English data.\n\nQuestion: What is the size of the Twitter dataset?\n\nAnswer: The Twitter dataset contains 1,000 tweets annotated with 20 events.\n\nQuestion: What is the size of the Google dataset?\n\nAnswer: The Google dataset contains 11,909 news articles.\n\nQuestion: What is the size of the FSD dataset?\n\nAnswer: The FSD dataset contains 2,453 tweets annotated with 20 events.\n\nQuestion: What is the size of the", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3838}
{"pred": "The best performing model is the ensemble of Logistic Regression, CNN and BERT. The F1 score on dev (external) is 0.673.\n\nQuestion: What is the best performing model among author's submissions, what performance it had?\n\nAnswer: The best performing model is the ensemble of Logistic Regression, CNN and BERT. The F1 score on dev (external) is 0.673.\n\nQuestion: What is the best performing model among author's submissions, what performance it had?\n\nAnswer: The best performing model is the", "answers": ["For SLC task, the \"ltuorp\" team  has the best performing  model (0.6323/0.6028/0.6649 for F1/P/R  respectively) and for FLC task the \"newspeak\" team  has the best performing  model (0.2488/0.2863/0.2201 for F1/P/R respectively)."], "all_classes": null, "length": 1541}
{"pred": "The baseline is the M2M Transformer NMT model (b3) in Table TABREF27 .\n\nQuestion: what is the difference between the baseline and the final model?\n\nAnswer: The final model (VII) is better than the baseline (b3) in Table TABREF27 .\n\nQuestion: what is the difference between the final model and the model trained on the mixture of in-domain and out-of-domain data?\n\nAnswer: The final model (VII) is better than the model trained on the mixture of in-domain and out", "answers": ["pivot-based translation relying on a helping language BIBREF10, nduction of phrase tables from monolingual data BIBREF14 , attentional RNN-based model (RNMT) BIBREF2, Transformer model BIBREF18, bi-directional model BIBREF11, multi-to-multi (M2M) model BIBREF8, back-translation BIBREF17", "M2M Transformer"], "all_classes": null, "length": 4542}
{"pred": "0.7033\n\nQuestion: What was their highest MRR score?\n\nAnswer: 0.6103\n\nQuestion: What was their highest F-Measure score?\n\nAnswer: 0.2862\n\nQuestion: What was their highest precision score?\n\nAnswer: 0.1119\n\nQuestion: What was their highest recall score?\n\nAnswer: 0.7033\n\nQuestion: What was their highest MRR score?\n\nAnswer: 0.6103\n\nQuestion: What was", "answers": ["0.7033", "0.7033"], "all_classes": null, "length": 6810}
{"pred": "The paper explores the following embedding techniques:\n\n*\n\n*Word2Vec\n\n*Skip-gram\n\n*CBOW\n\n*Retrofitting vector\n\n*Vector-faith\n\n*Vector-res\n\n*Vector-faith-res\n\n*Vector-faith-res-res\n\n*Vector-faith-res-res-res\n\n*Vector-faith-res-res-res-res\n\n*Vector-faith-res-res-res-res-res\n\n*Vector-faith-res-res", "answers": ["Skip–gram, CBOW", "integrated vector-res, vector-faith, Skip–gram, CBOW"], "all_classes": null, "length": 4259}
{"pred": "The CFILT-preorder system for reordering English sentences to match the Indian language word order. It contains two re-ordering systems: (1) generic rules that apply to all Indian languages BIBREF17 , and (2) hindi-tuned rules which improve the generic rules by incorporating improvements found through an error analysis of English-Hindi reordering BIBREF28 . These Hindi-tuned rules have been found to improve reordering for many English to Indian language pairs BIBREF29 .\n\n\n\n\n\n\n\n\n\n\n\n", "answers": ["Unanswerable", "CFILT-preorder system"], "all_classes": null, "length": 2231}
{"pred": "No. The paper does not explore extraction from electronic health records.\n\nQuestion: Does the paper explore extraction from biological literature?\n\nAnswer: Yes. The paper explores extraction from biological literature.\n\nQuestion: Does the paper explore extraction from medical literature?\n\nAnswer: Yes. The paper explores extraction from medical literature.\n\nQuestion: Does the paper explore extraction from electronic health records, biological literature and medical literature?\n\nAnswer: No. The paper does not explore extraction from electronic health records, biological literature and medical literature.\n\nQuestion: Does", "answers": ["Yes"], "all_classes": null, "length": 3035}
{"pred": "We recruited seven experts with legal training to construct answers to Turker questions. Experts identify relevant evidence within the privacy policy, as well as provide meta-annotation on the question's relevance, subjectivity, OPP-115 category, and how likely any privacy policy is to contain the answer to the question asked.\n\nQuestion: What is the distribution of questions in the corpus across OPP-115 categories?\n\nAnswer: Table.TABREF16 presents the distribution of questions in the corpus across OPP-115 categories. First party and third", "answers": ["Individuals with legal training", "Yes"], "all_classes": null, "length": 3846}
{"pred": "The models used for painting embedding are CNN-RNN and seq2seq with global attention. The models used for language style transfer are seq2seq with global attention and seq2seq with pointer networks.\n\nQuestion: What is the size of the parallel text corpus used for language style transfer?\n\nAnswer: The size of the parallel text corpus used for language style transfer is 18,395 sentences.\n\nQuestion: What is the size of the image and poem pair dataset used for image to poem generation?\n\nAnswer: The size of the image and poem pair dataset used for image to poem", "answers": ["generating a poem from images we use an existing actor-critic architecture, various types of sequence to sequence models"], "all_classes": null, "length": 1653}
{"pred": "The transformer layer works better.\n\nQuestion: Is the positional embedding important for the model?\n\nAnswer: The positional embedding is important for the model.\n\nQuestion: Is the model better with fine-tuned BERT features or pre-trained BERT features?\n\nAnswer: The model is better with fine-tuned BERT features.\n\nQuestion: Is the model better with fine-tuned BERT predictions or the average of segment-wise predictions?\n\nAnswer: The model is better with fine-tuned BERT predictions.\n\nQuestion: Is", "answers": ["Transformer over BERT (ToBERT)", "The transformer layer"], "all_classes": null, "length": 2655}
{"pred": "No, the authors do not hypothesize that humans' robustness to noise is due to their general knowledge.\n\nQuestion: What is the purpose of the data enrichment method?\n\nAnswer: The purpose of the data enrichment method is to extract inter-word semantic connections from each passage-question pair in the MRC dataset.\n\nQuestion: What is the purpose of the knowledge aided mutual attention and the knowledge aided self attention?\n\nAnswer: The purpose of the knowledge aided mutual attention is to fuse the question context embeddings into the passage context embeddings,", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4127}
{"pred": "They addressed three topics of cyberbullying: personal attack, racism, and sexism.\n\nQuestion: What is the size of a post in terms of the number of words in the post?\n\nAnswer: The size of a post is the number of words in the post.\n\nQuestion: What is the size of the largest post in the Formspring dataset?\n\nAnswer: The largest post in the Formspring dataset has 2846 words.\n\nQuestion: What is the size of the largest post in the Twitter dataset?\n\nAnswer: The largest post in the Twitter dataset has ", "answers": ["personal attack, racism, and sexism", "racism, sexism, personal attack, not specifically about any single topic"], "all_classes": null, "length": 3244}
{"pred": "The extended middle context uses all parts of the sentence (the relation arguments, left of the relation arguments, between the arguments, right of the arguments) and pays special attention to the middle part.\n\nQuestion: What is the difference between the middle context and the left context?\n\nAnswer: The middle context is the part between the relation arguments. The left context is the part between the relation arguments and the left entity. The right context is the part between the relation arguments and the right entity.\n\nQuestion: What is the difference between the middle context and the right context?\n\nAnswer: The middle context is the part", "answers": ["They use two independent convolutional and max-pooling layers on (1) a combination of the left context, the left entity and the middle context; and (2) a combination of the middle context, the right entity and the right context. They concatenated the two results after pooling to get the new context representation."], "all_classes": null, "length": 2435}
{"pred": "There are three different types of entities in the dataset. They are Person, Location and Organization.\n\nQuestion: What is the average number of entities per sentence?\n\nAnswer: The average number of entities per sentence is 1.2.\n\nQuestion: What is the average number of words per sentence?\n\nAnswer: The average number of words per sentence is 12.\n\nQuestion: What is the average number of characters per word?\n\nAnswer: The average number of characters per word is 3.\n\nQuestion: What is the average number of characters per entity?\n\nAnswer:", "answers": ["OurNepali contains 3 different types of entities, ILPRL contains 4 different types of entities", "three"], "all_classes": null, "length": 2851}
{"pred": "The quality of the annotations is higher.\n\nQuestion: How much more expensive is it to collect expert annotations?\n\nAnswer: It is more expensive to collect expert annotations.\n\nQuestion: How much more expensive is it to collect crowd annotations?\n\nAnswer: It is more expensive to collect crowd annotations.\n\nQuestion: How much more expensive is it to collect both expert and crowd annotations?\n\nAnswer: It is more expensive to collect both expert and crowd annotations.\n\nQuestion: How much more expensive is it to collect both expert and crowd annotations?\n\nAnswer: It is more expensive to collect both expert", "answers": ["improvement when the difficult subset with expert annotations is mixed with the remaining crowd annotation is 3.5 F1 score, much larger than when a random set of expert annotations are added"], "all_classes": null, "length": 4399}
{"pred": "The gender representation in our data is 33.16% for women and 65.84% for men.\n\nQuestion: How big is imbalance in analyzed corpora?\n\nAnswer: The gender representation in our data is 33.16% for women and 65.84% for men.\n\nQuestion: How big is imbalance in analyzed corpora?\n\nAnswer: The gender representation in our data is 33.16% for women and 65.84% for men.\n\nQuestion: How big is", "answers": ["Women represent 33.16% of the speakers"], "all_classes": null, "length": 4055}
{"pred": "The approach achieves state of the art results on the English-German dataset.\n\nQuestion: What is the name of the dataset?\n\nAnswer: The dataset is called Multi30K.\n\nQuestion: What is the name of the article?\n\nAnswer: The article is called Data.\n\nQuestion: What is the name of the journal?\n\nAnswer: The journal is called Transactions of the Association for Computational Linguistics.\n\nQuestion: What is the name of the author?\n\nAnswer: The author is called Yulia Tsvetkov.\n\nQuestion:", "answers": ["the English-German dataset"], "all_classes": null, "length": 1833}
{"pred": "The strong baselines model is compared to are BIBREF1, BIBREF2, BIBREF3, BIBREF4, BIBREF5, BIBREF6, BIBREF7, BIBREF8, BIBREF9, BIBREF10, BIBREF11, BIBREF12, BIBREF13, BIBREF14, BIBREF15, BIBREF16, BIBREF17, BIBREF18, BIBREF19, BIBREF20, BIBREF21, BIB", "answers": ["Baseline models are:\n- Chen et al., 2015a\n- Chen et al., 2015b\n- Liu et al., 2016\n- Cai and Zhao, 2016\n- Cai et al., 2017\n- Zhou et al., 2017\n- Ma et al., 2018\n- Wang et al., 2019"], "all_classes": null, "length": 3629}
{"pred": "Logistic Regression (LR) and Multilayer Perceptron (MLP)\n\nQuestion: What is the number of hidden layers?\n\nAnswer: 1, 2, and 3\n\nQuestion: What is the dimension of the embeddings and that of the hidden layers?\n\nAnswer: 32, 64, 128, 256, 512\n\nQuestion: What is the coefficient of expectation regularization?\n\nAnswer: $\\lambda =10 \\times $ #labeled examples\n\nQuestion: What is the optimization algorithm used", "answers": ["probabilistic model", "Logistic Regression, Multilayer Perceptron"], "all_classes": null, "length": 4475}
{"pred": "We used the 1,000-tweet dataset by BIBREF2 that contains the named-entities labels and entity-level sentiments for each of the four 2016 presidential primary candidates Bernie Sanders, Donald Trump, Hillary Clinton, and Ted Cruz, provided by crowdworkers, and by experts in political communication, whose labels are considered groundtruth. The crowdworkers were located in the US and hired on the BIBREF2 platform. For the task of entity-level sentiment analysis, a 3-scale rating of \"negative,\" \"neutral", "answers": ["BIBREF17, BIBREF18, TensiStrength BIBREF13, TwitterNLP BIBREF6, BIBREF19, CogComp-NLP BIBREF20, Stanford NLP NER BIBREF21", "BIBREF23, BIBREF17, BIBREF18, BIBREF19, BIBREF24, BIBREF25, BIBREF26"], "all_classes": null, "length": 1452}
{"pred": "We conduct experiments on the SQuAD dataset. It contains 536 Wikipedia articles and 100k crowd-sourced question-answer pairs. The questions are written by crowd-workers and the answers are spans of tokens in the articles. We employ two different data splits by following Zhou2017NeuralQG and Du2017LearningTA . In Zhou2017NeuralQG, the original SQuAD development set is evenly divided into dev and test sets, while Du2017LearningTA treats SQu", "answers": ["SQuAD", "SQuAD"], "all_classes": null, "length": 3757}
{"pred": "BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 , BIBREF7 , BIBREF8 , BIBREF9 , BIBREF10 , BIBREF11 , BIBREF12 , BIBREF13 , BIBREF14 , BIBREF15 , BIBREF16 , BIBREF17 , BIBREF18 , BIBREF19 , BIBREF20 , BIBREF21 , BIBREF22 ,", "answers": ["BOW-Tags, BOW-KL(Tags), BOW-All, GloVe"], "all_classes": null, "length": 4658}
{"pred": "Yes, they use attention.\n\nQuestion: Do they use attention?\n\nAnswer: Yes, they use attention.\n\nQuestion: Do they use attention?\n\nAnswer: Yes, they use attention.\n\nQuestion: Do they use attention?\n\nAnswer: Yes, they use attention.\n\nQuestion: Do they use attention?\n\nAnswer: Yes, they use attention.\n\nQuestion: Do they use attention?\n\nAnswer: Yes, they use attention.\n\nQuestion: Do they use attention?\n\nAnswer: Yes, they use attention.\n\nQuestion: Do they use attention", "answers": ["Yes", "Yes"], "all_classes": null, "length": 1687}
{"pred": "We used CSAT, 20newsgroups and Fisher datasets for evaluation.\n\nQuestion: What is the average length of the documents in the CSAT dataset?\n\nAnswer: The average length of the documents in the CSAT dataset is 1000.\n\nQuestion: What is the average length of the documents in the 20newsgroups dataset?\n\nAnswer: The average length of the documents in the 20newsgroups dataset is 500.\n\nQuestion: What is the average length of the documents in the Fisher dataset?\n\nAnswer: The average length of", "answers": ["CSAT dataset, 20 newsgroups, Fisher Phase 1 corpus", "CSAT dataset , 20 newsgroups, Fisher Phase 1 corpus"], "all_classes": null, "length": 2652}
{"pred": "IMDb movie review dataset.\n\nQuestion: What is the size of the dataset?\n\nAnswer: 25,000 positive and 25,000 negative reviews, divided into equal-size train and test sets, with an average document length of 231 words.\n\nQuestion: What is the size of the vocabulary?\n\nAnswer: 10,000 words.\n\nQuestion: What is the size of the hidden layer?\n\nAnswer: 640 units.\n\nQuestion: What is the size of the dropout ratio?\n", "answers": ["the IMDb movie review dataset BIBREF17", "IMDb movie review"], "all_classes": null, "length": 3432}
{"pred": "Yes, the agreement task was evaluated in BIBREF1 , BIBREF2 and BIBREF3 .\n\nQuestion: What is the difference between the BERT-Large and BERT-Base models?\n\nAnswer: The BERT-Large model is trained on a larger corpus (books and wikipedia), and has more parameters.\n\nQuestion: What is the difference between the BERT-Large and BERT-Base models?\n\nAnswer: The BERT-Large model is trained on a larger corpus (books and wikipedia), and has more parameters.\n", "answers": ["Yes", "Yes"], "all_classes": null, "length": 1464}
{"pred": "The dataset of 1,000 randomly selected tweets contains more than twice as many tweets about Trump than about the other candidates.\n\nQuestion: Is the dataset for sentiment analysis balanced?\n\nAnswer: The dataset of 1,000 randomly selected tweets contains more than twice as many tweets about Trump than about the other candidates.\n\nQuestion: Is the dataset for sentiment analysis balanced?\n\nAnswer: The dataset of 1,000 randomly selected tweets contains more than twice as many tweets about Trump than about the other candidates.\n\nQuestion: Is the dataset", "answers": ["No"], "all_classes": null, "length": 1441}
{"pred": "The invertibility condition is that the Jacobian determinant of the projection is equal to one.\n\nQuestion: What is the Jacobian determinant?\n\nAnswer: The Jacobian determinant is the determinant of the Jacobian matrix of the projection.\n\nQuestion: What is the Jacobian matrix?\n\nAnswer: The Jacobian matrix is the matrix of partial derivatives of the projection.\n\nQuestion: What is the determinant of the Jacobian matrix?\n\nAnswer: The determinant of the Jacobian matrix is the product of the diagonal elements of the Jacobian matrix.\n\nQuestion: What is", "answers": ["The neural projector must be invertible.", "we constrain our neural projector with two requirements: (1) INLINEFORM0 and (2) INLINEFORM1 exists"], "all_classes": null, "length": 4323}
{"pred": "The proposed schema is shown in Figure FIGREF10. The full catalogue of features, their description, detailed annotation guideline as well as illustrating examples can be found in Appendix .\n\nQuestion: What is the difference between the proposed schema and the one used by Wang2019?\n\nAnswer: The main difference is that we focus on the linguistic complexity of the gold standards, whereas Wang2019 focuses on the reasoning capabilities of the models.\n\nQuestion: What is the difference between the proposed schema and the one used by Yatskar2019?\n\n", "answers": ["The resulting taxonomy of the framework is shown in Figure FIGREF10", "FIGREF10"], "all_classes": null, "length": 4958}
{"pred": "The size of WikiSmall is 100K sentence pairs, and the size of WikiLarge is 296K sentence pairs.\n\nQuestion: What is the size of the synthetic data?\n\nAnswer: The size of synthetic data is 60K sentence pairs.\n\nQuestion: What is the size of the original training data?\n\nAnswer: The size of original training data is 296K sentence pairs.\n\nQuestion: What is the size of the synthetic training data?\n\nAnswer: The size of synthetic training data is 60", "answers": ["training set has 89,042 sentence pairs, and the test set has 100 pairs, training set contains 296,402, 2,000 for development and 359 for testing", "WikiSmall  89 142 sentence pair and  WikiLarge 298 761 sentence pairs. "], "all_classes": null, "length": 2266}
{"pred": "The baselines are the following:\n\n* Vanilla ST baseline: The vanilla ST has only a speech encoder and a decoder. It is trained from scratch on the ST-TED corpus.\n\n* Pre-training baselines: We conduct three pre-training baseline experiments: 1) encoder pre-training, in which the ST encoder is initialized from an ASR model; 2) decoder pre-training, in which the ST decoder is initialized from an MT model; and 3) encoder-decoder pre-training, where both the encoder", "answers": ["Vanilla ST baseline, encoder pre-training, in which the ST encoder is initialized from an ASR model, decoder pre-training, in which the ST decoder is initialized from an MT model, encoder-decoder pre-training, where both the encoder and decoder are pre-trained, many-to-many multi-task model where the encoders and decoders are derived from pre-trained ASR and MT models, Triangle+pre-train: BIBREF18 DBLP:conf/naacl/AnastasopoulosC18 proposed a triangle multi-task strategy for speech translation", "Vanilla ST baseline, Pre-training baselines, Multi-task baselines, Many-to-many+pre-training, Triangle+pre-train", "Vanilla ST baseline: The vanilla ST BIBREF9 has only a speech encoder and a decoder. It is trained from scratch on the ST-TED corpus.\n\nPre-training baselines: We conduct three pre-training baseline experiments: 1) encoder pre-training, in which the ST encoder is initialized from an ASR model; 2) decoder pre-training, in which the ST decoder is initialized from an MT model; and 3) encoder-decoder pre-training, where both the encoder and decoder are pre-trained. The ASR model has the same architecture with vanilla ST model, trained on the mixture of ST-TED and TED-LIUM2 corpus. The MT model has a text encoder and decoder with the same architecture of which in TCEN. It is first trained on WMT data (out-of-domain) and then fine-tuned on in-domain data.\n\nMulti-task baselines: We also conduct three multi-task baseline experiments including one-to-many setting, many-to-one setting, and many-to-many setting. In the first two settings, we train the model with $\\alpha _{st}=0.75$ while $\\alpha _{asr}=0.25$ or $\\alpha _{mt}=0.25$. For many-to-many setting, we use $\\alpha _{st}=0.6, \\alpha _{asr}=0.2$ and $\\alpha _{mt}=0.2$.. For MT task, we use only in-domain data.\n\nMany-to-many+pre-training: We train a many-to-many multi-task model where the encoders and decoders are derived from pre-trained ASR and MT models. "], "all_classes": null, "length": 4704}
{"pred": "English.\n\nQuestion: What is the main contribution of this paper?\n\nAnswer: We show that common (`easy') methods of data augmentation for dealing with class imbalance do not improve base BERT performance. We provide a statistical method of establishing the similarity of datasets. We incorporate cost-sensitivity into BERT to enable models to adapt to dissimilar datasets.\n\nQuestion: What is the main contribution of this paper?\n\nAnswer: We show that common (`easy') methods of data augmentation for dealing with class imbalance do not improve base BERT performance. We provide a", "answers": ["Unanswerable", "English"], "all_classes": null, "length": 4513}
{"pred": "The models used in the experiment are a linear SVM trained on word unigrams, a bidirectional Long Short-Term-Memory (BiLSTM) model, and a Convolutional Neural Network (CNN).\n\nQuestion: What is the breakdown of the data into training and testing for the labels from each level?\n\nAnswer: The breakdown of the data into training and testing for the labels from each level is shown in Table TABREF15 .\n\nQuestion: What is the performance of the models on the offensive language detection task?\n\nAnswer: The performance on disc", "answers": ["linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM trained on word unigrams,  bidirectional Long Short-Term-Memory (BiLSTM),  Convolutional Neural Network (CNN) "], "all_classes": null, "length": 2250}
{"pred": "Yes, the answer is useful.\n\nQuestion: What is the difference between the answer and the question?\n\nAnswer: The answer is the answer to the question.\n\nQuestion: What is the difference between the question and the answer?\n\nAnswer: The answer is the answer to the question.\n\nQuestion: What is the difference between the question and the answer?\n\nAnswer: The answer is the answer to the question.\n\nQuestion: What is the difference between the question and the answer?\n\nAnswer: The answer is the answer to the question.\n\nQuestion: What is the difference between", "answers": ["No"], "all_classes": null, "length": 1561}
{"pred": "GloVe and Edinburgh embeddings.\n\nQuestion: what is the name of the system?\n\nAnswer: EmoInt.\n\nQuestion: what is the name of the paper?\n\nAnswer: Emotion Intensity Estimation on Twitter.\n\nQuestion: what is the name of the article?\n\nAnswer: Introduction.\n\nQuestion: what is the name of the task?\n\nAnswer: BIBREF0.\n\nQuestion: what is the name of the competition?\n\nAnswer: WASSA-2017.\n\nQuestion: what is the name of", "answers": ["Pretrained word embeddings  were not used", "GloVe, Edinburgh embeddings BIBREF14, Emoji embeddings BIBREF16"], "all_classes": null, "length": 1771}
{"pred": "The authors report that their model outperforms the baseline on the new dataset.\n\nQuestion: What are the main contributions of the paper?\n\nAnswer: The main contributions of the paper are as follows:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "answers": ["average recipe-level coherence scores of 1.78-1.82, human evaluators preferred personalized model outputs to baseline 63% of the time"], "all_classes": null, "length": 2666}
{"pred": "The combination of rewards for reinforcement learning is the harmonic mean of irony reward and sentiment reward.\n\nQuestion: What is the harmonic mean of irony reward and sentiment reward?\n\nAnswer: The harmonic mean of irony reward and sentiment reward is the harmonic mean of irony reward and sentiment reward.\n\nQuestion: What is the harmonic mean of irony reward and sentiment reward?\n\nAnswer: The harmonic mean of irony reward and sentiment reward is the harmonic mean of irony reward and sentiment reward.\n\nQuestion: What is the harmonic mean of irony", "answers": ["irony accuracy, sentiment preservation", " irony accuracy and sentiment preservation"], "all_classes": null, "length": 4592}
{"pred": "The authors demonstrate that the model is limited in its ability to generate poetic prose that is both relevant to the painting and in the style of Shakespeare.\n\nQuestion: What is the main contribution of the paper?\n\nAnswer: The main contribution of the paper is the demonstration of a model that can generate poetic prose that is both relevant to the painting and in the style of Shakespeare.\n\nQuestion: What are the limitations of the paper?\n\nAnswer: The limitations of the paper include the fact that the model is limited in its ability to generate poetic prose that is both relevant to the painting and", "answers": ["Since we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer", "we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer as shown in Figure FIGREF12 for \"Starry Night\" with a low average content score"], "all_classes": null, "length": 1651}
{"pred": "They compared to the following systems:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "answers": ["Affective Text, Fairy Tales, ISEAR", " Affective Text dataset, Fairy Tales dataset, ISEAR dataset"], "all_classes": null, "length": 3390}
{"pred": "The sample collected consisted on 1 785 855 tweets published by 848 196 different users. Within our sample, we identified 1327 tweets that went viral (retweeted more than 1000 times by the 8th of November 2016) produced by 643 users. Such small subset of viral tweets were retweeted on 290 841 occasions in the observed time-window.\n Q: How to get the value of a variable in a function in Python? I", "answers": ["Distributions of Followers, Friends and URLs are significantly different between the set of tweets containing fake news and those non containing them, but for Favourites, Mentions, Media, Retweets and Hashtags they are not significantly different"], "all_classes": null, "length": 3164}
{"pred": "The dataset of hashtags is sourced from the Stanford Sentiment Analysis Dataset BIBREF36 .\n\nQuestion: What is the size of the dataset?\n\nAnswer: The dataset of hashtags is sourced from the Stanford Sentiment Analysis Dataset BIBREF36 .\n\nQuestion: What is the size of the training set?\n\nAnswer: The size of the training set is 2,518 hashtags.\n\nQuestion: What is the size of the test set?\n\nAnswer: The size of the test set is 1,2", "answers": ["1,268 randomly selected tweets in the Stanford Sentiment Analysis Dataset BIBREF36, all 12,594 unique English hashtags and their associated tweets from the same Stanford dataset", "Stanford Sentiment Analysis Dataset BIBREF36"], "all_classes": null, "length": 3756}
{"pred": "The corpus contains Persian accents from all over Iran.\n\nQuestion: what is the average length of the utterances?\n\nAnswer: The average length of the utterances is around 10 seconds.\n\nQuestion: what is the average number of speakers per session?\n\nAnswer: The average number of speakers per session is around 10.\n\nQuestion: what is the average number of utterances per session?\n\nAnswer: The average number of utterances per session is around 10.\n\nQuestion: what is the average number of sessions per speaker?\n\nAnswer", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3794}
{"pred": "Word subspace can represent sets of word vectors, retaining most of the variability of the data.\n\nQuestion: What is the main goal of the mutual subspace method?\n\nAnswer: The main goal of the mutual subspace method is to compare sets of word vectors, retaining most of the variability of the data.\n\nQuestion: What is the main goal of the term-frequency weighted word subspace?\n\nAnswer: The main goal of the term-frequency weighted word subspace is to compare sets of word vectors, retaining most of the variability of the data", "answers": ["Word vectors, usually in the context of others within the same class"], "all_classes": null, "length": 5151}
{"pred": "We use Random Forests (RF) BIBREF23 . We learn the RF on all computed features in Table TABREF21 . The optimization on RF is done by splitting the feature space into multiple trees that are considered as ensemble classifiers. Consequently, for each classifier it computes the margin function as a measure of the average count of predicting the correct class in contrast to any other class. The higher the margin score the more robust the model.\n\nQuestion: What is the evaluation metric used?\n\nAnswer: We compute precision P, recall R and F1 score for the relevant", "answers": ["For Article-Entity placement, they consider two baselines: the first one using only salience-based features, and the second baseline checks if the entity appears in the title of the article. \n\nFor Article-Section Placement, they consider two baselines: the first picks the section with the highest lexical similarity to the article, and the second one picks the most frequent section.", "B1. The first baseline uses only the salience-based features by Dunietz and Gillick BIBREF11 ., B2. The second baseline assigns the value relevant to a pair INLINEFORM0 , if and only if INLINEFORM1 appears in the title of INLINEFORM2 .\n\n, S1: Pick the section from template INLINEFORM0 with the highest lexical similarity to INLINEFORM1 : S1 INLINEFORM2, S2: Place the news into the most frequent section in INLINEFORM0"], "all_classes": null, "length": 7891}
{"pred": "No. SemCor3.0 is a manually annotated corpus of English language data. It is not a general English language corpus.\n\nQuestion: Is the article's title \"Introduction\" or \"Introduction to Word Sense Disambiguation\"?\n\nAnswer: The article's title is \"Introduction\".\n\nQuestion: Is the article's abstract \"Introduction\" or \"Introduction to Word Sense Disambiguation\"?\n\nAnswer: The article's abstract is \"Introduction\".\n\nQuestion: Is the article's introduction \"Introduction\" or \"Introduction to Word Sense Disambiguation\"?\n\nAnswer:", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 2000}
{"pred": "Augmented LibriSpeech dataset is a large-scale dataset for speech recognition and speech translation. It contains 960 hours of English speeches, which is 10 times larger than the original LibriSpeech dataset. The dataset is divided into train, development and test sets. The train set is used for training models, the development set is used for model evaluation, and the test set is used for model testing.\n\nQuestion: What is the size of the LibriSpeech dataset?\n\nAnswer: LibriSpeech dataset is a large-scale dataset for speech recognition and speech translation. It", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 2410}
{"pred": "The dataset they used is the SemEval-2016 dataset.\n\nQuestion: What is the difference between the ternary and fine-grained classification?\n\nAnswer: The ternary classification is a classification problem with three classes, while the fine-grained classification is a classification problem with five classes.\n\nQuestion: What is the difference between the ternary and fine-grained classification?\n\nAnswer: The ternary classification is a classification problem with three classes, while the fine-grained classification is a classification problem with five classes.\n\nQuestion: What is the difference", "answers": [" high-quality datasets  from SemEval-2016 “Sentiment Analysis in Twitter” task", " SemEval-2016 “Sentiment Analysis in Twitter”"], "all_classes": null, "length": 2738}
{"pred": "They use the pre-trained uncased BERT$_\\mathrm {BASE}$ model for fine-tuning.\n\nQuestion: Do they use the same hyper-parameters for all datasets?\n\nAnswer: No. They use the development set (SE07) to find the optimal settings for our experiments.\n\nQuestion: Do they use the same hyper-parameters for all models?\n\nAnswer: No. They keep the dropout probability at 0.1, set the number of epochs to 4. The initial learning rate is 2e-5, and the batch size is 6", "answers": ["small BERT", "small BERT"], "all_classes": null, "length": 1999}
{"pred": "Yes. We have a number of quality control checks in place. First, we use a set of heuristic functions to generate gold questions and distractors. We also use a set of baseline models to check for systematic biases in the data. We also use a set of cluster-based metrics to check for consistency and robustness.\n\nQuestion: What is the size of the datasets?\n\nAnswer: The size of the datasets varies. The WordNetQA dataset has 10k examples, the DictionaryQA dataset has 9k examples, and the WordSense dataset has ", "answers": ["No", "No"], "all_classes": null, "length": 6391}
{"pred": "The images are generated using the ShapeWorld framework. ShapeWorld is a controlled data generation framework consisting of abstract colored shapes.\n\nQuestion: What is the size of the training and test sets?\n\nAnswer: Each dataset variant consists of around 200k training instances and 4,096 validation instances, plus 4,096 test instances. Each training instance consists of an image and a reference caption. At test time, only the test images are available to the evaluated models.\n\nQuestion: What is the size of the vocabulary?\n\nAnswer: The vocab", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3472}
{"pred": "The best model (B-M) on development data relies entirely on automatically obtained information, both in terms of training data as well as features.\n\nArticle: Introduction\nThis work is licenced under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/\nIn the spirit of the brevity of social media's messages and reactions, people have got used to express feelings minimally and symbolically, as with hashtags on Twitter and Instagram. On Facebook, people tend to be more word", "answers": ["Answer with content missing: (Table 3) Best author's model B-M average micro f-score is 0.409, 0.459, 0.411 on Affective, Fairy Tales and ISEAR datasets respectively. "], "all_classes": null, "length": 3410}
{"pred": "The tagging scheme is designed to capture the structural property that each context contains a maximum of one pun.\n\nQuestion: What is the tagging scheme employed?\n\nAnswer: The tagging scheme is designed to capture the structural property that each context contains a maximum of one pun.\n\nQuestion: What is the tagging scheme employed?\n\nAnswer: The tagging scheme is designed to capture the structural property that each context contains a maximum of one pun.\n\nQuestion: What is the tagging scheme employed?\n\nAnswer: The tagging scheme is designed to capture the structural property that each", "answers": ["A new tagging scheme that tags the words before and after the pun as well as the pun words.", "a new tagging scheme consisting of three tags, namely { INLINEFORM0 }"], "all_classes": null, "length": 2974}
{"pred": "No, Arabic is not one of the 11 languages in CoVost.\n\nQuestion: Is there a way to get the audio files for the CoVost corpus?\n\nAnswer: No, there is no way to get the audio files for the CoVost corpus.\n\nQuestion: Is there a way to get the transcripts for the CoVost corpus?\n\nAnswer: No, there is no way to get the transcripts for the CoVost corpus.\n\nQuestion: Is there a way to get the translations for the CoVost corpus", "answers": ["No", "No"], "all_classes": null, "length": 2413}
{"pred": "Robustness is defined as the ability of a model to handle the bias in the prior knowledge.\n\nQuestion: What is the difference between the proposed methods and GE-FL?\n\nAnswer: GE-FL is a GE method which leverages labeled features as prior knowledge. The proposed methods are regularization terms on top of GE-FL.\n\nQuestion: What is the difference between the proposed methods and the maximum entropy principle?\n\nAnswer: The maximum entropy principle is a principle that maximizes the entropy of the model distribution. The proposed methods are regularization terms on top of GE-", "answers": ["ability to accurately classify texts even when the amount of prior knowledge for different classes is unbalanced, and when the class distribution of the dataset is unbalanced", "Low sensitivity to bias in prior knowledge"], "all_classes": null, "length": 3609}
{"pred": "We evaluate SBERT on the following sentence embedding methods:\n\n*\n\n*Average GloVe embeddings\n\n*InferSent BIBREF4\n\n*Universal Sentence Encoder BIBREF5\n\n*Average BERT embeddings\n\n*BERT CLS-token output\n\n*Skip-Thought BIBREF12\n\n*Poly-encoders BIBREF13\n\n*InferSent BIBREF4\n\n*Universal Sentence Encoder BIBREF5\n\n*Skip-Th", "answers": ["GloVe, BERT, Universal Sentence Encoder, TF-IDF, InferSent", "Avg. GloVe embeddings, Avg. fast-text embeddings, Avg. BERT embeddings, BERT CLS-vector, InferSent - GloVe and Universal Sentence Encoder."], "all_classes": null, "length": 3862}
{"pred": "The proposed method obtains significant performance boosts on both English and Chinese datasets. For English datasets, the proposed method outperforms BERT-MRC by +0.29 and +0.96 respectively. For Chinese datasets, the proposed method achieves F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively.\n\nQuestion: What are method's improvements of F1 for MRC task for English and Chinese datasets?\n\nAnswer: The proposed method obtains significant performance boosts on both English and Chinese datasets. For English", "answers": ["English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively, Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively", "For English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively., huge performance boosts on Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively"], "all_classes": null, "length": 3586}
{"pred": "They test their conflict method on two tasks:\n\n*\n\n*Task 1: Quora Duplicate Question Pair Detection\n\n*Task 2: Ranking questions in Bing's People Also Ask\n\n\nA: They test their conflict method on two tasks:\n\n*\n\n*Task 1: Quora Duplicate Question Pair Detection\n\n*Task 2: Ranking questions in Bing's People Also Ask\n\n\nA: They test their conflict method on two tasks:\n\n*\n\n*Task 1: Quora Duplicate Question", "answers": ["Task 1: Quora Duplicate Question Pair Detection, Task 2: Ranking questions", "Quora Duplicate Question Pair Detection, Ranking questions in Bing's People Also Ask"], "all_classes": null, "length": 2577}
{"pred": "The authors compare their model against the following baselines:\n\n*\n\n*Tree-LSTM (BIBREF13)\n\n*Tree-LSTM with tag embeddings (BIBREF18)\n\n*Tree-LSTM with tag embeddings and SPINN (BIBREF8)\n\n*Tree-LSTM with tag embeddings and SPINN (BIBREF8) with a bidirectional leaf-LSTM\n\n*Tree-LSTM with tag embeddings and SPINN (BIBREF8)", "answers": ["Various tree structured neural networks including variants of Tree-LSTM, Tree-based CNN, RNTN, and non-tree models including variants of LSTMs, CNNs, residual, and self-attention based networks", "Sentence classification baselines: RNTN (Socher et al. 2013), AdaMC-RNTN (Dong et al. 2014), TE-RNTN (Qian et al. 2015), TBCNN (Mou et al. 2015), Tree-LSTM (Tai, Socher, and Manning 2015), AdaHT-LSTM-CM (Liu, Qiu, and Huang 2017), DC-TreeLSTM (Liu, Qiu, and Huang 2017), TE-LSTM (Huang, Qian, and Zhu 2017), BiConTree (Teng and Zhang 2017), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), TreeNet (Cheng et al. 2018), CNN (Kim 2014), AdaSent (Zhao, Lu, and Poupart 2015), LSTM-CNN (Zhou et al. 2016), byte-mLSTM (Radford, Jozefowicz, and Sutskever 2017), BCN + Char + CoVe (McCann et al. 2017), BCN + Char + ELMo (Peters et al. 2018). \nStanford Natural Language Inference baselines: Latent Syntax Tree-LSTM (Yogatama et al. 2017), Tree-based CNN (Mou et al. 2016), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), NSE (Munkhdalai and Yu 2017), Reinforced Self- Attention Network (Shen et al. 2018), Residual stacked encoders: (Nie and Bansal 2017), BiLSTM with generalized pooling (Chen, Ling, and Zhu 2018)."], "all_classes": null, "length": 4781}
{"pred": "The core component for KBQA is the relation detection.\n\nQuestion: What is the difference between KBQA and QA?\n\nAnswer: KBQA is a task that answers questions about a knowledge base, while QA is a task that answers questions about a corpus.\n\nQuestion: What is the difference between KBQA and NLU?\n\nAnswer: KBQA is a task that answers questions about a knowledge base, while NLU is a task that answers questions about a corpus.\n\nQuestion: What is the difference between KBQA", "answers": ["answer questions by obtaining information from KB tuples ", "hierarchical matching between questions and relations with residual learning"], "all_classes": null, "length": 4527}
{"pred": "The baseline models are the encoder-decoder model and the checklist model.\n\nQuestion: What are the personalized models?\n\nAnswer: The personalized models are the personalized models that attend over the user's previously consumed recipes.\n\nQuestion: What are the evaluation metrics?\n\nAnswer: The evaluation metrics are the BLEU-1, BLEU-4, ROUGE-L, Distinct-1, Distinct-2, and Coherence metrics.\n\nQuestion: What are the coherence metrics?\n\nAnswer: The coher", "answers": ["name-based Nearest-Neighbor model (NN), Encoder-Decoder baseline with ingredient attention (Enc-Dec)"], "all_classes": null, "length": 2655}
{"pred": "The methods are manual inspection and automatic detection.\n\nQuestion: What is the purpose of the Flickr30K dataset?\n\nAnswer: The Flickr30K dataset is used to train and evaluate neural network models that generate image descriptions.\n\nQuestion: What is the assumption behind the Flickr30K dataset?\n\nAnswer: The assumption is that the descriptions are based on the images, and nothing else.\n\nQuestion: What is the problem with this assumption?\n\nAnswer: The problem is that stereotypes may be pervasive enough for the data to", "answers": ["spot patterns by just looking at a collection of images, tag all descriptions with part-of-speech information, I applied Louvain clustering", "Looking for adjectives marking the noun \"baby\" and also looking for most-common adjectives related to certain nouns using POS-tagging"], "all_classes": null, "length": 2204}
{"pred": "English\n\nQuestion: What is the gender of the pronoun?\n\nAnswer: They\n\nQuestion: What is the gender of the antecedent?\n\nAnswer: They\n\nQuestion: What is the gender of the pronoun?\n\nAnswer: They\n\nQuestion: What is the gender of the antecedent?\n\nAnswer: They\n\nQuestion: What is the gender of the pronoun?\n\nAnswer: They\n\nQuestion: What is the gender of the antecedent?\n\nAnswer: They\n\nQuestion: What is the gender of the pronoun?\n\nAnswer", "answers": ["English, French, German ", "French, English, Spanish, Italian, Portuguese, Hebrew, Arabic"], "all_classes": null, "length": 2285}
{"pred": "They experimented with the following models:\n\n*\n\n*Stacked LSTMs\n\n*Cell-aware Stacked LSTMs\n\n*Multidimensional RNNs\n\n*Tree-structured LSTMs\n\n*Grid LSTMs\n\n*Bidirectional LSTMs\n\n*\n\n*Stacked LSTMs\n\n*Cell-aware Stacked LSTMs\n\n*Multidimensional RNNs\n\n*Tree-structured LSTMs\n\n*Grid LSTMs\n\n*", "answers": ["Stacked LSTMs, Cell-aware Stacked LSTMs, Sentence Encoders, Top-layer Classifiers"], "all_classes": null, "length": 3210}
{"pred": "Yes, they report results only on English data.\n\nQuestion: Do they report results only on English data?\n\nAnswer: Yes, they report results only on English data.\n\nQuestion: Do they report results only on English data?\n\nAnswer: Yes, they report results only on English data.\n\nQuestion: Do they report results only on English data?\n\nAnswer: Yes, they report results only on English data.\n\nQuestion: Do they report results only on English data?\n\nAnswer: Yes, they report results only on English data.\n\nQuestion: Do they report results only on", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 6169}
{"pred": "We used the Sumy package to generate summaries.\n\nQuestion: What is the difference between the two algorithms used for multi-class multi-label classification?\n\nAnswer: The first algorithm is based on the co-training framework and uses goal descriptions and self-appraisal comments as two separate perspectives. The second approach uses semantic similarity under a weak supervision framework.\n\nQuestion: What is the difference between the two algorithms used for multi-class classification?\n\nAnswer: The first algorithm is based on the co-training framework and uses goal descriptions and self-appraisal comments as two", "answers": ["LSA, TextRank, LexRank and ILP-based summary.", "LSA, TextRank, LexRank"], "all_classes": null, "length": 3045}
{"pred": "BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 , BIBREF7 , BIBREF8 .\n\nQuestion: What is the primary problem?\n\nAnswer: Predicting instructor intervention.\n\nQuestion: What is the secondary problem?\n\nAnswer: Inferring the appropriate amount of context to intervene.\n\nQuestion: What is the primary model?\n\nAnswer: Hierarchical LSTM with attention.\n\nQuestion: What is the secondary model?\n\n", "answers": ["hLSTM", "hLSTM"], "all_classes": null, "length": 3725}
{"pred": "The least impactful component is the master node.\n\nQuestion: Which component is the most impactful?\n\nAnswer: The most impactful component is the master node.\n\nQuestion: Which component is the most impactful?\n\nAnswer: The most impactful component is the master node.\n\nQuestion: Which component is the least impactful?\n\nAnswer: The least impactful component is the master node.\n\nQuestion: Which component is the most impactful?\n\nAnswer: The most impactful component is the master node.\n\nQuestion: Which component is the least impactful?\n\n", "answers": ["Based on table results provided changing directed to undirected edges had least impact - max abs difference of 0.33 points on all three datasets."], "all_classes": null, "length": 4180}
{"pred": "The corpus used for the task is the diachronic corpus pair from BIBREF0: DTA18 and DTA19. They consist of subparts of DTA corpus BIBREF11 which is a freely available lemmatized, POS-tagged and spelling-normalized diachronic corpus of German containing texts from the 16th to the 20th century. DTA18 contains 26 million sentences published between 1750-1799 and DTA19 40 million between 1850-1", "answers": ["DTA18, DTA19", "Diachronic Usage Relatedness (DURel) gold standard data set"], "all_classes": null, "length": 1908}
{"pred": "Kannada, Hindi, Telugu, Malayalam, Bengali, and English.\n\nQuestion: What is the name of the article?\n\nAnswer: INTRODUCTION\n\nQuestion: What is the name of the author?\n\nAnswer: A. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S.", "answers": ["Hindi, English, Kannada, Telugu, Assamese, Bengali and Malayalam", "Kannada, Hindi, Telugu, Malayalam, Bengali,  English and Assamese (in table, missing in text)"], "all_classes": null, "length": 2453}
{"pred": "The model performance on target language reading comprehension is unanswerable.\n\nQuestion: What is the model performance on target language reading comprehension?\n\nAnswer: The model performance on target language reading comprehension is unanswerable.\n\nQuestion: What is the model performance on target language reading comprehension?\n\nAnswer: The model performance on target language reading comprehension is unanswerable.\n\nQuestion: What is the model performance on target language reading comprehension?\n\nAnswer: The model performance on target language reading comprehension is unanswerable.\n\nQuestion: What is the model performance on target language", "answers": ["Table TABREF6, Table TABREF8", "when testing on English, the F1 score of the model training on Chinese (Zh) is 53.8,  F1 score is only 44.1 for the model training on Zh-En"], "all_classes": null, "length": 2492}
{"pred": "The difference in performance between ALOHA and the baselines is significant. ALOHA achieves a mean Hits@1/20 score of 40.67% compared to the baseline mean of 30.67%. This is a 30% improvement.\n\nQuestion: How big is the difference in performance between proposed model and human?\n\nAnswer: The difference in performance between ALOHA and humans is significant. ALOHA achieves a mean Hits@1/20 score of 40.67% compared to the human mean of 40.", "answers": ["Metric difference between Aloha and best baseline score:\nHits@1/20: +0.061 (0.3642 vs 0.3032)\nMRR: +0.0572(0.5114 vs 0.4542)\nF1: -0.0484 (0.3901 vs 0.4385)\nBLEU: +0.0474 (0.2867 vs 0.2393)"], "all_classes": null, "length": 5151}
{"pred": "ARAML outperforms other baselines in terms of forward perplexity and Self-BLEU, indicating that our model can generate more fluent samples.\nQuestion: How does ARAML perform in terms of reverse perplexity?\n\nAnswer: ARAML outperforms other baselines in terms of reverse perplexity, indicating that our model can generate more diverse samples.\nQuestion: How does ARAML perform in terms of standard deviation?\n\nAnswer: ARAML outperforms other baselines in terms of standard deviation, indicating that our model is more stable in adversarial training.", "answers": ["ARAM has achieved improvement over all baseline methods using reverese perplexity and  slef-BLEU metric.  The maximum reverse perplexity improvement 936,16 is gained for EMNLP2017  WMT  dataset and  48,44 for COCO dataset.", "Compared to the baselines, ARAML does not do better in terms of perplexity on COCO and EMNLP 2017 WMT datasets, but it does by up to 0.27 Self-BLEU points on COCO and 0.35 Self-BLEU on EMNLP 2017 WMT. In terms of Grammaticality and Relevance, it scores better than the baselines on up to 75.5% and 73% of the cases respectively."], "all_classes": null, "length": 3796}
{"pred": "The authors present evidence that the model can capture some biases in data annotation and collection by examining the test datasets and their confusion matrices resulted from the BERTbase + CNN model as the best fine-tuning approach. According to Figure FIGREF19 for Waseem-dataset, it is obvious that the model can separate sexism from racism content properly. Only two samples belonging to racism class are misclassified as sexism and none of the sexism samples are misclassified as racism. A large majority of the errors come from misclassifying hateful categories (racism and sexism) as", "answers": ["The authors showed few tweets where neither and implicit hatred content exist but the model was able to discriminate"], "all_classes": null, "length": 4119}
{"pred": "We did not test other baselines, as we were interested in the performance of neural models on this task.\n\nQuestion: What is the size of the test set?\n\nAnswer: The test set is 400 questions over 8 policies.\n\nQuestion: What is the size of the training set?\n\nAnswer: The training set is 27 policies and 1350 questions.\n\nQuestion: What is the size of the development set?\n\nAnswer: The development set is 27 policies and 1350 questions.\n\nQuestion: What is the size of", "answers": ["SVM, No-Answer Baseline (NA) , Word Count Baseline, Human Performance", "No-Answer Baseline (NA), Word Count Baseline, Human Performance"], "all_classes": null, "length": 3855}
{"pred": "The dataset is 10000 sentences long.\n\nQuestion: What is the size of the vocabulary?\n\nAnswer: The vocabulary size is 16225.\n\nQuestion: What is the size of the training set?\n\nAnswer: The training set is 64% of the total dataset.\n\nQuestion: What is the size of the development set?\n\nAnswer: The development set is 16% of the total dataset.\n\nQuestion: What is the size of the test set?\n\nAnswer: The test set is 2", "answers": ["Dataset contains 3606 total sentences and 79087 total entities.", "ILPRL contains 548 sentences, OurNepali contains 3606 sentences"], "all_classes": null, "length": 2843}
{"pred": "We find that replacing the training objective with DSC introduces performance boost for both BERT and XLNet. Using DSC loss improves the F1 score by +0.58 for MRPC and +0.73 for QQP.\n\nQuestion: What are the hyperparameters of TI?\n\nAnswer: As mentioned in Section SECREF10, TI offers the flexibility in controlling the tradeoff between false-negatives and false-positives. In this subsection, we explore the effect of hyperparameters (i.e., $\\alpha $ and $\\beta $", "answers": ["Using DSC loss improves the F1 score by +0.58 for MRPC and +0.73 for QQP", "+0.58"], "all_classes": null, "length": 3566}
{"pred": "The datasets used are the same as in the main paper.\n\nQuestion: What is the architecture of the neural network?\n\nAnswer: The architecture of the neural network is the same as in the main paper.\n\nQuestion: What is the training objective?\n\nAnswer: The training objective is the same as in the main paper.\n\nQuestion: What is the training data?\n\nAnswer: The training data is the same as in the main paper.\n\nQuestion: What is the evaluation data?\n\nAnswer: The evaluation data is the same as in the main paper.\n\nQuestion: What is", "answers": ["Answer with content missing: (Whole Method and Results sections) The primary dataset we use is the ERP data collected and computed by Frank et al. (2015), and we also use behavioral data (eye-tracking data and self-paced reading times) from Frank et al. (2013) which were collected on the same set of 205 sentences.\nSelect:\n- ERP data collected and computed by Frank et al. (2015)\n- behavioral data (eye-tracking data and self-paced reading times) from Frank et al. (2013)", "the ERP data: BIBREF0"], "all_classes": null, "length": 1971}
{"pred": "The subjects were presented with a series of 11-second-long audio-visual stimuli, consisting of a 5-second-long audio clip and a 5-second-long video clip. The audio clip was a 1-second-long sound clip of a single vowel (either /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/) or a 1-second-long sound clip of a single word (either pat, pot, knew or gnaw). The video clip was a 5-second-long video clip", "answers": ["7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)", "KARA ONE BIBREF17 , composed of multimodal data for stimulus-based, imagined and articulated speech state corresponding to 7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)"], "all_classes": null, "length": 2379}
{"pred": "We compare our model with the following baselines.\nPointer-Gen is the baseline model trained by optimizing $L_\\text{MLE}$ in Equation DISPLAY_FORM13.\nPointer-Gen+Pos is the baseline model by training Pointer-Gen only on positive examples whose sensationalism score is larger than 0.5\nPointer-Gen+Same-FT is the model which fine-tunes Pointer-Gen on the training samples whose sensationalism score is larger than 0.1\nPointer-Gen+Pos-FT is the model which fine-tunes", "answers": ["Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN"], "all_classes": null, "length": 4085}
{"pred": "We implement five feature engineering based machine learning classifiers that are most often used for abusive language detection. In data preprocessing, text sequences are converted into Bag Of Words (BOW) representations, and normalized with Term Frequency-Inverse Document Frequency (TF-IDF) values. We experiment with word-level features using n-grams ranging from 1 to 3, and character-level features from 3 to 8-grams. Each classifier is implemented with the following specifications:\nNaïve Bayes (NB): Multinomial NB with additive sm", "answers": ["Naïve Bayes (NB), Logistic Regression (LR), Support Vector Machine (SVM), Random Forests (RF), Gradient Boosted Trees (GBT),  Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN)", "Naïve Bayes (NB), Logistic Regression (LR), Support Vector Machine (SVM), Random Forests (RF), Gradient Boosted Trees (GBT), CNN, RNN"], "all_classes": null, "length": 2074}
{"pred": "We use a bi-directional language model to augment the sequence to sequence encoder and a uni-directional model to augment the decoder. Both use self-attention. The bi-directional model solves a cloze-style token prediction task at training time. The model consists of two towers, the forward tower operates left-to-right and the tower operating right-to-left as backward tower; each tower contains transformer blocks. The forward and backward representations are combined via a self-attention module and the output of this module is used to predict the token at position. The model", "answers": ["uni-directional model to augment the decoder", "bi-directional language model to augment the sequence to sequence encoder ,  uni-directional model to augment the decoder"], "all_classes": null, "length": 1914}
{"pred": "We use a dynamic weight adjusting strategy, which associates each training example with a weight in proportion to $(1-p)$, and this weight dynamically changes as training proceeds. This strategy helps to deemphasize confident examples during training as their $p$ approaches the value of 1, makes the model attentive to hard-negative examples, and thus alleviates the dominating effect of easy-negative examples.\n\nQuestion: How does the proposed method compare with other methods?\n\nAnswer: We compare the proposed method with other methods on four NLP tasks: part-of-speech tagging,", "answers": ["One can think $(1-p_{i1})$ as a weight associated with each example, which changes as training proceeds. The intuition of changing $p_{i1}$ to $(1-p_{i1}) p_{i1}$ is to push down the weight of easy examples. For easy examples whose probability are approaching 0 or 1, $(1-p_{i1}) p_{i1}$ makes the model attach significantly less focus to them. Figure FIGREF23 gives gives an explanation from the perspective in derivative: the derivative of $\\frac{(1-p)p}{1+(1-p)p}$ with respect to $p$ approaches 0 immediately after $p$ approaches 0, which means the model attends less to examples once they are correctly classified.", "associates each training example with a weight in proportion to $(1-p)$, and this weight dynamically changes as training proceeds"], "all_classes": null, "length": 3640}
{"pred": "The results from these proposed strategies are that the knowledge graph is critical to the success of the agent. The knowledge graph is able to help the agent pass the bottleneck in Zork1. The knowledge graph is able to help the agent pass the bottleneck in Zork1. The knowledge graph is able to help the agent pass the bottleneck in Zork1. The knowledge graph is able to help the agent pass the bottleneck in Zork1. The knowledge graph is able to help the agent pass the bottleneck in Zork1. The knowledge graph is able to help", "answers": ["Reward of 11.8 for the A2C-chained model, 41.8 for the KG-A2C-chained model, 40 for A2C-Explore and 44 for KG-A2C-Explore.", "KG-A2C-chained and KG-A2C-Explore both pass the bottleneck of a score of 40"], "all_classes": null, "length": 2443}
{"pred": "The individual model consists of a set of parameters, which are learned from the data.\n\nQuestion: What is the generative process of the model?\n\nAnswer: The generative process is as follows:\n\nQuestion: What is the probability of a role being assigned to a constituent?\n\nAnswer: The probability of a role being assigned to a constituent is given by the following equation:\n\nQuestion: What is the probability of a role being assigned to a constituent given the predicate, its voice, and syntactic features of all the identified arguments?\n\nAnswer: The probability of a role being", "answers": ["Bayesian model of garg2012unsupervised as our base monolingual model"], "all_classes": null, "length": 3701}
{"pred": "The transcriptions were done by native speakers of Mapudungun, who were trained in the use of the transcription tool. The transcribers were also trained in the use of the digital recording systems that were available at the time. We also followed human subject protocol. Each person signed a consent form to release the recordings for research purposes and the data have been accordingly anonymized.\n\nQuestion: How many speakers are in the corpus?\n\nAnswer: The corpus includes 285 dialogues, with 12 and 46 conversations reserved for the development and test set.", "answers": ["Unanswerable", "Original transcription was labeled with additional labels in [] brackets with nonstandard pronunciation."], "all_classes": null, "length": 3018}
{"pred": "A semichar architecture is a neural network architecture that takes a sequence of characters as input and outputs a sequence of characters.\n\nQuestion: What is a word-only architecture?\n\nAnswer: A word-only architecture is a neural network architecture that takes a sequence of words as input and outputs a sequence of words.\n\nQuestion: What is a char-only architecture?\n\nAnswer: A char-only architecture is a neural network architecture that takes a sequence of characters as input and outputs a sequence of characters.\n\nQuestion: What is a word+char architecture?\n\nAnswer: A word+char architecture", "answers": ["A semi-character based RNN (ScRNN) treats the first and last characters individually, and is agnostic to the ordering of the internal characters", "processes a sentence of words with misspelled characters, predicting the correct words at each step"], "all_classes": null, "length": 4186}
{"pred": "16 languages are explored: Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish.\n\nQuestion: what is the name of the system used for the feature-based models?\n\nAnswer: MElt.\n\nQuestion: what is the name of the system used for the neural models?\n\nAnswer: MarMoT.\n\nQuestion: what is the name of the system used for the feature-based models?\n\nAnswer: MElt.\n\nQuestion:", "answers": ["Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish", "Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish , Swedish"], "all_classes": null, "length": 2697}
{"pred": "NCEL outperforms all baseline methods in most cases with an average gain of 2% on Micro F1 and 3% Macro F1.\n\nQuestion: What is the impact of the prior probability?\n\nAnswer: The prior probability performs quite well in TAC2010 but poorly in WW. Compared with NCEL-local, the global module in NCEL brings more improvements in the \"hard\" case than that for \"easy\" dataset, because local features are discriminative enough in most cases of TAC2010, and global information becomes", "answers": ["NCEL consistently outperforms various baselines with a favorable generalization ability"], "all_classes": null, "length": 4113}
{"pred": "Yes, the data is de-identified.\n\nQuestion: What is the average number of words in a sentence?\n\nAnswer: The average number of words in a sentence is 15.\n\nQuestion: What is the average number of sentences in a transcript?\n\nAnswer: The average number of sentences in a transcript is 15.\n\nQuestion: What is the average number of words in a transcript?\n\nAnswer: The average number of words in a transcript is 15.\n\nQuestion: What is the average number of sentences in a transcript?\n\n", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4592}
{"pred": "The baseline used was the system by Rei2016, trained on the same FCE dataset.\n\nQuestion: What is the difference between the two error generation methods?\n\nAnswer: The pattern-based method is based on statistical machine translation, while the machine translation method is based on learning to translate from correct to incorrect sentences.\n\nQuestion: What is the difference between the two error detection models?\n\nAnswer: The error detection model is based on a neural sequence labeling model, while the other model is based on a traditional sequence labeling model.\n\nQuestion: What is the difference between the", "answers": ["error detection system by Rei2016", "error detection system by Rei2016"], "all_classes": null, "length": 2132}
{"pred": "The annotated clinical notes were obtained from the 2010 i2b2/VA BIBREF0 challenge.\n\nQuestion: what is the difference between the two models?\n\nAnswer: The i2b2 NER model was trained on clinical notes only, while the hybrid NER model was trained on the hybrid data consisting of clinical notes and synthesized user queries.\n\nQuestion: what is the difference between the two datasets?\n\nAnswer: The i2b2 data is the clinical notes from the 2010 i2b2/VA B", "answers": ["clinical notes from the CE task in 2010 i2b2/VA", "clinical notes from the CE task in 2010 i2b2/VA "], "all_classes": null, "length": 3432}
{"pred": "MSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSMS", "answers": ["ecause this process is similar to the cloze task in BERT's pre-train process, therefore by using the ability of the contextual language model the decoder can generate more fluent and natural sequences."], "all_classes": null, "length": 3919}
{"pred": "They use the Twitter dataset from the SemEval 2015 task.\n\nQuestion: What is the objective function of the model?\n\nAnswer: The objective function of the model is to predict the next word in a sentence.\n\nQuestion: What is the architecture of the model?\n\nAnswer: The model is a recurrent neural network with a bidirectional LSTM layer.\n\nQuestion: What is the training objective?\n\nAnswer: The training objective is to minimize the negative log-likelihood of the next word in a sentence.\n\nQuestion: What is the test objective", "answers": ["Unanswerable", " Paraphrase Database (PPDB) ,  book corpus", "Unanswerable"], "all_classes": null, "length": 1902}
{"pred": "The features used are TF-IDF features.\n\nQuestion: What is the accuracy of the model?\n\nAnswer: The accuracy of the model is 92%.\n\nQuestion: What is the F-score of the model?\n\nAnswer: The F-score of the model is 0.92.\n\nQuestion: What is the F-score of the model for the macro metric?\n\nAnswer: The F-score of the model for the macro metric is 0.31.\n\nQuestion: What is the F-score of the model for the micro metric?\n", "answers": ["Unanswerable"], "all_classes": null, "length": 2108}
{"pred": "The dataset is annotated by a human annotator.\n\nQuestion: What is the dataset size?\n\nAnswer: The dataset contains 9,473 annotations for 9,300 tweets.\n\nQuestion: What is the dataset split?\n\nAnswer: The dataset is split into 9,473 annotations for 9,300 tweets.\n\nQuestion: What is the dataset encoding?\n\nAnswer: The dataset is encoded with 7 feature groups with associated feature values binarized (i.e., present=1 or absent=0) to represent potentially inform", "answers": ["no evidence of depression, depressed mood, disturbed sleep, fatigue or loss of energy", "The annotations are based on evidence of depression and further annotated by the depressive symptom if there is evidence of depression"], "all_classes": null, "length": 1947}
{"pred": "They evaluated on the eight NER tasks used in BIBREF2.\n\nQuestion: What is the difference between the BioBERTv1.0 and BioBERTv1.1?\n\nAnswer: BioBERTv1.0 was initialized from general-domain BERT (bert-base-cased) and pretrained on PubMed+PMC. BioBERTv1.1 was initialized from general-domain BERT (bert-large-uncased) and pretrained on PubMed+PMC.\n\nQuestion: What is the difference between the BioBERTv1", "answers": ["BC5CDR-disease, NCBI-disease, BC5CDR-chem, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800", "BC5CDR-disease, NCBI-disease, BC5CDR-chem, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800"], "all_classes": null, "length": 2800}
{"pred": "The training data was translated using the machine translation platform Apertium.\n\nQuestion: What is the size of the training data?\n\nAnswer: The training data contains 100,000 tweets.\n\nQuestion: What is the size of the test data?\n\nAnswer: The test data contains 10,000 tweets.\n\nQuestion: What is the size of the development data?\n\nAnswer: The development data contains 10,000 tweets.\n\nQuestion: What is the size of the silver data?\n\nAnswer: The silver data", "answers": ["using the machine translation platform Apertium ", "machine translation platform Apertium BIBREF5"], "all_classes": null, "length": 2423}
{"pred": "They used a multinomial NB classifier.\n\nQuestion: What is the difference between a multinomial NB classifier and a multinomial logistic regression?\n\nAnswer: The multinomial NB classifier is a special case of the multinomial logistic regression.\n\nQuestion: What is the difference between a multinomial NB classifier and a multinomial logistic regression?\n\nAnswer: The multinomial NB classifier is a special case of the multinomial logistic regression.\n\nQuestion: What is the difference between a multinomial NB", "answers": ["AllWords model by counting the frequencies of all the remaining words and training a multinomial Naive Bayes classifier"], "all_classes": null, "length": 4177}
{"pred": "The baseline for the FLC task was a random baseline, where a word was selected randomly from the 18 techniques. The baseline for the SLC task was a logistic regression classifier with default parameters, where the input instance was represented by a single feature: the length of the sentence.\n\nQuestion: What is the difference between the development and the test set?\n\nAnswer: The development set was used to evaluate the participants' performance on the FLC task. The test set was used to evaluate the participants' performance on the SLC task.\n\nQuestion: What is the difference between the development", "answers": ["The baseline system for the SLC task is a very simple logistic regression classifier with default parameters. The baseline for the FLC task generates spans and selects one of the 18 techniques randomly.", "SLC task is a very simple logistic regression classifier, FLC task generates spans and selects one of the 18 techniques randomly"], "all_classes": null, "length": 3001}
{"pred": "The baselines are the systems that do not adopt joint learning.\n\nQuestion: What is the difference between the INLINEFORM0 and INLINEFORM1 tagging schemes?\n\nAnswer: The INLINEFORM0 tagging scheme is the simple one that does not consider the structural constraint that there exists a maximum of one pun in a context. The INLINEFORM1 tagging scheme is the one that considers such a constraint.\n\nQuestion: What is the difference between the INLINEFORM2 and INLINEFORM3 tagging schemes?\n\nAnswer: The INLINEFORM2 tagging scheme is the one", "answers": ["They compare with the following models: by Pedersen (2017), by Pramanick and Das (2017), by Mikhalkova and Karyakin (2017),  by Vadehra (2017), Indurthi and Oota (2017), by Vechtomova (2017), by (Cai et al., 2018), and CRF."], "all_classes": null, "length": 2991}
{"pred": "We used the procedure described in BIBREF2 to label different outlets as left-biased or right-biased.\n\nQuestion: How do you define the political bias of a news source?\n\nAnswer: We used the list of sources provided in BIBREF18.\n\nQuestion: How do you define the political bias of a news article?\n\nAnswer: We used the list of sources provided in BIBREF18.\n\nQuestion: How do you define the political bias of a tweet?\n\nAnswer: We used the list of sources provided in BIBREF18.", "answers": ["By assigning a political bias label to each news article and training only on left-biased or right-biased outlets of both disinformation and mainstream domains", "we also account for political biases inherent to different news sources, referring to the procedure proposed in BIBREF2 to label different outlets. Overall we show that we are able to classify credible vs non-credible diffusion networks (and consequently news articles) with high accuracy (AUROC up to 94%), even when accounting for the political bias of sources (and training only on left-biased or right-biased articles). We observe that the layer of mentions alone conveys useful information for the classification, denoting a different usage of this functionality when sharing news belonging to the two news domains. We also show that most discriminative features, which are relative to the breadth and depth of largest cascades in different layers, are the same across the two countries."], "all_classes": null, "length": 4882}
{"pred": "We collected 1.7K bilingual ancient-modern Chinese articles from the internet. More specifically, a large part of the ancient Chinese data we used come from ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era. They used plain and accurate words to express what happened at that time, and thus ensure the generality of the translated materials.\nQuestion: How do you get the data augmentation?\n\nAnswer: Given an aligned clause pair, we merged its adjacent clause pairs as a new sample pair. For", "answers": ["ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era", "Ancient Chinese history records in several dynasties and articles written by celebrities during 1000BC-200BC collected from the internet "], "all_classes": null, "length": 3722}
{"pred": "English\n\nQuestion: What is the target of the offensive language?\n\nAnswer: Individual\n\nQuestion: What is the type of the offensive language?\n\nAnswer: Profanity\n\nQuestion: What is the target of the profanity?\n\nAnswer: Individual\n\nQuestion: What is the type of the insult?\n\nAnswer: Insult\n\nQuestion: What is the target of the insult?\n\nAnswer: Individual\n\nQuestion: What is the type of the threat?\n\nAnswer: Threat\n\nQuestion: What is the target of", "answers": ["English", "English ", "English"], "all_classes": null, "length": 2240}
{"pred": "We used the PTB dataset for English and the CTB dataset for Chinese.\n\nQuestion: what is the difference between the neural PCFG and the compound PCFG?\n\nAnswer: The neural PCFG is a PCFG with neural rule probabilities. The compound PCFG is a PCFG with neural rule probabilities and a latent vector.\n\nQuestion: what is the difference between the neural PCFG and the compound PCFG?\n\nAnswer: The neural PCFG is a PCFG with neural rule probabilities. The compound PCFG", "answers": ["Answer with content missing: (Data section) Chinese with version 5.1 of the Chinese Penn Treebank (CTB)"], "all_classes": null, "length": 2545}
{"pred": "The model has three layers: the user matrix embedding layer, the user vector embedding layer, and the document composition layer.\n\nQuestion: What is the difference between the user matrix embedding layer and the user vector embedding layer?\n\nAnswer: The user matrix embedding layer is used to generate user embeddings for all users, including authors and likers. The user vector embedding layer is used to generate user embeddings for users who have never posted anything.\n\nQuestion: What is the difference between the topic matrix embedding layer and the topic vector embedding layer?\n\nAnswer: The topic matrix embedding layer is used to generate topic", "answers": ["eight layers"], "all_classes": null, "length": 4487}
{"pred": "The dataset used in this paper is the same as the one used in BIBREF7 .\n\nQuestion: what is the difference between the tag selection method in this paper and the one in BIBREF7 ?\n\nAnswer: The tag selection method in this paper is based on KL divergence, while the one in BIBREF7 is based on Bayesian smoothing.\n\nQuestion: what is the difference between the tag selection method in this paper and the one in BIBREF50 ?\n\nAnswer: The tag selection method in this paper is based on KL divergence, while the one", "answers": [" the same datasets as BIBREF7", "same datasets as BIBREF7"], "all_classes": null, "length": 4661}
{"pred": "The clinical datasets used in the paper are NUBes-PHI and MEDDOCAN.\n\nQuestion: What is the difference between NUBes-PHI and MEDDOCAN?\n\nAnswer: NUBes-PHI is a corpus of real medical reports manually annotated with sensitive information. MEDDOCAN is a corpus of clinical cases enriched with sensitive information by health documentalists.\n\nQuestion: What is the difference between the two experiments?\n\nAnswer: The first experiment set uses NUBes-PHI, a corpus of real medical reports manually", "answers": ["MEDDOCAN, NUBes-PHI", "MEDDOCAN, NUBes "], "all_classes": null, "length": 4528}
{"pred": "Unigram (with principal components of unigram feature vectors), Sarcasm (the feature-set reported by joshi2015harnessing subsuming unigram features and features from other reported systems), and Gaze (the simple and complex cognitive features we introduce, along with readability and word count features).\n\nQuestion: What is the difference between the gaze features and the gaze+sarcasm features?\n\nAnswer: The gaze features are the simple gaze based features, whereas the gaze+sarcasm features are the gaze features augmented with the s", "answers": ["Unanswerable"], "all_classes": null, "length": 3543}
{"pred": "The metrics used to establish that this makes chatbots more knowledgeable and better at learning and conversation are the following:\n\n*\n\n*The metrics used to establish that this makes chatbots more knowledgeable and better at learning and conversation are the following:\n\n*The metrics used to establish that this makes chatbots more knowledgeable and better at learning and conversation are the following:\n\n*The metrics used to establish that this makes chatbots more knowledgeable and better at learning and conversation are the following:\n\n*The metrics used to establish that this makes chatbots more knowledgeable and better at", "answers": ["Coverage, Avg. MCC and avg. +ve F1 score", "strategy formulation ability, we introduce a measure called Coverage( INLINEFORM0 ), To evaluate the predictive performance, we use Avg. MCC and avg. +ve F1 score"], "all_classes": null, "length": 5869}
{"pred": "No, they do not. They use the same Wikipedia dataset as the one used in the previous work.\n\nQuestion: What is the size of the dataset?\n\nAnswer: The size of the dataset is 107K+ questions on 536 Wikipedia articles.\n\nQuestion: What is the size of the answer candidates?\n\nAnswer: The size of the answer candidates is 107K+ sentences.\n\nQuestion: What is the size of the answer contexts?\n\nAnswer: The size of the answer contexts is 107K+ paragraphs.\n\nQuestion", "answers": ["Yes", "No"], "all_classes": null, "length": 1910}
{"pred": "Galatasaray and Fenerbahçe.\n\nQuestion: Which stance is more common in the tweets?\n\nAnswer: Favor.\n\nQuestion: Which stance is more common in the tweets?\n\nAnswer: Against.\n\nQuestion: Which stance is more common in the tweets?\n\nAnswer: Neither.\n\nQuestion: Which stance is more common in the tweets?\n\nAnswer: Unanswerable.\n\nQuestion: Which stance is more common in the tweets?\n\nAnswer: Unanswerable.\n\nQuestion:", "answers": ["Galatasaray, Fenerbahçe", "Galatasaray , Fenerbahçe "], "all_classes": null, "length": 2234}
{"pred": "We crawl over 2M tweets from twitter to build a dataset with 262,755 ironic and 112,330 non-ironic tweets. Then, we implement a neural network to transfer non-ironic sentences to ironic sentences in an unsupervised way. As ironic style is hard to model and describe, we implement our model with the control of classifiers and reinforcement learning. Different from other studies in style transfer, the transformation from non-ironic to ironic sentences has to preserve sentiment polarity as mentioned above. Therefore, we not", "answers": ["Irony Classifier, Sentiment Classifier for Irony, Sentiment Classifier for Non-irony, transformation from ironic sentences to non-ironic sentences"], "all_classes": null, "length": 4600}
{"pred": "Gaussian-masked directional multi-head attention is a variant of multi-head attention which is used to capture the localness information and directional information. Gaussian-masked directional attention is a function to map queries and key-value pairs to the representation of input. Gaussian-masked directional attention is calculated by dotting query $Q$ with all keys $K$, dividing each values by $\\sqrt{d_k}$, where $\\sqrt{d_k}$ is the dimension of keys, and apply a softmax function to generate the weights in the attention:\nDifferent from scaled dot-product attention", "answers": ["pays attentions to adjacent characters and casts a localness relationship between the characters as a fixed Gaussian weight assuming the weight relies on the distance between characters", "Gaussian-masked directional attention can be described as a function to map queries and key-value pairs to the representation of input, Gaussian-masked directional attention expects to pay attention to the adjacent characters of each positions and cast the localness relationship between characters as a fix Gaussian weight for attention, Gaussian weight only relys on the distance between characters"], "all_classes": null, "length": 3612}
{"pred": "They considered Facebook, Twitter, and Reddit.\n\nQuestion: What is the purpose of the study?\n\nAnswer: The purpose of the study is to investigate the relationship between causal explanations and demographics.\n\nQuestion: What is the relationship between causal explanations and demographics?\n\nAnswer: The relationship between causal explanations and demographics is that causal explanations are more likely to be used by older people and women.\n\nQuestion: What is the relationship between causal explanations and polarity ratings?\n\nAnswer: The relationship between causal explanations and polar", "answers": ["Facebook status update messages", "Facebook status update messages"], "all_classes": null, "length": 4005}
{"pred": "The baseline features are the features extracted from the baseline CNN.\nQuestion: What is the network's baseline method?\n\nAnswer: The baseline method is the method that uses the baseline CNN to extract features.\nQuestion: What is the network's baseline + sentiment + emotion + personality method?\n\nAnswer: The baseline + sentiment + emotion + personality method is the method that uses the baseline CNN to extract features and then merges the features extracted by the pre-trained sentiment, emotion and personality models.\nQuestion: What is the network's", "answers": [" The features extracted from CNN."], "all_classes": null, "length": 4855}
{"pred": "The hyperparameters varied in the experiments are the number of clusters and the number of dimensions of the word vectors.\n\nQuestion: What is the difference between the skip-gram model and the cbow model?\n\nAnswer: The skip-gram model is a type of word embedding model that is used to generate word vectors. The cbow model is a type of word embedding model that is used to generate word vectors.\n\nQuestion: What is the difference between the skip-gram model and the glove model?\n\nAnswer: The skip-gram model is a type of word embedding model that is used to generate word vectors", "answers": ["number of clusters, seed value in clustering, selection of word vectors, window size and dimension of embedding", "different number of clusters, different embeddings"], "all_classes": null, "length": 2753}
{"pred": "The scores of their system are 0.716, 0.716, 0.716, 0.716, 0.716, 0.716, 0.716, 0.716, 0.716, 0.716, 0.716, 0.716, 0.716, 0.716, 0.716, 0.716, 0.716, 0.", "answers": ["column Ens Test in Table TABREF19"], "all_classes": null, "length": 2424}
{"pred": "The corpus contains 53 documents, which contain an average number of 156.1 sentences per document, each with 19.55 tokens on average. The corpus comprises 8,275 sentences and 167,739 words in total. However, as mentioned above, only case presentation sections, headings and abstracts are annotated. The numbers of annotated entities are summarized in Table TABREF24.\n\nQuestion: What is the average length of entities?\n\nAnswer: The numbers for conditions and factors are lower with seven and two,", "answers": ["8,275 sentences and 167,739 words in total", "The corpus comprises 8,275 sentences and 167,739 words in total."], "all_classes": null, "length": 2669}
{"pred": "Yes, we can convert cloze-style questions to naturally-looking questions. We can use a language model to generate a question from a cloze-style question.\n\nQuestion: How do you generate cloze-style questions?\n\nAnswer: We use a standard NLP pipeline based on Stanford CoreNLP (for SQuAD, TrivaQA and PubMed) and the BANNER Named Entity Recognizer (only for PubMed articles) to identify entities and phrases. Assume that a document comprises of introduction sentences $\\lbrace q_1, q_2, ... q", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 2764}
{"pred": "They consider text categorization, sentiment classification, and topic modeling.\n\nQuestion: What is the difference between the first and second line?\n\nAnswer: The first line is the introduction of the paper, and the second line is the summary of the paper.\n\nQuestion: What is the difference between the first and second paragraph?\n\nAnswer: The first paragraph is the introduction of the paper, and the second paragraph is the summary of the paper.\n\nQuestion: What is the difference between the first and second sentence?\n\nAnswer: The first sentence is the introduction of the paper, and the second sentence is", "answers": ["text classification for themes including sentiment, web-page, science, medical and healthcare"], "all_classes": null, "length": 3591}
{"pred": "The authors compare their model to the best performing model on the TREC-6 dataset, which is a model that uses hand-crafted rules to infer question classes.\n\nQuestion: What is the performance of their model?\n\nAnswer: The authors report that their model achieves state-of-the-art performance on TREC-6, and near state-of-the-art performance on TREC-50.\n\nQuestion: What is the performance of their model on the ARC dataset?\n\nAnswer: The authors report that their model achieves state-of-the-art", "answers": ["bag-of-words model, CNN"], "all_classes": null, "length": 5838}
{"pred": "The training sets are larger by a factor of 1000.\n\nQuestion: How many languages are included in the ELMoForManyLangs project?\n\nAnswer: The project includes 100 languages.\n\nQuestion: How many languages are included in the EMBEDDIA project?\n\nAnswer: The project includes 7 languages.\n\nQuestion: How many languages are included in the ELMoForManyLangs project?\n\nAnswer: The project includes 100 languages.\n\nQuestion: How many languages are included in the EMBEDDIA project?\n\n", "answers": ["By 14 times.", "up to 1.95 times larger"], "all_classes": null, "length": 3294}
{"pred": "The dataset contains 10000 sentences.\n\nQuestion: What is the average length of a sentence?\n\nAnswer: The average length of a sentence is 15.\n\nQuestion: What is the average number of entities per sentence?\n\nAnswer: The average number of entities per sentence is 1.\n\nQuestion: What is the average number of words per sentence?\n\nAnswer: The average number of words per sentence is 15.\n\nQuestion: What is the average number of characters per word?\n\nAnswer: The average number of characters per word is 3.\n", "answers": ["3606", "6946"], "all_classes": null, "length": 2835}
{"pred": "They compare to MLP, Eusboost and MWMOTE.\n\nQuestion: What is the difference between the proposed method and the other methods?\n\nAnswer: The proposed method is a novel approach to address the task of classification in low data resource scenarios. The proposed approach involves simultaneously considering more than one sample (in this work, two samples are considered) to train the classifier. The proposed approach is also applicable to low resource data suffering with data imbalance.\n\nQuestion: What is the difference between the proposed method and the other methods?\n\nAnswer: The proposed method is a novel approach to address", "answers": ["MLP", "Eusboost, MWMOTE"], "all_classes": null, "length": 2474}
{"pred": "Yes, the model learns NER from both text and images.\n\nQuestion: What is the name of the model?\n\nAnswer: The model is called Bi-LSTM-CRF.\n\nQuestion: What is the name of the dataset?\n\nAnswer: The dataset is called SnapCaptions.\n\nQuestion: What is the name of the paper?\n\nAnswer: The paper is called \"Multimodal Named Entity Recognition for Social Media Posts\".\n\nQuestion: What is the name of the authors?\n\nAnswer: The authors are Yun-N", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3784}
{"pred": "No, they evaluate on the Wall Street Journal (WSJ) portion of the Penn Treebank.\n\nQuestion: What is the dimensionality of the latent embedding?\n\nAnswer: The dimensionality of the latent embedding is set to 100.\n\nQuestion: What is the dimensionality of the input to the neural projector?\n\nAnswer: The dimensionality of the input to the neural projector is set to the same as the number of input units.\n\nQuestion: What is the dimensionality of the output of the neural projector?\n\nAnswer: The dimensionality of", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 4320}
{"pred": "0.6103\n\nQuestion: What was their highest recall score?\n\nAnswer: 0.7033\n\nQuestion: What was their highest precision score?\n\nAnswer: 0.1119\n\nQuestion: What was their highest F-measure score?\n\nAnswer: 0.2862\n\nQuestion: What was their highest F-measure score for List-type questions?\n\nAnswer: 0.7033\n\nQuestion: What was their highest F-measure score for Yes/No questions?\n\nAnswer: 0", "answers": ["0.5115", "0.6103"], "all_classes": null, "length": 6810}
{"pred": "They evaluate on the Wall Street Journal (WSJ) portion of the Penn Treebank.\n\nQuestion: What is the dimensionality of the word embeddings?\n\nAnswer: The dimensionality of the word embeddings is 100.\n\nQuestion: What is the context window size?\n\nAnswer: The context window size is 1.\n\nQuestion: What is the number of tag clusters?\n\nAnswer: The number of tag clusters is 45.\n\nQuestion: What is the number of coupling layers?\n\nAnswer: The number of coupling layers is 4", "answers": [" Wall Street Journal (WSJ) portion of the Penn Treebank", "Unanswerable"], "all_classes": null, "length": 4327}
{"pred": "The authors evidence the claim by conducting a survey among engineers. The survey results showed that 87.5% of the NLP related jobs belong to a few common tasks, including sentence classification, text matching, sequence labeling, MRC, etc. It further suggested that more than 90% of the networks were composed of several common components, such as embedding, CNN/RNN, Transformer and so on.\n\nQuestion: What is the difference between NeuronBlocks and other deep learning toolkits?\n\nAnswer: NeuronBlocks is built on PyTorch. The", "answers": ["By conducting a survey among engineers", "Unanswerable"], "all_classes": null, "length": 1692}
{"pred": "We use the SimpleQuestions BIBREF2 and WebQSP BIBREF25 datasets. Each question in these datasets is labeled with the gold semantic parse. Hence we can directly evaluate relation detection performance independently as well as evaluate on the KBQA end task.\n\nQuestion: What is the size of the KB?\n\nAnswer: We use the entire Freebase KB for evaluation purposes. Following yih-EtAl:2016:P16-2, we use S-MART BIBREF26 entity-linking outputs. In order to evaluate the", "answers": ["SimpleQuestions, WebQSP", "WebQSP, SimpleQuestions"], "all_classes": null, "length": 4524}
